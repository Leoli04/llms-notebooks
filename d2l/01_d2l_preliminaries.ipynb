{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4iqw9LtyyZI7pNMk36au1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leoli04/llms-notebooks/blob/main/d2l/01_d2l_preliminaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预备知识\n",
        "\n",
        "机器学习通常需要处理大型数据集。 我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。 线性代数为人们提供了一些用来处理表格数据的方法。 我们不会太深究细节，而是将重点放在矩阵运算的基本原理及其实现上。\n",
        "\n",
        "深度学习是关于优化的学习。 对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。 在算法的每个步骤中，决定以何种方式调整参数需要一点微积分知识。幸运的是，autograd包会自动计算微分。\n",
        "\n",
        "\n",
        "以下以`pytorch`为示例演示，\n",
        "\n"
      ],
      "metadata": {
        "id": "JpcyrN3CdvxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据操作--Tensor\n",
        "\n",
        "为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。 通常，我们需要做两件重要的事：\n",
        "- （1）获取数据；\n",
        "- （2）将数据读入计算机后对其进行处理。\n",
        "\n",
        "𝑛维数组，也称为*张量*（tensor）。 使用过Python中NumPy计算包的读者会对本部分很熟悉。 无论使用哪个深度学习框架，它的*张量类*（在MXNet中为`ndarray`， 在PyTorch和TensorFlow中为`Tensor`）都与Numpy的`ndarray`类似。 但深度学习框架又比Numpy的`ndarray`多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jewLbMNZCaXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 入门\n",
        "[张量表示一个由数值组成的数组，这个数组可能有多个维度]。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。"
      ],
      "metadata": {
        "id": "lebM7Z5wIz9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOSo_l8Sdn4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83b8e11-8119-4dfc-d051-5c1c614a07e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "torch.Size([12])\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "# 导入 虽然它被称为PyTorch，但是代码中使用`torch`而不是`pytorch`。\n",
        "import torch\n",
        "\n",
        "\n",
        "# 使用 arange 创建一个行向量 x。这个行向量包含以0开始的前12个整数，它们默认创建为整数。\n",
        "x = torch.arange(12)\n",
        "\n",
        "print(x)\n",
        "# 可以通过张量的shape属性来访问张量（沿每个轴的长度）的形状\n",
        "print(x.shape)\n",
        "# 如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）\n",
        "print(x.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。**\n",
        "注意，\n",
        "- 通过改变张量的形状，张量的大小不会改变。\n",
        "- 我们不需要通过手动指定每个维度来改变形状。即我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)\n"
      ],
      "metadata": {
        "id": "nliXKTjrEgTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.reshape(3,4))\n",
        "X = x.reshape(3,-1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifwovGnUEtsP",
        "outputId": "fc6c4858-73f2-435f-c815-2ffc12dc3768"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。"
      ],
      "metadata": {
        "id": "_AwI-dZoHJN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。\n",
        "print(torch.zeros((2, 3, 4)))\n",
        "print(\"*\"*100)\n",
        "print(torch.ones(2,3,4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH21GTyHHKfR",
        "outputId": "0da301e7-0f29-4faf-d824-df48097e5db2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "****************************************************************************************************\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。\n",
        "torch.randn(3,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJt2IKeXHx0S",
        "outputId": "6dd27ace-fb1e-4ff2-cebd-2dcb3e9ff6ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7964,  0.6282,  0.9531, -1.5982],\n",
              "        [-0.0423, -0.2268,  1.1224,  1.0580],\n",
              "        [-1.3670,  1.1304,  1.0922, -0.0257]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。"
      ],
      "metadata": {
        "id": "6ARpwu9_IW1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh9mdV54IZdJ",
        "outputId": "71f88989-cee8-4096-91b2-cdc6eb6f9cb0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 运算符\n",
        "\n",
        "最简单且最有用的操作是按元素（elementwise）运算。 它们将标准标量运算符应用于数组的每个元素。\n",
        "\n",
        "对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算。"
      ],
      "metadata": {
        "id": "QYBjupwLJrx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0,2,4,8])\n",
        "y = torch.tensor([2,2,2,2])\n",
        "# **运算符是求幂运算\n",
        "x+y,x-y,x*y,x/y,x**y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QTZsT5bKIr7",
        "outputId": "cb1b8492-297c-441f-ca96-b8f9c3ecc099"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "算输入张量 x 的指数值。"
      ],
      "metadata": {
        "id": "ykS2vypDLT7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffymb8w_LShl",
        "outputId": "2dd39929-5172-4576-c38d-e97553bd6a57"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。\n",
        "\n",
        "我们也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。"
      ],
      "metadata": {
        "id": "x_xYzn9zK8WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12,dtype=torch.float32).reshape(3,-1)\n",
        "Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
        "# 分别沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时\n",
        "torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqJp3CudP2BR",
        "outputId": "496d36e0-573d-4eb8-a79f-684d4230ce7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]),\n",
              " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "判断张量每个位置元素是否相等 以X == Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X == Y在该位置处为真，否则该位置为0。"
      ],
      "metadata": {
        "id": "e_e5AtPeRHyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVOw4h3hQ7Jd",
        "outputId": "3f44c01c-8e89-41dd-ca80-516d32138b7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True, False,  True],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X < Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVOKe8uQGI4f",
        "outputId": "b1e03888-3999-4164-c888-2e957fba33fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True, False,  True, False],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对张量中的所有元素进行求和"
      ],
      "metadata": {
        "id": "FGf8mGU4RVSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfIg2ju1RWfJ",
        "outputId": "7158c6f5-616c-4543-b681-4300e9fc1428"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 广播机制\n",
        "\n",
        "在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n",
        "\n",
        "- 1.通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
        "\n",
        "- 2.对生成的数组执行按元素操作。"
      ],
      "metadata": {
        "id": "SxfxEMJtRbx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape(3,1)\n",
        "b = torch.arange(2).reshape(1,2)\n",
        "print(a,b)\n",
        "print(\"*\"*100)\n",
        "print(a+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CCzcSl5RtkA",
        "outputId": "d0f3dcae-374d-47be-fec9-d46ad25a23be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]]) tensor([[0, 1]])\n",
            "****************************************************************************************************\n",
            "tensor([[0, 1],\n",
            "        [1, 2],\n",
            "        [2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 索引和切片\n",
        "\n",
        "张量中的元素可以通过索引访问。与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。"
      ],
      "metadata": {
        "id": "YESBD-vhSzmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "# 用[-1]选择最后一个元素\n",
        "print(X[-1])\n",
        "# 用[1:3]选择第二个和第三个元素\n",
        "print(X[1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwxoaurLTCG8",
        "outputId": "6fd2f7cd-0bb3-4720-ea73-1e00fe51b5d0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([ 8.,  9., 10., 11.])\n",
            "tensor([[ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。"
      ],
      "metadata": {
        "id": "GsXBN-46T3dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。\n",
        "X[0:2, :] = 12\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3Al_orT9J2",
        "outputId": "f3161544-1c43-4cd7-dbfc-efbfd299cc89"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12., 12.],\n",
              "        [12., 12., 12., 12.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 节省内存\n",
        "\n",
        "运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。\n",
        " 这是因为**Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。**\n",
        "\n",
        " > Python的id()函数提供了内存中引用对象的确切地址。"
      ],
      "metadata": {
        "id": "4dULvpGEULEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(Y)\n",
        "\n",
        "Y = Y + X\n",
        "id(Y) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bft1jJUtgpv6",
        "outputId": "0b72e0bb-e677-4f9e-eb5e-0f690a3590c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这可能是不可取的，原因有两个：\n",
        "\n",
        "1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；\n",
        "2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n",
        "\n",
        "我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。"
      ],
      "metadata": {
        "id": "6nHlq4WikwzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print(\"id(Z):\",id(Z))\n",
        "\n",
        "Z[:] = X+Y\n",
        "print(\"id(Z):\",id(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQZHd0E-lDtr",
        "outputId": "747de6c9-f207-4d30-a696-ecfe7377b0e0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 136727586731600\n",
            "id(Z): 136727586731600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。(在后续计算中没有重复使用X)"
      ],
      "metadata": {
        "id": "qL3Mi-lylzUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(X)\n",
        "X += Y\n",
        "id(X) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQdG_47Qlpii",
        "outputId": "ffc00a64-c8d2-42f7-a854-b09de7baea8c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 转换为其他Python对象\n",
        "\n",
        "将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。"
      ],
      "metadata": {
        "id": "xW1q0-RDFK4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "A = X.numpy()\n",
        "B = torch.tensor(A)\n",
        "type(A),type(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flls7sclFVSo",
        "outputId": "837d4368-3c5b-4281-dc1c-cfa0f294fd2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "深度学习存储和操作数据的主要接口是张量（\n",
        "维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。"
      ],
      "metadata": {
        "id": "ZKJpmd3fF1SA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据预处理-pandas\n",
        "\n",
        "为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始， 而不是从那些准备好的张量格式数据开始。 在Python中常用的数据分析工具中，我们通常使用pandas软件包。"
      ],
      "metadata": {
        "id": "ntomtd7NJbkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 读取数据集"
      ],
      "metadata": {
        "id": "Rs-GOxysJ2c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv中\n",
        "os.makedirs(os.path.join('..','data'),exist_ok=True)\n",
        "data_file=os.path.join('..','data','house_tuny.csv')\n",
        "with open(data_file,'w') as f:\n",
        "  f.write('NumRooms,Alley,Price\\n') #列名\n",
        "  f.write('NA,Pave,127500\\n')   # 每行表示一个数据样本\n",
        "  f.write('2,NA,106000\\n')\n",
        "  f.write('4,NA,178100\\n')\n",
        "  f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "rkF3j1lpKZZc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。\n",
        "\n",
        "\n",
        "```\n",
        "# 如果没有安装pandas，只需取消对以下行的注释来安装pandas\n",
        "# !pip install pandas\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "A-s1D8qFMg71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "eKIlqyYTMcfb",
        "outputId": "29b5f4ba-454f-41a8-b60c-e6d0b64571c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NumRooms Alley   Price\n",
              "0       NaN  Pave  127500\n",
              "1       2.0   NaN  106000\n",
              "2       4.0   NaN  178100\n",
              "3       NaN   NaN  140000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1167ce53-4c08-4002-81bf-1e6c489c7a9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumRooms</th>\n",
              "      <th>Alley</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Pave</td>\n",
              "      <td>127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>178100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1167ce53-4c08-4002-81bf-1e6c489c7a9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1167ce53-4c08-4002-81bf-1e6c489c7a9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1167ce53-4c08-4002-81bf-1e6c489c7a9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b175a44-6965-4cd4-973d-7b5877fba037\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b175a44-6965-4cd4-973d-7b5877fba037')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b175a44-6965-4cd4-973d-7b5877fba037 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"NumRooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4142135623730951,\n        \"min\": 2.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Alley\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Pave\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30255,\n        \"min\": 106000,\n        \"max\": 178100,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          106000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 处理缺失值\n",
        "\n",
        "注意，“NaN”项代表缺失值。 为了处理缺失的数据，**典型的方法包括插值法和删除法**， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。\n",
        "\n",
        "> 注意：\n",
        "- 对于数值列，您可以使用 mean() 方法来计算平均值并填充缺失值。\n",
        "- 对于非数值列，您可以使用 mode() 方法来获取最常见的值（众数）并填充缺失值，或者使用其他适当的方法。"
      ],
      "metadata": {
        "id": "3HebtC3-MpBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "\n",
        "print(inputs)\n",
        "print('*' * 100)\n",
        "print(outputs)\n",
        "print('*' * 100)\n",
        "# fillna 方法用于填充 DataFrame 中的缺失值\n",
        "# inputs.mean() 计算 inputs 中每个列的平均值。\n",
        "# 由于inputs包含非数字列表直接使用inputs.fillna(inputs.mean())会报错\n",
        "# inputs = inputs.fillna(inputs.mean())\n",
        "# print(inputs)\n",
        "# 处理数值列的缺失值，使用平均值填充\n",
        "# numerical_cols = inputs.select_dtypes(include=['number']).columns\n",
        "# inputs[numerical_cols] = inputs[numerical_cols].fillna(inputs[numerical_cols].mean())\n",
        "\n",
        "# 处理非数值列的缺失值，这里使用众数填充，也可以选择其他方法\n",
        "# categorical_cols = inputs.select_dtypes(exclude=['number']).columns\n",
        "# inputs[categorical_cols] = inputs[categorical_cols].fillna(inputs[categorical_cols].mode().iloc[0])\n",
        "\n",
        "inputs = inputs.fillna(inputs.select_dtypes(include='number').mean())\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3z8_LiONB2J",
        "outputId": "ce7f9c05-46b2-4617-b146-2a151d522898"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley\n",
            "0       NaN  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       NaN   NaN\n",
            "****************************************************************************************************\n",
            "0    127500\n",
            "1    106000\n",
            "2    178100\n",
            "3    140000\n",
            "Name: Price, dtype: int64\n",
            "****************************************************************************************************\n",
            "   NumRooms Alley\n",
            "0       3.0  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       3.0   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用删除法处理缺失值最多列步骤：\n",
        "- 使用 isnull() 或 isna() 方法来识别缺失值。\n",
        "- 使用 sum() 函数来计算每列的缺失值数量。\n",
        "- 使用 idxmax() 函数找到缺失值最多的列的索引。\n",
        "- 使用 drop 方法删除该列。\n"
      ],
      "metadata": {
        "id": "odnzjU2PUind"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 假设 data 是已经加载的 DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4],\n",
        "    'B': [None, 2, 3, 4],\n",
        "    'C': [1, None, None, 4]\n",
        "})\n",
        "\n",
        "# 计算每列的缺失值数量\n",
        "missing_values_count = data.isnull().sum()\n",
        "\n",
        "print(missing_values_count)\n",
        "\n",
        "# 找到缺失值最多的列的索引\n",
        "column_to_drop = missing_values_count.idxmax()\n",
        "\n",
        "# 删除缺失值最多的列\n",
        "data_dropped = data.drop(columns=column_to_drop)\n",
        "\n",
        "print(data_dropped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPRNfwVwU_HP",
        "outputId": "e50e2f17-3057-49fd-f6ed-f1f4fb3a444d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A    1\n",
            "B    1\n",
            "C    2\n",
            "dtype: int64\n",
            "     A    B\n",
            "0  1.0  NaN\n",
            "1  2.0  2.0\n",
            "2  NaN  3.0\n",
            "3  4.0  4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。\n",
        "\n",
        "由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。"
      ],
      "metadata": {
        "id": "_9GMeASJS9Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990DzRAzS4AB",
        "outputId": "cbe64664-e40e-4ff2-99e3-2cb57bcaedb6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0        True      False\n",
            "1       2.0       False       True\n",
            "2       4.0       False       True\n",
            "3       3.0       False       True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 转换为张量格式\n",
        "\n",
        "现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式。 然后可以使用‘数据操作’这一节方法操作数据"
      ],
      "metadata": {
        "id": "AvA2dNh8TJ15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
        "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7l65OpTYq_",
        "outputId": "08c6d0c3-31fe-446b-f62a-baf45897f01e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3., 1., 0.],\n",
              "         [2., 0., 1.],\n",
              "         [4., 0., 1.],\n",
              "         [3., 0., 1.]], dtype=torch.float64),\n",
              " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "* `pandas`软件包是Python中常用的数据分析工具中，`pandas`可以与张量兼容。\n",
        "* 用`pandas`处理缺失的数据时，我们可根据情况选择用插值法和删除法。"
      ],
      "metadata": {
        "id": "Y_gBY38STwox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 线性代数(linear-algebra)\n",
        "\n",
        "接下来将简要地回顾一下部分基本线性代数内容。 这些内容有助于读者了解和实现本书中介绍的大多数模型。"
      ],
      "metadata": {
        "id": "6Cmje3F4Wxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 标量\n",
        "\n",
        "标量由只有一个元素的张量表示。在这里，标量变量由普通小写字母表示（例如，$x$、$y$和$z$）"
      ],
      "metadata": {
        "id": "u_EzeUq3Yd9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "print(x)\n",
        "\n",
        "print(y)\n",
        "\n",
        "x + y, x * y, x / y, x**y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45vPiJPgY5J-",
        "outputId": "9ebfedb7-4c11-4977-b1bd-d56203c7efa0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(2.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 向量\n",
        "\n",
        "向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。\n",
        "\n",
        "当向量表示数据集中的样本时，它们的值具有一定的现实意义。 例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联， 其分量与其收入、工作年限、过往违约次数和其他因素相对应。\n",
        "\n",
        "在数学表示法中，向量通常记为粗体、小写的符号\n",
        "（例如，$\\mathbf{x}$、$\\mathbf{y}$和$\\mathbf{z})$）。\n"
      ],
      "metadata": {
        "id": "utqSCK8lZIK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4)\n",
        "print(x)\n",
        "# 可以使用下标来引用向量的任一元素\n",
        "print(x[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJNaSy4vZxyu",
        "outputId": "10c265e4-ffb9-4206-d1e4-5a9d70b4abf6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3])\n",
            "tensor(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 长度、维度和形状\n",
        "\n",
        "向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。\n",
        "\n",
        "我们可以通过调用Python的内置len()函数来访问张量的长度。\n",
        "\n",
        "向量的长度通常称为向量的维度（dimension）。\n",
        "\n",
        "当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。"
      ],
      "metadata": {
        "id": "4zD_AGqza1zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x))\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irFuhtccbOEP",
        "outputId": "54d01b07-48b8-4f5e-b65a-9086cb683a2f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，\n",
        "- 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。\n",
        "- 张量的维度用来表示张量具有的轴数。"
      ],
      "metadata": {
        "id": "JAuwQHOQbv4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 矩阵\n",
        "\n",
        "正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。\n",
        "\n",
        "矩阵，我们通常用粗体、大写字母来表示\n",
        "（例如，$\\mathbf{X}$、$\\mathbf{Y}$和$\\mathbf{Z}$），\n",
        "在代码中表示为具有两个轴的张量。\n",
        "\n",
        "当调用函数来实例化张量时，\n",
        "我们可以[**通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的矩阵**]。\n",
        "\n",
        "\n",
        "当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。\n",
        "通常用$\\mathbf{a}^\\top$来表示矩阵的转置。\n",
        "\n",
        "可以通过`.T`在代码中访问(**矩阵的转置**)"
      ],
      "metadata": {
        "id": "gSOZOyJub81f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20).reshape(5, 4)\n",
        "print(A)\n",
        "\n",
        "print(A.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N7PXUi5cdxv",
        "outputId": "b391a57d-ae5e-416c-f34b-713ededbe195"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11],\n",
            "        [12, 13, 14, 15],\n",
            "        [16, 17, 18, 19]])\n",
            "tensor([[ 0,  4,  8, 12, 16],\n",
            "        [ 1,  5,  9, 13, 17],\n",
            "        [ 2,  6, 10, 14, 18],\n",
            "        [ 3,  7, 11, 15, 19]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 张量\n",
        "\n",
        "张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的$n$维数组的通用方法。\n",
        "例如，向量是一阶张量，矩阵是二阶张量。\n",
        "张量用特殊字体的大写字母表示（例如，$\\mathsf{X}$、$\\mathsf{Y}$和$\\mathsf{Z}$）\n",
        "\n",
        "当我们开始处理图像时，张量将变得更加重要，图像以\n",
        "维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。"
      ],
      "metadata": {
        "id": "xEcnbOBidqEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(24).reshape(2,3,4)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CASqgj8heMQ5",
        "outputId": "aff557d8-b2a8-4f0c-8a1a-8ffd52f1839e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 张量算法的基本性质\n",
        "\n",
        "标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。"
      ],
      "metadata": {
        "id": "KPe3u-j8enxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
        "A, A + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jwZ8vTe9ra",
        "outputId": "a26b1058-8fd2-4c12-aaf6-297ce58a4436"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]),\n",
              " tensor([[ 0.,  2.,  4.,  6.],\n",
              "         [ 8., 10., 12., 14.],\n",
              "         [16., 18., 20., 22.],\n",
              "         [24., 26., 28., 30.],\n",
              "         [32., 34., 36., 38.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）**]。"
      ],
      "metadata": {
        "id": "g7uPdv44shL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqzQRFiXsjkp",
        "outputId": "afdbd089-1278-4f17-ddc7-f8637326a045"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
      ],
      "metadata": {
        "id": "jBPis9kfs-e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "print(X)\n",
        "print('*'*100)\n",
        "a + X, (a * X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wjNPL2ktBLv",
        "outputId": "0657757b-647a-49f5-ddf1-5ebfed8dc48d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]),\n",
              " torch.Size([2, 3, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  降维\n",
        "\n",
        "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。\n",
        "- axis=0，求和所有行的元素来降维（轴0）；\n",
        "- axis=1求和所有列方向\n",
        "- axis=[0, 1]，沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和，也是不指定的时，默认处理方式。"
      ],
      "metadata": {
        "id": "f0YTNB76tNxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4, dtype=torch.float32)\n",
        "print((x, x.sum()))\n",
        "\n",
        "print('*'*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XVxqggFtfig",
        "outputId": "4bbcbd5f-5eb3-4b35-a2c9-46028eea2ca4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0., 1., 2., 3.]), tensor(6.))\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(A)\n",
        "print('*'*100)\n",
        "A.shape, A.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4EqRYiIuKzd",
        "outputId": "ce8eef62-0ed2-410c-9ec7-19c3b28c682f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), tensor(190.))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过求和所有行的元素来降维（轴0）\n",
        "A_sum_axis0 = A.sum(axis=0)\n",
        "# 指定axis=1将通过汇总所有列的元素降维（轴1）\n",
        "A_sum_axis1 = A.sum(axis=1)\n",
        "\n",
        "A_sum_axis0,A_sum_axis0.shape,A_sum_axis1,A_sum_axis1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03XzrC3uuZkS",
        "outputId": "6ef158c2-ca96-4378-d5fb-76ba529d8b40"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([40., 45., 50., 55.]),\n",
              " torch.Size([4]),\n",
              " tensor([ 6., 22., 38., 54., 70.]),\n",
              " torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(axis=[0, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwRuGhBjvoPv",
        "outputId": "ec10f0f7-66e5-497b-ffaa-d53de6d63bfe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(190.)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算平均值的函数（mean或average）也可以沿指定轴降低张量的维度。"
      ],
      "metadata": {
        "id": "-ohcAmSZwBAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(), A.sum() / A.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_1rAKNdwGuW",
        "outputId": "3f5d8049-0b3c-4166-b1b2-26d423361ba2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(9.5000), tensor(9.5000))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(A.sum(axis=0))\n",
        "print(A.shape[0])\n",
        "\n",
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2crdIFNwRi4",
        "outputId": "d454cd2f-7c48-4297-8c1a-a81a738ffa1e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([40., 45., 50., 55.])\n",
            "5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 非降维求和\n",
        "\n",
        "可以通过`keepdims=True `在调用函数来计算总和或均值时保持轴数不变,即轴数保持不变"
      ],
      "metadata": {
        "id": "dJlR7lDmwqCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(A)\n",
        "print(\"*\"*100)\n",
        "print(A.sum(axis=1))\n",
        "print(\"*\"*100)\n",
        "sum_A = A.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(sum_A)\n",
        "print(\"*\"*100)\n",
        "A / sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq8PokGvw8E7",
        "outputId": "7e1463c4-6d87-4640-a9bd-78a19455e0ad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "****************************************************************************************************\n",
            "tensor([ 6., 22., 38., 54., 70.])\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "tensor([[ 6.],\n",
            "        [22.],\n",
            "        [38.],\n",
            "        [54.],\n",
            "        [70.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
              "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
              "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
              "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
              "        [0.2286, 0.2429, 0.2571, 0.2714]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "调用cumsum函数 沿某个轴计算A元素的累积总和，也可以指定axis=0（按行计算）"
      ],
      "metadata": {
        "id": "8GQGZm-3zDEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.cumsum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8gHq1f_zM6F",
        "outputId": "a8ed539b-f34d-4ee0-8248-c0b5daf5d3c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  6.,  8., 10.],\n",
              "        [12., 15., 18., 21.],\n",
              "        [24., 28., 32., 36.],\n",
              "        [40., 45., 50., 55.]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 点积（Dot Product）\n",
        "\n",
        "点积， 是相同位置的按元素乘积的和。\n",
        "\n",
        "我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积："
      ],
      "metadata": {
        "id": "9m_vNyV7zQuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones(4, dtype = torch.float32)\n",
        "\n",
        "print(torch.sum(x * y))\n",
        "print(\"*\"*100)\n",
        "x, y, torch.dot(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQK22_2Ezq-p",
        "outputId": "b6d973e0-0ad1-48f8-902e-96677c210a7a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.)\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 矩阵-向量积\n",
        "\n",
        "矩阵-向量积就是，向量x与矩阵的每一行进行点击运算(相同位置的按元素乘积的和)\n",
        "\n",
        "当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n",
        "\n"
      ],
      "metadata": {
        "id": "GoO7_uEO0Crd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(A)\n",
        "\n",
        "print(x)\n",
        "print(\"*\"*100)\n",
        "A.shape, x.shape, torch.mv(A, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvnJwqmY0Ye2",
        "outputId": "a7f78c69-4564-495e-bb87-95e9f98eb627"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "tensor([0., 1., 2., 3.])\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 矩阵-矩阵乘法\n",
        "\n",
        "[**我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \\times m$矩阵**]。\n",
        "\n",
        "- n 是A矩阵的函数\n",
        "- m 是B矩阵的列数\n",
        "\n",
        "在下面的代码中，我们在`A`和`B`上执行矩阵乘法。\n",
        "这里的`A`是一个5行4列的矩阵，`B`是一个4行3列的矩阵。\n",
        "两者相乘后，我们得到了一个5行3列的矩阵。\n",
        "\n"
      ],
      "metadata": {
        "id": "83FZHXMy23OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.ones(4, 3)\n",
        "print(A)\n",
        "print(B)\n",
        "print(\"*\"*100)\n",
        "torch.mm(A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYg4pwUCCaiw",
        "outputId": "0e3c1267-46e3-4bfd-b3e5-804a74f6f40d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  6.,  6.],\n",
              "        [22., 22., 22.],\n",
              "        [38., 38., 38.],\n",
              "        [54., 54., 54.],\n",
              "        [70., 70., 70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 范数\n",
        "\n",
        "线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。"
      ],
      "metadata": {
        "id": "en6i6lPnDuSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**$L_1$范数，曼哈顿距离，它表示为向量元素的绝对值之和。**\n",
        "\n",
        "**$L_2$范数，它是最常用的范数，也称为欧几里得范数。L2 范数是指向量元素平方和的平方根。**\n",
        "**Frobenius范数是矩阵范数的一种，定义为矩阵所有元素平方和的平方根**\n",
        "\n",
        "**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**\n",
        "\n",
        "对于一维张量（向量），默认情况下 torch.norm 计算的是 L2 范数，"
      ],
      "metadata": {
        "id": "aRferTmPb0s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMq3AvssGUrb",
        "outputId": "d0f8ec6d-6549-4b25-8ff2-1b2cd9e55c98"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(u).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0jr2iaKaxfp",
        "outputId": "b3cc82e3-3fc5-4b0d-f0c7-9f595c81ebcb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 调用以下函数将计算矩阵的Frobenius范数。\n",
        "torch.norm(torch.ones((4, 9)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zflqpKpXa2jP",
        "outputId": "05d87ac8-6827-47be-9bd7-532503bd5d3f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
      ],
      "metadata": {
        "id": "iXPDBODxdWIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "* 标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
        "* 向量泛化自标量，矩阵泛化自向量。\n",
        "* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
        "* 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。\n",
        "* 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。\n",
        "* 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。\n",
        "* 我们可以对标量、向量、矩阵和张量执行各种操作。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQqHWisqdX9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 练习"
      ],
      "metadata": {
        "id": "NZt54YqtgR3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义了形状的张量X。len(X)的输出结果是什么？\n",
        "a = torch.arange(24).reshape(2,3,4)\n",
        "print(a)\n",
        "len(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfDFcrSLe4CI",
        "outputId": "1772f2cc-c6cc-44d8-ff30-0d710cd24e24"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于任意形状的张量X,len(X)是否总是对应于X特定轴的长度?这个轴是什么?\n",
        "\n",
        " 对应0轴的长度，它描述的是n-1维张量的数量，如形状$(2,3,4)$的张量`X`可以理解为有2个形状为(3,4)的张量"
      ],
      "metadata": {
        "id": "em1hquFugXbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行`A/A.sum(axis=1)`，看看会发生什么。请分析一下原因？\n",
        "\n",
        "A的形状如下：\n",
        "```\n",
        "tensor([[ 0., 1., 2.,  3.],\n",
        "    [ 4., 5., 6.,  7.],\n",
        "    [ 8., 9., 10., 11.],\n",
        "    [12., 13., 14., 15.],\n",
        "    [16., 17., 18., 19.]])\n",
        "```\n",
        "\n",
        "会报错，因为 PyTorch 不会自动进行这种跨维度的广播。\n",
        "\n",
        "在 PyTorch 中，要进行这种类型的操作，你需要确保要操作的张量在对应的维度上具有相同的大小，或者其中一个张量在该维度上的大小为 1，这样才能进行广播。"
      ],
      "metadata": {
        "id": "hj0PsyqMhq04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(A)\n",
        "print(A.sum(axis=1))\n",
        "\n",
        "# A/A.sum(axis=1)\n",
        "\n",
        "row_sums = A.sum(axis=1).unsqueeze(1)  # 增加一个维度，使其成为形状为 (5, 1) 的张量\n",
        "result = A / row_sums  # 现在可以正常进行广播和除法操作\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGnG-aFWhuyy",
        "outputId": "2ede5edd-a6d6-4abf-a65b-cd44a1ef0eba"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "tensor([ 6., 22., 38., 54., 70.])\n",
            "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
            "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
            "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
            "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
            "        [0.2286, 0.2429, 0.2571, 0.2714]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "一个具有形状$(2,3,4)$的张量，在轴0、1、2上的求和输出是什么形状?"
      ],
      "metadata": {
        "id": "0zu3whmOlSh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(24).reshape(2,3,4)\n",
        "print(a.sum(axis=0))\n",
        "print(a.sum(axis=1))\n",
        "print(a.sum(axis=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFrHn5wQlYKT",
        "outputId": "81222580-d921-48b1-8699-5226fbebf5d9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12, 14, 16, 18],\n",
            "        [20, 22, 24, 26],\n",
            "        [28, 30, 32, 34]])\n",
            "tensor([[12, 15, 18, 21],\n",
            "        [48, 51, 54, 57]])\n",
            "tensor([[ 6, 22, 38],\n",
            "        [54, 70, 86]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 微积分(calculus)\n",
        "\n",
        "在微分学最重要的应用是优化问题，即考虑如何把事情做到最好。 正如在 2.3.10.1节中讨论的那样， 这种问题在深度学习中是无处不在的。\n",
        "\n",
        "在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。\n",
        "通常情况下，变得更好意味着最小化一个*损失函数*（loss function），\n",
        "即一个衡量“模型有多糟糕”这个问题的分数。\n",
        "最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。\n",
        "但“训练”模型只能将模型与我们实际能看到的数据相拟合。\n",
        "因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
        "\n",
        "* *优化*（optimization）：用模型拟合观测数据的过程；\n",
        "* *泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n",
        "\n",
        "符号说明：\n",
        "* $\\frac{dy}{dx}$：$y$关于$x$的导数\n",
        "* $\\frac{\\partial y}{\\partial x}$：$y$关于$x$的偏导数\n",
        "* $\\nabla_{\\mathbf{x}} y$：$y$关于$\\mathbf{x}$的梯度\n",
        "* $\\int_a^b f(x) \\;dx$: $f$在$a$到$b$区间上关于$x$的定积分\n",
        "* $\\int f(x) \\;dx$: $f$关于$x$的不定积分"
      ],
      "metadata": {
        "id": "0zM96V7snVMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 导数和微分\n",
        "\n",
        "**导数**衡量的是一个函数在某一点处的瞬时变化率。如果有一个函数$f(x)$，它的导数通常表示为 $f'(x)$或$\\frac{df}{dx}$.\n",
        "\n",
        "**微分**是一个过程，它描述了当函数的一个自变量发生微小变化 $Δx$ 时，函数$f(x)$的变化量$df$。如果$f(x)$在$x=a$处可导，那么可以近似表示为：$df=f'(a)⋅dx$\n",
        "\n",
        "其中$dx=Δx$ 是自变量的一个无穷小变化量，$df$ 是函数值的相应变化量。微分 $df$ 与 $dx$ 成正比，比例系数就是导数f'(a) 。\n",
        "\n"
      ],
      "metadata": {
        "id": "ghp9G35_Er8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "假设我们有一个函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$，其输入和输出都是标量。\n",
        "(**如果$f$的*导数*存在，这个极限被定义为**)\n",
        "\n",
        "**$$ f'(x)= \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**\n",
        "\n",
        "\n",
        "如果$f'(a)$存在，则称$f$在$a$处是*可微*（differentiable）的。\n",
        "如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。\n",
        "\n",
        "#### 求导公式\n",
        "\n",
        "给定$y=f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量。以下表达式是等价的：\n",
        "\n",
        "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
        "\n",
        "其中符号$\\frac{d}{dx}$和$D$是*微分运算符*，表示*微分*操作。\n",
        "我们可以使用以下规则来对常见函数求微分：\n",
        "\n",
        "* $DC = 0$（$C$是一个常数）\n",
        "* $Dx^n = nx^{n-1}$（*幂律*（power rule），$n$是任意实数）\n",
        "* $De^x = e^x$\n",
        "* $D\\ln(x) = 1/x$\n",
        "\n",
        "#### 求导方法四则运算\n",
        "\n",
        "假设函数$f$和$g$都是可微的，$C$是一个常数，则：\n",
        "\n",
        "*常数相乘法则*\n",
        "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
        "\n",
        "*加法法则*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
        "\n",
        "*乘法法则*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
        "\n",
        "*除法法则*\n",
        "\n",
        "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n"
      ],
      "metadata": {
        "id": "0zAOYiYpFP7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了更好地解释导数，让我们做一个实验。\n",
        "(**定义$u=f(x)=3x^2-4x$**)如下：\n",
        "\n",
        "**通过令$x=1$并让$h$接近$0$，$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$**。\n",
        "我们可以应用上述几个法则来计算$u'=f'(x)=3\\frac{d}{dx}x^2-4\\frac{d}{dx}x=6x-4$。\n",
        "令$x=1$，我们有$u'=2$：在这个实验中，数值结果接近$2$，\n",
        "这一点得到了在本节前面的实验的支持。\n",
        "当$x=1$时，此导数也是曲线$u=f(x)$切线的斜率。"
      ],
      "metadata": {
        "id": "A2AV_ybaGqAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "id": "pzZJPjrSKcR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib_inline import backend_inline\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def f(x):\n",
        "  return 3*x**2-4*x\n",
        "\n",
        "def numerical_lim(f,x,h):\n",
        "  return (f(x+h)-f(x))/h\n",
        "\n",
        "h=0.1\n",
        "for i in range(5):\n",
        "  print(f'h={h:.5f},numerical limit={numerical_lim(f, 1, h):.5f}')\n",
        "  h *= 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CVoEvsqEDGV",
        "outputId": "cda7d123-1fb0-4266-fb58-cddff058e516"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h=0.10000,numerical limit=2.30000\n",
            "h=0.01000,numerical limit=2.03000\n",
            "h=0.00100,numerical limit=2.00300\n",
            "h=0.00010,numerical limit=2.00030\n",
            "h=0.00001,numerical limit=2.00003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了对导数的这种解释进行可视化，我们将使用matplotlib， 这是一个Python中流行的绘图库。 要配置matplotlib生成图形的属性，我们需要定义几个函数。"
      ],
      "metadata": {
        "id": "JC8nMEppLyU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use_svg_display函数指定matplotlib软件包输出svg图表以获得更清晰的图像\n",
        "def use_svg_display():\n",
        "  \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
        "  backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "# set_figsize函数来设置图表大小\n",
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "  \"\"\"设置matplotlib的图表大小\"\"\"\n",
        "  use_svg_display()\n",
        "  d2l.plt.rcParams['figure.figsize'] = figsize\n",
        "\n",
        "# 设置由matplotlib生成图表的轴的属性\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"设置matplotlib的轴\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "\n",
        "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "         ylim=None, xscale='linear', yscale='linear',\n",
        "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
        "    \"\"\"绘制数据点\"\"\"\n",
        "    if legend is None:\n",
        "        legend = []\n",
        "\n",
        "    set_figsize(figsize)\n",
        "    axes = axes if axes else d2l.plt.gca()\n",
        "\n",
        "    # 如果X有一个轴，输出True\n",
        "    def has_one_axis(X):\n",
        "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
        "                and not hasattr(X[0], \"__len__\"))\n",
        "\n",
        "    if has_one_axis(X):\n",
        "        X = [X]\n",
        "    if Y is None:\n",
        "        X, Y = [[]] * len(X), X\n",
        "    elif has_one_axis(Y):\n",
        "        Y = [Y]\n",
        "    if len(X) != len(Y):\n",
        "        X = X * len(Y)\n",
        "    axes.cla()\n",
        "    for x, y, fmt in zip(X, Y, fmts):\n",
        "        if len(x):\n",
        "            axes.plot(x, y, fmt)\n",
        "        else:\n",
        "            axes.plot(y, fmt)\n",
        "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
      ],
      "metadata": {
        "id": "bQBHXoxQLzkW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们可以[**绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$**]，\n",
        "其中系数$2$是切线的斜率。\n",
        "\n",
        "**计算函数在某一点的切线：**\n",
        "- 求导$f'(x)$\n",
        "- 计算斜率，上述计算的斜率为2\n",
        "- 使用点斜式方程 $y−y1=m(x−x1)$ 来表示切线，其中$(x1,y1)$是切点，\n",
        "𝑚是斜率。将切点和斜率代入方程，得到切线的方程。\n",
        "\n",
        "$y=f(x)=3x^2-4x$,当$x=1时，y=-1$,代入上述公式，x=1时的切线如下：\n",
        "\n",
        "$(y-(-1))=2(x-1)简化得到y=2x-3$"
      ],
      "metadata": {
        "id": "10hSGKkyOCp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.arange(start, stop, step)，其中：\n",
        "\n",
        "# start 是序列的起始值。\n",
        "# stop 是序列结束的值，但不包括这个值（即最大值加1才是这个参数的值）。\n",
        "# step 是序列中每个数值之间的间隔。\n",
        "x = np.arange(0, 3, 0.1)\n",
        "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "CuklseNWNXgA",
        "outputId": "1789bcbf-c616-4464-d457-0d5202296037"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-06-13T06:11:40.080253</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 243.529359 183.35625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 145.8 \nL 49.480398 7.2 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"me0ed3f3af7\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me0ed3f3af7\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 145.8 \nL 110.702968 7.2 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#me0ed3f3af7\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 145.8 \nL 171.925539 7.2 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#me0ed3f3af7\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 145.8 \nL 233.148109 7.2 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#me0ed3f3af7\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 116.769994 \nL 235.903125 116.769994 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"ma73080884b\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma73080884b\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 120.569213) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 78.886651 \nL 235.903125 78.886651 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#ma73080884b\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 82.685869) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 41.003307 \nL 235.903125 41.003307 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#ma73080884b\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.802526) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 116.769994 \nL 55.602655 119.573361 \nL 61.724912 121.922129 \nL 67.847169 123.816296 \nL 73.969426 125.255863 \nL 80.091683 126.24083 \nL 86.21394 126.771197 \nL 92.336197 126.846963 \nL 98.458454 126.46813 \nL 104.580711 125.634696 \nL 110.702968 124.346663 \nL 116.825225 122.604029 \nL 122.947482 120.406795 \nL 129.069739 117.754961 \nL 135.191996 114.648527 \nL 141.314254 111.087492 \nL 147.436511 107.071858 \nL 153.558768 102.601624 \nL 159.681025 97.676789 \nL 165.803282 92.297354 \nL 171.925539 86.463319 \nL 178.047796 80.174684 \nL 184.170053 73.431449 \nL 190.29231 66.233614 \nL 196.414567 58.581179 \nL 202.536824 50.474143 \nL 208.659081 41.912508 \nL 214.781338 32.896272 \nL 220.903595 23.425436 \nL 227.025852 13.5 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 139.5 \nL 55.602655 137.984666 \nL 61.724912 136.469333 \nL 67.847169 134.953999 \nL 73.969426 133.438665 \nL 80.091683 131.923331 \nL 86.21394 130.407998 \nL 92.336197 128.892664 \nL 98.458454 127.37733 \nL 104.580711 125.861996 \nL 110.702968 124.346663 \nL 116.825225 122.831329 \nL 122.947482 121.315995 \nL 129.069739 119.800661 \nL 135.191996 118.285328 \nL 141.314254 116.769994 \nL 147.436511 115.25466 \nL 153.558768 113.739327 \nL 159.681025 112.223993 \nL 165.803282 110.708659 \nL 171.925539 109.193325 \nL 178.047796 107.677992 \nL 184.170053 106.162658 \nL 190.29231 104.647324 \nL 196.414567 103.13199 \nL 202.536824 101.616657 \nL 208.659081 100.101323 \nL 214.781338 98.585989 \nL 220.903595 97.070655 \nL 227.025852 95.555322 \n\" clip-path=\"url(#p5fb8f62153)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 145.8 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 145.8 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298438 \nL 59.603125 20.298438 \nL 69.603125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798438) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5fb8f62153\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 偏导数\n",
        "\n",
        "**偏导数是多元函数的导数概念，用于衡量多变量函数沿着某个特定方向的变化率。**\n",
        "\n",
        "到目前为止，我们只讨论了仅含一个变量的函数的微分。\n",
        "在深度学习中，函数通常依赖于许多变量。\n",
        "因此，我们需要将微分的思想推广到*多元函数*（multivariate function）上。\n",
        "\n",
        "设$y = f(x_1, x_2, \\ldots, x_n)$是一个具有$n$个变量的函数。\n",
        "$y$关于第$i$个参数$x_i$的*偏导数*（partial derivative）为：\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
        "\n",
        "为了计算$\\frac{\\partial y}{\\partial x_i}$，\n",
        "我们可以简单地将$x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$看作常数，\n",
        "并计算$y$关于$x_i$的导数。\n",
        "对于偏导数的表示，以下是等价的：\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$"
      ],
      "metadata": {
        "id": "MsC878dzOlUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 梯度\n",
        "\n",
        "梯度是指向函数增长最快的方向的向量，其方向与函数增长速率最大的方向相同，而其大小则表示函数在该方向上增长的速率。\n",
        "\n",
        "我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量。"
      ],
      "metadata": {
        "id": "ZbkhCQPZPnaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
        "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出是一个标量。\n",
        "函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
        "\n",
        "其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n"
      ],
      "metadata": {
        "id": "I2GYvOQNSB6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n",
        "\n",
        "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n",
        "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n",
        "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
        "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n",
        "\n",
        "同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n",
        "正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。"
      ],
      "metadata": {
        "id": "Aj26mTkJSxVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 链式法则\n",
        "\n",
        "在深度学习中，多元函数通常是复合（composite）的， 所以难以应用上述任何规则来微分这些函数。 幸运的是，链式法则可以被用来微分复合函数。\n",
        "\n",
        "让我们先考虑单变量函数。假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} . \\frac{du}{dx}$$\n",
        "\n",
        "现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。\n",
        "假设可微分函数$y$有变量$u_1, u_2, \\ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \\ldots, x_n$。\n",
        "注意，$y$是$x_1, x_2， \\ldots, x_n$的函数。\n",
        "对于任意$i = 1, 2, \\ldots, n$，链式法则给出：\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}$$"
      ],
      "metadata": {
        "id": "pwoopklFTA9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "- 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。\n",
        "\n",
        "- 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。\n",
        "\n",
        "- 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。\n",
        "\n",
        "- 链式法则可以用来微分复合函数。"
      ],
      "metadata": {
        "id": "bRV2npiATm-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 练习\n",
        "\n",
        "1. 绘制函数$y = f(x) = x^3 - \\frac{1}{x}$和其在$x = 1$处切线的图像。\n",
        "1. 求函数$f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。\n",
        "1. 函数$f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2$的梯度是什么？\n",
        "1. 尝试写出函数$u = f(x, y, z)$，其中$x = x(a, b)$，$y = y(a, b)$，$z = z(a, b)$的链式法则。\n",
        "\n",
        "练习4：\n",
        "\n",
        "对于$u关于a$ 的偏导数，我们有：\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial a} = \\frac{\\partial u}{\\partial x}.\\frac{\\partial x}{\\partial a} +  \\frac{\\partial u}{\\partial y}.\\frac{\\partial y}{\\partial a} + \\frac{\\partial u}{\\partial z}.\\frac{\\partial z}{\\partial a}$$\n",
        "\n",
        "\n",
        "对于$u关于b$ 的偏导数，我们有：\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial b} = \\frac{\\partial u}{\\partial x}.\\frac{\\partial x}{\\partial b} +  \\frac{\\partial u}{\\partial y}.\\frac{\\partial y}{\\partial b} + \\frac{\\partial u}{\\partial z}.\\frac{\\partial z}{\\partial b}$$\n",
        "\n",
        "其中，$\\frac{\\partial u}{\\partial x},\\frac{\\partial u}{\\partial u},\\frac{\\partial u}{\\partial z}$是$u$在$(x,y,z)$空间中的偏导数，而$\\frac{\\partial x}{\\partial a},\\frac{\\partial y}{\\partial a},\\frac{\\partial z}{\\partial a},\\frac{\\partial x}{\\partial b},\\frac{\\partial y}{\\partial b},\\frac{\\partial z}{\\partial b}$分别是$x，y,z$关于$a和b$的偏导数。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GK_LD1ohTuF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "练习1：\n",
        "- 对$y = f(x) = x^3 - \\frac{1}{x}$ 得$f'(x)=2x^2+1/x^2$\n",
        "- $y在x = 1$的斜率为$f'(1)=2+1=3$\n",
        "- 切点$(1,f(1))=(1,0)$\n",
        "- 在$x=1处的切线为(y-0)=3(x-1) =》 y=3x-3$"
      ],
      "metadata": {
        "id": "hkf3r1-j8n5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0, 3, 0.2)\n",
        "plot(x, [x**3-1/x, 3* x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "9SXDYwsoPg6p",
        "outputId": "82aa9cf7-e0e0-4d3d-e23c-6e27f3438dae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2e926da71a34>:2: RuntimeWarning: divide by zero encountered in divide\n",
            "  plot(x, [x**3-1/x, 3* x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"245.120313pt\" height=\"183.35625pt\" viewBox=\"0 0 245.120313 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-06-13T09:43:42.253876</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 245.120313 183.35625 \nL 245.120313 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.620312 145.8 \nL 237.920313 145.8 \nL 237.920313 7.2 \nL 42.620312 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 51.497585 145.8 \nL 51.497585 7.2 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m2982ac91a5\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m2982ac91a5\" x=\"51.497585\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(48.316335 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 114.906676 145.8 \nL 114.906676 7.2 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m2982ac91a5\" x=\"114.906676\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(111.725426 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 178.315767 145.8 \nL 178.315767 7.2 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m2982ac91a5\" x=\"178.315767\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(175.134517 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_4\">\n     <!-- x -->\n     <g transform=\"translate(137.310937 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path d=\"M 42.620312 139.537913 \nL 237.920313 139.537913 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"m9006f00d43\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"139.537913\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- −5 -->\n      <g transform=\"translate(20.878125 143.337132) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path d=\"M 42.620312 115.841995 \nL 237.920313 115.841995 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"115.841995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(29.257812 119.641214) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <path d=\"M 42.620312 92.146076 \nL 237.920313 92.146076 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"92.146076\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(29.257812 95.945295) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <path d=\"M 42.620312 68.450158 \nL 237.920313 68.450158 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"68.450158\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(22.895312 72.249377) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <path d=\"M 42.620312 44.754239 \nL 237.920313 44.754239 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"44.754239\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 15 -->\n      <g transform=\"translate(22.895312 48.553458) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_17\">\n      <path d=\"M 42.620312 21.058321 \nL 237.920313 21.058321 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m9006f00d43\" x=\"42.620312\" y=\"21.058321\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g transform=\"translate(22.895312 24.85754) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 64.179403 139.5 \nL 76.861222 127.386646 \nL 89.54304 122.716971 \nL 102.224858 119.339513 \nL 114.906676 115.841995 \nL 127.588494 111.602005 \nL 140.270312 106.222806 \nL 152.952131 99.392288 \nL 165.633949 90.835955 \nL 178.315767 80.298117 \nL 190.997585 67.533341 \nL 203.679403 52.302179 \nL 216.361222 34.368865 \nL 229.04304 13.5 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 51.497585 130.059546 \nL 64.179403 127.216036 \nL 76.861222 124.372526 \nL 89.54304 121.529015 \nL 102.224858 118.685505 \nL 114.906676 115.841995 \nL 127.588494 112.998485 \nL 140.270312 110.154975 \nL 152.952131 107.311464 \nL 165.633949 104.467954 \nL 178.315767 101.624444 \nL 190.997585 98.780934 \nL 203.679403 95.937423 \nL 216.361222 93.093913 \nL 229.04304 90.250403 \n\" clip-path=\"url(#p5785fccd27)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.620312 145.8 \nL 42.620312 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 237.920313 145.8 \nL 237.920313 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.620312 145.8 \nL 237.920312 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.620312 7.2 \nL 237.920312 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 49.620312 44.55625 \nL 174.170313 44.55625 \nQ 176.170313 44.55625 176.170313 42.55625 \nL 176.170313 14.2 \nQ 176.170313 12.2 174.170313 12.2 \nL 49.620312 12.2 \nQ 47.620312 12.2 47.620312 14.2 \nL 47.620312 42.55625 \nQ 47.620312 44.55625 49.620312 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 51.620312 20.298438 \nL 61.620312 20.298438 \nL 71.620312 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- f(x) -->\n     <g transform=\"translate(79.620312 23.798438) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 51.620312 34.976562 \nL 61.620312 34.976562 \nL 71.620312 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(79.620312 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5785fccd27\">\n   <rect x=\"42.620312\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "练习2\n",
        "\n",
        "函数$f(\\mathbf{x})=3x_1^2+5e^{x_2}$是一个多元函数，其中$\\mathbf{x}$是一个向量，包含两个变量$x_1和x_2$。要求这个函数的梯度$\\nabla f(\\mathbf{x})$，我们需要分别对$x_1和x_2$求偏导数。\n",
        "\n",
        "对于$x_1$的偏导数：\n",
        "$$\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(3x_1^2)=6x_1 $$\n",
        "\n",
        "对于$x_2$的偏导数：\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(5e^{x_2})=5e^{x_2} $$\n",
        "\n",
        "所以，函数$f(\\mathbf{x})$的梯度向量$\\nabla f(\\mathbf{x})$为：\n",
        "$$\\nabla f(\\mathbf{x})=(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2})=(6x_1,5e^{x_2})$$\n",
        "\n",
        "练习3：\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3dS9fVYmBB4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自动微积分\n",
        "\n",
        "深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。"
      ],
      "metadata": {
        "id": "Ofm_OHJxL_TL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一个简单的例子\n",
        "\n",
        "**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**\n",
        "\n"
      ],
      "metadata": {
        "id": "1x-90BSzNgyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 首先，我们创建一个包含四个元素的张量，值从0开始，默认步长为1。\n",
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yH5fK75NtA1",
        "outputId": "59156a4d-6660-4ad1-cc02-c4a93ed81015"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置张量x的.requires_grad属性为True，意味着对x进行的操作将在计算图中被跟踪，从而允许之后计算其梯度。\n",
        "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
        "\n",
        "# 在这里调用x.grad实际上并没有显示任何内容，因为在这个点梯度还没有被计算出来，它默认是None。\n",
        "# 通常在进行反向传播计算梯度之前查看梯度是没有意义的。\n",
        "x.grad  # 默认值是None"
      ],
      "metadata": {
        "id": "xtt47JMsOE75"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算x与其自身的点积（即所有元素相乘求和），然后将结果乘以2。\n",
        "# torch.dot(x, x)相当于计算x[0]*x[0] + x[1]*x[1] + x[2]*x[2] + x[3]*x[3]\n",
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJCB9s92OLp0",
        "outputId": "4bb6816f-2033-482e-faa5-2c2fef7959ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 执行反向传播计算，计算输出y关于输入x的梯度。\n",
        "# y是x各元素平方和的两），所以x的梯度应该是每个元素的两倍，因为对每个x[i]的偏导数是2*x[i]。\n",
        "y.backward()\n",
        "# 这次调用会显示计算出的梯度，即每个元素都是其值的两倍\n",
        "# 因为我们计算的是y = 2*(x[0]^2 + x[1]^2 + x[2]^2 + x[3]^2)的梯度，所以对于x[i]，其梯度确实是4*x[i]。\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REp2jI23Ofzq",
        "outputId": "c192acc0-9827-4464-fd22-1db4af560f3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用来验证计算出的梯度是否确实等于4 * x\n",
        "x.grad == 4 * x"
      ],
      "metadata": {
        "id": "g3gEno8tnftd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJjsVecIO17F",
        "outputId": "98bf45e8-644d-4c1d-8869-10daa33d22be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 非标量变量的反向传播\n",
        "\n",
        "当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。"
      ],
      "metadata": {
        "id": "C3bEaHx5p-gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
        "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "print(x)\n",
        "print(y.sum())\n",
        "print(torch.ones(len(x)))\n",
        "# 等价于y.backward(torch.ones(len(x)))\n",
        "y.sum().backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEiKTUiWqzWW",
        "outputId": "3100e732-43bb-4b84-8b6f-823a08a6f2bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor(14., grad_fn=<SumBackward0>)\n",
            "tensor([1., 1., 1., 1.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分离计算\n",
        "\n",
        "有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。"
      ],
      "metadata": {
        "id": "xB2pF9JysiCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面的反向传播函数计算$z=u*x$关于x的偏导数，同时将u作为常数处理， 而不是$z=x*x*x$关于x的偏导数。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RGYFSvKDtjVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "print('x:',x)\n",
        "y = x * x\n",
        "print('y:',y)\n",
        "u = y.detach()\n",
        "print('u:',u)\n",
        "z = u * x\n",
        "print('z:',z)\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocWO4H4zs4iF",
        "outputId": "42e0e312-b46a-4a0d-d62e-1932fcfcdf09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "y: tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
            "u: tensor([0., 1., 4., 9.])\n",
            "z: tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。"
      ],
      "metadata": {
        "id": "lESQm--VtTH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGtNVSptVwK",
        "outputId": "ccfeb0cf-5ac5-493a-8ae9-811b83a5238d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python控制流的梯度计算\n",
        "\n",
        "使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。"
      ],
      "metadata": {
        "id": "07gLCBauvkT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "metadata": {
        "id": "kH_PVDge1jJc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "print('a:',a)\n",
        "d = f(a)\n",
        "print('d:',d)\n",
        "# 使用 d.backward() 执行反向传播，计算 d 相对于 a 的梯度。\n",
        "# 由于 a 被设置为 requires_grad=True，PyTorch 将自动计算 d 相对于 a 的梯度，并将结果存储在 a.grad 属性中。\n",
        "d.backward()\n",
        "print('a:',a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZim_e8Y1lDR",
        "outputId": "631d1eea-f6a0-4be0-e4f3-442e51738c0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor(-0.7571, requires_grad=True)\n",
            "d: tensor(-155060.5156, grad_fn=<MulBackward0>)\n",
            "a: tensor(-0.7571, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d / a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18CIRSOx1qFc",
        "outputId": "1591f651-84e1-49a9-8e5b-e053e4ba7fd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。"
      ],
      "metadata": {
        "id": "RC6I4_qY40Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 概率"
      ],
      "metadata": {
        "id": "y-AjQKFs58CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基本概率论\n",
        "\n",
        " 如果骰子是公平的，那么所有六个结果$\\{1, \\ldots, 6\\}$都有相同的可能发生， 因此我们可以说$1$发生的概率为$\\frac{1}{6}$\n",
        "\n",
        "。"
      ],
      "metadata": {
        "id": "-npQ4wI0Al_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.distributions import multinomial\n",
        "from d2l import torch as d2l\n",
        "\n",
        "fair_probs = torch.ones([6]) / 6\n",
        "print(fair_probs)\n",
        "# multinomial.Multinomial是PyTorch中用于创建多项式分布的对象。两个参数分别是指实验次数和概率分布\n",
        "# .sample()：调用这个方法是从之前定义的多项式分布中抽取样本。\n",
        "# 如果实验次数设置为1，将返回一个长度为6的张量，其中一个位置的值为1，表示抽中的结果，其余位置为0。\n",
        "print(multinomial.Multinomial(1, fair_probs).sample())\n",
        "print(multinomial.Multinomial(10, fair_probs).sample())\n",
        "\n",
        "# 将结果存储为32位浮点数以进行除法\n",
        "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
        "counts / 1000  # 相对频率作为估计值\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTf2w0HI_IFB",
        "outputId": "6e40f846-fb5e-4aaf-fc0f-443a90dcfcc6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n",
            "tensor([0., 0., 1., 0., 0., 0.])\n",
            "tensor([0., 4., 1., 2., 3., 0.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1600, 0.1630, 0.1760, 0.1700, 0.1640, 0.1670])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 模拟500组实验，每组实验包含10次六面骰子的投掷，并记录每组实验中各个点数出现的频次。\n",
        "counts = multinomial.Multinomial(10, fair_probs).sample((500,))\n",
        "cum_counts = counts.cumsum(dim=0)\n",
        "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
        "\n",
        "d2l.set_figsize((6, 4.5))\n",
        "for i in range(6):\n",
        "    d2l.plt.plot(estimates[:, i].numpy(),\n",
        "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
        "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
        "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
        "d2l.plt.gca().set_ylabel('Estimated probability')\n",
        "d2l.plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "ELcB1Ue9_XHp",
        "outputId": "89c307c3-e1e6-42ee-d96d-67f62a31ab84"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x450 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"392.14375pt\" height=\"294.23625pt\" viewBox=\"0 0 392.14375 294.23625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-06-13T14:25:44.073012</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 294.23625 \nL 392.14375 294.23625 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 256.68 \nL 384.94375 256.68 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m6cc912de31\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"65.361932\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"126.356649\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(116.812899 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"187.351365\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(177.807615 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"248.346082\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(238.802332 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"309.340799\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(299.797049 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m6cc912de31\" x=\"370.335515\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(360.791765 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Groups of experiments -->\n     <g transform=\"translate(160.397656 284.956562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-47\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"116.353516\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"177.535156\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"240.914062\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"304.390625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"356.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"388.277344\"/>\n      <use xlink:href=\"#DejaVuSans-66\" x=\"449.458984\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"484.664062\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"516.451172\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"576.224609\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"635.404297\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"698.880859\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"760.404297\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"801.517578\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"829.300781\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"926.712891\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"988.236328\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"1051.615234\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"1090.824219\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"ma963afa220\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma963afa220\" x=\"50.14375\" y=\"220.140004\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 223.939223) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#ma963afa220\" x=\"50.14375\" y=\"169.740006\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 173.539225) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#ma963afa220\" x=\"50.14375\" y=\"119.340008\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 123.139227) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#ma963afa220\" x=\"50.14375\" y=\"68.94001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 72.739229) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#ma963afa220\" x=\"50.14375\" y=\"18.540012\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 22.339231) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 185.463437) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"154.492188\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"182.275391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"279.6875\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"340.966797\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"441.699219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"505.175781\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"536.962891\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"600.439453\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"639.302734\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"700.484375\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"763.960938\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"825.240234\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"888.716797\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"916.5\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"944.283203\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"972.066406\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"1011.275391\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 65.361932 220.140003 \nL 65.971879 169.74 \nL 66.581826 186.539998 \nL 67.191773 144.54001 \nL 67.80172 159.66001 \nL 68.411668 186.539998 \nL 69.021615 148.140002 \nL 69.631562 144.54001 \nL 70.241509 141.740003 \nL 71.461403 155.994559 \nL 72.681298 165.863077 \nL 73.291245 169.74 \nL 74.511139 163.440006 \nL 75.121086 166.775294 \nL 75.731034 175.339999 \nL 76.340981 167.087373 \nL 77.560875 172.14 \nL 78.780769 176.31392 \nL 79.390717 178.140007 \nL 80.000664 171.756004 \nL 80.610611 177.493845 \nL 81.220558 175.339999 \nL 82.440452 178.429659 \nL 83.0504 176.460008 \nL 83.660347 177.869039 \nL 85.490188 172.704706 \nL 86.710083 175.339999 \nL 87.32003 173.826487 \nL 87.929977 169.74 \nL 89.149871 172.260013 \nL 89.759819 170.969267 \nL 90.369766 167.34 \nL 90.979713 170.9121 \nL 91.58966 169.74 \nL 92.199607 166.380003 \nL 94.029449 163.440006 \nL 94.639396 164.597146 \nL 95.249343 167.724011 \nL 95.85929 168.751765 \nL 97.079185 174.494728 \nL 97.689132 171.606671 \nL 98.299079 174.321829 \nL 98.909026 173.340007 \nL 100.738868 180.845085 \nL 101.348815 181.500003 \nL 101.958762 180.480991 \nL 102.568709 177.869039 \nL 103.178656 178.539999 \nL 103.788603 176.040009 \nL 104.398551 176.718464 \nL 105.008498 172.794557 \nL 105.618445 173.501206 \nL 106.228392 172.704706 \nL 106.838339 173.392173 \nL 107.448286 169.74 \nL 108.058234 169.030152 \nL 108.668181 169.74 \nL 109.278128 169.049589 \nL 110.498022 173.100012 \nL 111.107969 172.392642 \nL 111.717917 173.012728 \nL 112.327864 172.324615 \nL 113.547758 176.040009 \nL 114.157705 174.095562 \nL 114.767652 170.969267 \nL 115.3776 171.561685 \nL 116.597494 170.332944 \nL 117.207441 168.567915 \nL 117.817388 169.160694 \nL 118.427335 168.594547 \nL 119.037283 169.173717 \nL 120.257177 168.078462 \nL 120.867124 165.357402 \nL 121.477071 165.946455 \nL 122.087018 167.595332 \nL 122.696966 167.087373 \nL 123.306913 165.540004 \nL 123.91686 167.142062 \nL 124.526807 166.65429 \nL 125.136754 164.140001 \nL 125.746701 162.684001 \nL 126.356649 164.250896 \nL 127.576543 163.378843 \nL 128.18649 161.986155 \nL 128.796437 162.540001 \nL 130.016332 161.732536 \nL 130.626279 160.406673 \nL 131.236226 160.029917 \nL 132.45612 161.112975 \nL 133.066067 159.839999 \nL 133.676015 160.373628 \nL 134.285962 160.013695 \nL 134.895909 160.536524 \nL 135.505856 160.181382 \nL 136.115803 158.109232 \nL 137.335698 160.845883 \nL 137.945645 159.66001 \nL 138.555592 160.159843 \nL 139.165539 159.825249 \nL 139.775486 160.315619 \nL 140.385433 161.610976 \nL 140.995381 162.079206 \nL 141.605328 161.740001 \nL 142.215275 162.199849 \nL 143.435169 159.972569 \nL 144.045116 160.435392 \nL 144.655064 159.352228 \nL 145.265011 160.576373 \nL 145.874958 160.266322 \nL 146.484905 160.713133 \nL 148.314747 159.807165 \nL 148.924694 158.783481 \nL 149.534641 159.950083 \nL 150.144588 158.940009 \nL 150.754535 158.659158 \nL 151.364482 159.09212 \nL 151.97443 160.22392 \nL 153.804271 159.383845 \nL 154.414218 156.368581 \nL 155.024165 156.799471 \nL 155.634113 157.90108 \nL 156.24406 158.316002 \nL 156.854007 158.057892 \nL 157.463954 158.466326 \nL 158.073901 157.551766 \nL 158.683848 157.303644 \nL 159.293796 157.709044 \nL 159.903743 156.816924 \nL 160.51369 156.578221 \nL 161.123637 157.618486 \nL 161.733584 158.011705 \nL 162.343531 156.510013 \nL 164.173373 153.970684 \nL 164.78332 151.915612 \nL 165.393267 152.329093 \nL 166.613162 151.934012 \nL 167.223109 152.340013 \nL 169.05295 149.992645 \nL 169.662897 149.814428 \nL 170.272845 149.055614 \nL 170.882792 148.884832 \nL 172.102686 149.69455 \nL 173.32258 149.353483 \nL 173.932528 149.748939 \nL 174.542475 149.580005 \nL 175.152422 150.526754 \nL 175.762369 150.909233 \nL 176.372316 151.838363 \nL 177.592211 152.576764 \nL 178.202158 151.314197 \nL 178.812105 151.14322 \nL 179.422052 148.829362 \nL 180.031999 149.740002 \nL 180.641946 148.518951 \nL 181.861841 150.315012 \nL 182.471788 149.109958 \nL 183.081735 148.43691 \nL 183.691682 148.287697 \nL 184.911577 150.040515 \nL 185.521524 149.885459 \nL 186.131471 149.225434 \nL 186.741418 150.084014 \nL 188.57126 151.119323 \nL 189.791154 152.776099 \nL 190.401101 152.613789 \nL 191.011048 153.426962 \nL 191.620995 152.778473 \nL 192.84089 152.460011 \nL 193.450837 151.347587 \nL 194.060784 150.721133 \nL 194.670731 151.520291 \nL 195.280678 151.369907 \nL 195.890626 152.158612 \nL 196.500573 152.006666 \nL 197.11052 152.785172 \nL 197.720467 152.169367 \nL 198.330414 152.019463 \nL 198.940361 152.787275 \nL 199.550309 153.092038 \nL 201.990097 152.492004 \nL 202.600044 151.8993 \nL 203.209992 152.199918 \nL 203.819939 152.055798 \nL 204.429886 152.793283 \nL 205.64978 152.503645 \nL 206.869675 153.084212 \nL 208.699516 151.373903 \nL 209.309463 151.238734 \nL 209.91941 150.681178 \nL 211.749252 151.545811 \nL 212.359199 150.579671 \nL 212.969146 150.036309 \nL 213.579093 150.736724 \nL 214.189041 149.785724 \nL 214.798988 150.481468 \nL 215.408935 149.131106 \nL 216.018882 148.604522 \nL 216.628829 149.296631 \nL 218.458671 150.140009 \nL 219.068618 150.018269 \nL 219.678565 150.294344 \nL 220.288512 150.172949 \nL 220.898459 150.840004 \nL 221.508407 150.325226 \nL 223.338248 149.967696 \nL 224.558142 150.503367 \nL 225.16809 150.001597 \nL 225.778037 150.649095 \nL 226.387984 149.389817 \nL 226.997931 150.034747 \nL 227.607878 149.919781 \nL 228.217825 150.557921 \nL 229.43772 151.073345 \nL 230.657614 150.840004 \nL 231.267561 151.093848 \nL 231.877508 150.242193 \nL 233.70735 149.907509 \nL 234.317297 149.434968 \nL 234.927244 150.049677 \nL 235.537191 149.220012 \nL 237.97698 150.218881 \nL 238.586927 149.756855 \nL 239.196874 150.002949 \nL 239.806822 149.896109 \nL 241.636663 150.622764 \nL 243.466505 150.30247 \nL 244.076452 149.854292 \nL 245.296346 150.329191 \nL 245.906293 149.546074 \nL 246.51624 150.121219 \nL 248.346082 149.814428 \nL 249.565976 148.947933 \nL 250.175923 148.187376 \nL 250.785871 148.092793 \nL 251.395818 147.340002 \nL 252.005765 146.920468 \nL 252.615712 147.485459 \nL 253.225659 147.720588 \nL 253.835606 147.303878 \nL 254.445554 147.213966 \nL 255.055501 146.801552 \nL 255.665448 146.713803 \nL 257.495289 147.410898 \nL 258.105237 147.004357 \nL 258.715184 147.551324 \nL 259.325131 147.462884 \nL 259.935078 148.005014 \nL 260.545025 148.229734 \nL 261.154972 148.766096 \nL 261.76492 148.674998 \nL 262.374867 149.206674 \nL 262.984814 149.424935 \nL 263.594761 148.714246 \nL 264.204708 149.24092 \nL 264.814655 148.535128 \nL 265.424603 148.446388 \nL 266.03455 148.969097 \nL 267.254444 148.790609 \nL 268.474338 149.217849 \nL 269.084286 149.128658 \nL 269.694233 149.34001 \nL 270.914127 148.56604 \nL 271.524074 148.777181 \nL 272.134021 149.283533 \nL 272.743969 149.491325 \nL 273.963863 149.315511 \nL 274.57381 149.814428 \nL 275.183757 149.141741 \nL 275.793704 149.638269 \nL 277.623546 149.377831 \nL 278.233493 149.580005 \nL 278.84344 149.206674 \nL 279.453387 148.262734 \nL 280.673282 148.668824 \nL 281.283229 149.154087 \nL 281.893176 149.353483 \nL 282.503123 149.83412 \nL 283.723018 149.664239 \nL 284.942912 150.054694 \nL 285.552859 149.691381 \nL 286.772753 149.524625 \nL 287.382701 149.165758 \nL 287.992648 149.359686 \nL 289.212542 150.292181 \nL 290.432436 150.669733 \nL 291.042384 150.585288 \nL 292.262278 150.958244 \nL 293.482172 150.78961 \nL 294.092119 150.437878 \nL 294.702067 150.355387 \nL 295.312014 150.540002 \nL 295.921961 150.45769 \nL 296.531908 150.110539 \nL 297.141855 150.294344 \nL 297.751802 149.949431 \nL 298.971697 149.790005 \nL 299.581644 149.972728 \nL 300.191591 149.893375 \nL 300.801538 149.293491 \nL 302.021433 149.139488 \nL 302.63138 148.546168 \nL 303.241327 148.471472 \nL 303.851274 148.140002 \nL 305.681116 147.921276 \nL 306.291063 148.358188 \nL 306.90101 148.285099 \nL 308.120904 148.645272 \nL 308.730851 148.572003 \nL 309.340799 148.247743 \nL 309.950746 148.175826 \nL 311.780587 147.215558 \nL 313.000482 147.573914 \nL 314.830323 147.367324 \nL 315.44027 146.318106 \nL 316.050217 146.252633 \nL 316.660165 146.431525 \nL 317.270112 146.366096 \nL 317.880059 146.786757 \nL 318.490006 146.963081 \nL 319.099953 146.896841 \nL 319.7099 147.072069 \nL 320.319848 147.005874 \nL 320.929795 147.180005 \nL 321.539742 147.113886 \nL 322.149689 147.525789 \nL 322.759636 147.220861 \nL 323.369583 147.155101 \nL 323.979531 147.326829 \nL 324.589478 146.551268 \nL 325.199425 146.487551 \nL 325.809372 146.65964 \nL 326.419319 146.595953 \nL 327.029266 147.001398 \nL 327.639214 146.937216 \nL 328.249161 146.640008 \nL 328.859108 146.576952 \nL 329.469055 146.746457 \nL 330.079002 146.451728 \nL 333.738685 147.454292 \nL 334.348632 147.390681 \nL 334.95858 147.554899 \nL 335.568527 147.491362 \nL 336.178474 147.654619 \nL 337.398368 148.42994 \nL 339.22821 148.908009 \nL 339.838157 148.618942 \nL 342.277946 149.247694 \nL 343.49784 149.116807 \nL 344.107787 148.831705 \nL 345.327681 148.703491 \nL 345.937629 148.858441 \nL 347.76747 148.667593 \nL 348.377417 148.821296 \nL 348.987365 148.75803 \nL 349.597312 148.263349 \nL 350.207259 147.986164 \nL 350.817206 147.280311 \nL 352.0371 146.733645 \nL 352.647048 146.248487 \nL 353.256995 146.404699 \nL 354.476889 145.866324 \nL 355.086836 146.022355 \nL 355.696783 145.966419 \nL 356.306731 146.121595 \nL 357.526625 146.010009 \nL 358.136572 146.373682 \nL 358.746519 146.526724 \nL 359.966414 145.997857 \nL 360.576361 145.942898 \nL 361.186308 145.680747 \nL 362.406202 145.572796 \nL 363.016149 145.931422 \nL 363.626097 145.877153 \nL 364.236044 145.617812 \nL 365.455938 145.920127 \nL 366.065885 145.866324 \nL 366.675832 146.016377 \nL 367.28578 145.962589 \nL 367.895727 145.503386 \nL 368.505674 145.65326 \nL 369.115621 146.004541 \nL 369.725568 146.152807 \nL 369.725568 146.152807 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 65.361932 18.54 \nL 65.971879 119.340005 \nL 66.581826 85.740008 \nL 67.191773 68.94001 \nL 67.80172 99.18001 \nL 68.411668 119.340005 \nL 69.631562 119.340005 \nL 70.241509 130.540004 \nL 70.851456 149.580005 \nL 71.461403 137.667275 \nL 72.071351 119.340005 \nL 72.681298 127.09385 \nL 73.291245 119.340005 \nL 73.901192 119.340005 \nL 74.511139 113.040011 \nL 75.121086 125.269416 \nL 75.731034 124.940004 \nL 76.340981 129.950529 \nL 78.170822 128.503647 \nL 78.780769 132.48783 \nL 79.390717 127.740012 \nL 80.000664 127.404006 \nL 80.610611 130.970773 \nL 81.220558 138.006675 \nL 81.830505 140.940003 \nL 83.0504 139.5 \nL 83.660347 142.101303 \nL 84.270294 141.390013 \nL 84.880241 143.776374 \nL 85.490188 143.05765 \nL 86.100135 148.140002 \nL 86.710083 147.340002 \nL 87.929977 151.171578 \nL 88.539924 145.186156 \nL 89.759819 148.842445 \nL 90.369766 152.940002 \nL 90.979713 152.158612 \nL 91.58966 153.703637 \nL 92.199607 150.700014 \nL 93.419502 153.654896 \nL 94.029449 150.840004 \nL 94.639396 152.254292 \nL 95.249343 149.580005 \nL 95.85929 150.963531 \nL 96.469237 148.416933 \nL 97.079185 151.672087 \nL 98.909026 149.940013 \nL 99.518973 147.634747 \nL 100.12892 143.671037 \nL 100.738868 146.675606 \nL 101.348815 146.220008 \nL 101.958762 149.084273 \nL 103.178656 144.940002 \nL 103.788603 144.54001 \nL 104.398551 142.601541 \nL 105.008498 143.776374 \nL 105.618445 146.420605 \nL 106.228392 147.504716 \nL 106.838339 147.096522 \nL 107.448286 145.260011 \nL 108.058234 147.734377 \nL 108.668181 147.340002 \nL 109.278128 145.575619 \nL 109.888075 146.583246 \nL 110.498022 146.220008 \nL 111.107969 147.192637 \nL 111.717917 146.830917 \nL 112.327864 149.063079 \nL 112.937811 147.410898 \nL 113.547758 147.060008 \nL 114.157705 145.473346 \nL 114.767652 147.613178 \nL 115.987547 146.94001 \nL 116.597494 147.801188 \nL 117.817388 147.1469 \nL 118.427335 147.97637 \nL 119.037283 149.919781 \nL 120.257177 147.03231 \nL 120.867124 147.826963 \nL 121.477071 146.436782 \nL 122.087018 146.148511 \nL 123.306913 147.690007 \nL 123.91686 145.319387 \nL 124.526807 144.025728 \nL 125.746701 147.564001 \nL 126.356649 147.284562 \nL 127.576543 148.699226 \nL 128.18649 150.355387 \nL 128.796437 150.060011 \nL 130.626279 152.006666 \nL 131.236226 151.70698 \nL 131.846173 150.496368 \nL 132.45612 150.215682 \nL 133.066067 149.040008 \nL 133.676015 149.669211 \nL 134.285962 151.171578 \nL 134.895909 150.018269 \nL 135.505856 148.015874 \nL 136.115803 149.493848 \nL 136.72575 149.238307 \nL 137.335698 148.140002 \nL 137.945645 148.740006 \nL 139.775486 148.022933 \nL 140.385433 146.165815 \nL 140.995381 145.951204 \nL 144.045116 148.804623 \nL 144.655064 150.118635 \nL 145.265011 150.649095 \nL 145.874958 151.929476 \nL 146.484905 151.686281 \nL 147.094852 150.700014 \nL 148.314747 147.299132 \nL 148.924694 148.557403 \nL 149.534641 148.347208 \nL 150.754535 149.365544 \nL 151.364482 149.154087 \nL 151.97443 147.535808 \nL 153.194324 147.1469 \nL 153.804271 145.575619 \nL 154.414218 146.768582 \nL 155.024165 147.26433 \nL 155.634113 146.400403 \nL 156.24406 146.892005 \nL 156.854007 148.044638 \nL 158.073901 146.351767 \nL 158.683848 147.485459 \nL 159.293796 147.303878 \nL 160.51369 144.379502 \nL 161.123637 143.583047 \nL 161.733584 143.430575 \nL 162.953479 144.383483 \nL 163.563426 144.228893 \nL 164.173373 142.839389 \nL 164.78332 143.310743 \nL 165.393267 144.387283 \nL 166.003214 144.236388 \nL 166.613162 144.690905 \nL 167.223109 143.340003 \nL 167.833056 142.601541 \nL 168.443003 141.278832 \nL 169.05295 141.150528 \nL 169.662897 141.609776 \nL 170.272845 140.898381 \nL 170.882792 141.933105 \nL 171.492739 141.228003 \nL 172.102686 139.958196 \nL 172.712633 139.841698 \nL 173.32258 140.29282 \nL 176.372316 139.720334 \nL 176.982263 138.513928 \nL 177.592211 139.5 \nL 178.812105 139.284398 \nL 179.422052 139.714476 \nL 180.031999 139.606675 \nL 181.251894 140.449948 \nL 181.861841 139.29 \nL 182.471788 140.231192 \nL 183.691682 141.050778 \nL 184.911577 140.830369 \nL 185.521524 141.230917 \nL 186.131471 142.133972 \nL 186.741418 142.020013 \nL 187.351365 141.405679 \nL 189.181207 141.081179 \nL 189.791154 139.5 \nL 190.401101 139.891461 \nL 191.011048 139.305231 \nL 191.620995 139.693853 \nL 192.230943 139.596461 \nL 194.060784 140.736237 \nL 195.280678 142.42038 \nL 195.890626 142.31303 \nL 196.500573 141.740003 \nL 197.11052 142.565808 \nL 198.330414 143.274258 \nL 198.940361 142.707285 \nL 199.550309 143.513758 \nL 200.160256 143.404876 \nL 200.770203 142.844946 \nL 201.38015 143.190009 \nL 202.600044 144.763017 \nL 203.209992 143.762916 \nL 204.429886 143.549612 \nL 205.039833 143.882613 \nL 206.869675 143.566615 \nL 207.479622 143.893849 \nL 208.089569 143.789367 \nL 209.309463 144.433681 \nL 209.91941 143.481179 \nL 210.529358 142.95841 \nL 211.139305 143.700011 \nL 211.749252 143.598924 \nL 212.359199 144.331738 \nL 212.969146 144.643711 \nL 213.579093 144.54001 \nL 214.189041 144.025728 \nL 214.798988 144.335132 \nL 215.408935 144.23394 \nL 216.018882 143.727108 \nL 216.628829 144.033989 \nL 217.238776 143.128801 \nL 219.678565 142.754178 \nL 220.288512 143.05765 \nL 220.898459 143.752507 \nL 221.508407 143.657518 \nL 222.728301 144.248119 \nL 223.948195 144.057255 \nL 224.558142 143.193449 \nL 225.16809 143.86929 \nL 225.778037 143.394557 \nL 226.387984 144.06454 \nL 226.997931 143.592645 \nL 227.607878 142.746743 \nL 228.217825 142.659415 \nL 229.43772 143.233343 \nL 230.047667 143.889087 \nL 230.657614 143.428247 \nL 231.267561 143.340003 \nL 231.877508 143.620298 \nL 232.487456 143.165466 \nL 233.097403 143.079144 \nL 233.70735 143.357336 \nL 234.317297 142.545756 \nL 235.537191 142.380006 \nL 236.757086 141.501705 \nL 237.367033 141.423403 \nL 237.97698 140.63578 \nL 239.196874 141.191759 \nL 239.806822 140.413178 \nL 242.24661 140.123511 \nL 242.856557 140.397542 \nL 243.466505 140.32567 \nL 244.076452 140.597148 \nL 245.296346 140.453523 \nL 245.906293 140.721832 \nL 246.51624 140.311821 \nL 247.126188 140.915925 \nL 247.736135 141.180013 \nL 248.346082 141.10745 \nL 248.956029 141.702917 \nL 249.565976 141.296436 \nL 250.175923 141.555793 \nL 250.785871 140.821973 \nL 251.395818 140.422356 \nL 252.005765 140.353683 \nL 253.225659 140.870098 \nL 253.835606 140.800659 \nL 255.055501 141.955395 \nL 256.275395 141.81135 \nL 256.885342 141.420009 \nL 257.495289 141.669122 \nL 258.105237 142.234639 \nL 259.325131 142.091104 \nL 259.935078 142.335005 \nL 260.545025 141.949357 \nL 261.154972 141.879136 \nL 261.76492 142.433508 \nL 262.374867 142.673339 \nL 264.814655 144.847327 \nL 265.424603 145.076177 \nL 266.644497 144.920671 \nL 267.254444 145.147239 \nL 267.864391 144.464337 \nL 268.474338 144.690905 \nL 269.694233 144.54001 \nL 270.30418 145.063455 \nL 270.914127 145.285576 \nL 271.524074 145.209032 \nL 272.134021 144.836482 \nL 272.743969 145.352912 \nL 273.353916 145.276849 \nL 273.963863 145.788984 \nL 274.57381 145.712095 \nL 275.183757 145.927832 \nL 276.403652 146.936555 \nL 277.013599 147.1469 \nL 277.623546 146.778406 \nL 278.233493 146.988 \nL 279.453387 147.97637 \nL 280.063335 147.609693 \nL 280.673282 147.529845 \nL 281.283229 148.018322 \nL 281.893176 147.37147 \nL 282.503123 147.292943 \nL 284.942912 148.100123 \nL 285.552859 148.020665 \nL 286.162806 148.21934 \nL 287.382701 149.165758 \nL 287.992648 149.359686 \nL 288.602595 149.277885 \nL 289.212542 149.470446 \nL 289.822489 148.842445 \nL 290.432436 148.490277 \nL 291.042384 148.955098 \nL 292.262278 148.255824 \nL 292.872225 148.178514 \nL 293.482172 147.832806 \nL 294.092119 148.025111 \nL 295.312014 147.873346 \nL 295.921961 148.064014 \nL 297.141855 147.384102 \nL 297.751802 147.57456 \nL 298.36175 147.237653 \nL 298.971697 147.165 \nL 299.581644 147.616377 \nL 300.191591 147.281979 \nL 300.801538 147.209776 \nL 301.411485 147.657533 \nL 302.021433 147.584744 \nL 305.071168 148.505493 \nL 305.681116 148.176457 \nL 306.291063 147.594552 \nL 307.510957 147.452565 \nL 308.730851 147.816013 \nL 309.340799 147.744996 \nL 309.950746 147.925076 \nL 310.560693 147.60403 \nL 311.17064 147.783569 \nL 311.780587 147.713334 \nL 312.390534 147.89173 \nL 313.000482 147.573914 \nL 313.610429 147.998826 \nL 314.220376 147.435847 \nL 314.830323 147.613178 \nL 316.050217 148.454574 \nL 316.660165 148.140002 \nL 317.270112 147.340002 \nL 317.880059 147.272531 \nL 318.490006 147.447698 \nL 319.099953 147.138564 \nL 319.7099 147.313206 \nL 320.319848 147.727603 \nL 320.929795 147.420001 \nL 321.539742 147.592735 \nL 322.149689 147.525789 \nL 325.809372 148.54375 \nL 326.419319 148.475677 \nL 327.029266 148.876751 \nL 327.639214 149.042096 \nL 328.249161 149.44 \nL 328.859108 149.370486 \nL 329.469055 149.069042 \nL 330.079002 149.232419 \nL 331.908844 149.027676 \nL 332.518791 149.189671 \nL 333.128738 148.892733 \nL 334.95858 149.375217 \nL 335.568527 148.399464 \nL 336.788421 148.72117 \nL 338.008315 148.590012 \nL 338.618263 148.749364 \nL 339.22821 148.460011 \nL 341.058051 148.267165 \nL 341.667998 147.981417 \nL 342.277946 148.140002 \nL 342.887893 148.518951 \nL 344.107787 148.831705 \nL 344.717734 148.54785 \nL 345.327681 148.922608 \nL 346.547576 148.794545 \nL 347.76747 149.102072 \nL 348.377417 148.604522 \nL 348.987365 148.325414 \nL 350.207259 148.63231 \nL 350.817206 148.569855 \nL 352.647048 149.665426 \nL 354.476889 149.473901 \nL 355.086836 149.198833 \nL 357.526625 149.790005 \nL 358.136572 149.517145 \nL 358.746519 149.872782 \nL 359.356466 149.809576 \nL 360.576361 150.09959 \nL 361.186308 150.036309 \nL 361.796255 150.38723 \nL 362.406202 150.323618 \nL 363.016149 150.466387 \nL 363.626097 149.785724 \nL 364.236044 150.134302 \nL 365.455938 150.009377 \nL 366.065885 150.15135 \nL 366.675832 150.08909 \nL 367.28578 150.433553 \nL 367.895727 150.573813 \nL 368.505674 150.511088 \nL 369.115621 150.650627 \nL 369.725568 150.588007 \nL 369.725568 150.588007 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 65.361932 18.54 \nL 65.971879 119.340005 \nL 66.581826 186.539998 \nL 67.191773 169.74 \nL 67.80172 179.820005 \nL 68.411668 169.74 \nL 69.021615 191.339998 \nL 69.631562 194.940005 \nL 70.241509 186.539998 \nL 70.851456 179.820005 \nL 71.461403 192.649098 \nL 72.681298 196.878466 \nL 73.291245 198.540005 \nL 73.901192 186.539998 \nL 75.121086 178.634117 \nL 75.731034 169.74 \nL 76.950928 174.78001 \nL 77.560875 176.939999 \nL 78.170822 174.321829 \nL 79.390717 178.140007 \nL 80.610611 181.370768 \nL 81.220558 186.539998 \nL 81.830505 180.540006 \nL 83.0504 176.460008 \nL 83.660347 171.36582 \nL 84.880241 174.321829 \nL 85.490188 169.74 \nL 86.100135 168.300012 \nL 87.929977 172.392642 \nL 88.539924 173.616923 \nL 89.149871 169.74 \nL 89.759819 170.969267 \nL 90.979713 168.567915 \nL 92.199607 170.860009 \nL 93.419502 164.378299 \nL 94.639396 166.65429 \nL 95.249343 161.675999 \nL 95.85929 154.916472 \nL 96.469237 150.355387 \nL 98.299079 154.161819 \nL 98.909026 155.340001 \nL 99.518973 149.40317 \nL 100.12892 150.622764 \nL 100.738868 150.092545 \nL 101.348815 151.260003 \nL 101.958762 150.736724 \nL 102.568709 151.856132 \nL 103.178656 149.740002 \nL 105.618445 153.942987 \nL 106.228392 156.398832 \nL 106.838339 155.861749 \nL 107.448286 158.220007 \nL 108.058234 159.09212 \nL 108.668181 157.140013 \nL 109.278128 158.003023 \nL 109.888075 156.118387 \nL 110.498022 156.97201 \nL 111.107969 159.129476 \nL 111.717917 158.61273 \nL 112.327864 160.693847 \nL 112.937811 158.894437 \nL 113.547758 158.400011 \nL 114.157705 159.162235 \nL 115.3776 155.773745 \nL 115.987547 157.740001 \nL 116.597494 158.474122 \nL 117.207441 160.363264 \nL 117.817388 159.891729 \nL 118.427335 158.285466 \nL 119.64723 159.66001 \nL 120.257177 159.216924 \nL 120.867124 159.879142 \nL 121.477071 157.275496 \nL 122.087018 156.871914 \nL 122.696966 158.598956 \nL 123.91686 157.789493 \nL 125.746701 159.66001 \nL 126.356649 159.260799 \nL 128.18649 161.016935 \nL 128.796437 162.540001 \nL 129.406384 161.181513 \nL 130.016332 161.732536 \nL 130.626279 161.340008 \nL 131.846173 162.409098 \nL 132.45612 161.112975 \nL 133.066067 161.64001 \nL 133.676015 161.265673 \nL 134.285962 162.666322 \nL 134.895909 163.166095 \nL 135.505856 162.788287 \nL 136.115803 164.140001 \nL 136.72575 162.051869 \nL 137.335698 161.692942 \nL 137.945645 162.180008 \nL 139.165539 161.477715 \nL 139.775486 160.315619 \nL 140.385433 159.985171 \nL 142.825222 161.865 \nL 143.435169 161.535348 \nL 144.045116 161.986155 \nL 145.265011 161.340008 \nL 147.094852 162.646675 \nL 147.704799 162.328244 \nL 148.314747 162.750226 \nL 148.924694 160.244347 \nL 149.534641 159.2249 \nL 150.144588 159.66001 \nL 150.754535 160.803841 \nL 151.364482 160.51183 \nL 151.97443 160.928811 \nL 152.584377 162.040003 \nL 154.414218 163.225726 \nL 155.024165 164.291361 \nL 156.24406 165.036011 \nL 156.854007 164.06583 \nL 157.463954 163.771581 \nL 158.073901 164.798824 \nL 158.683848 163.849101 \nL 159.293796 163.561942 \nL 160.51369 164.282679 \nL 161.733584 163.717357 \nL 162.953479 164.418268 \nL 163.563426 164.140001 \nL 164.173373 165.101965 \nL 164.78332 165.437565 \nL 166.003214 167.311086 \nL 166.613162 166.420243 \nL 167.223109 166.740012 \nL 167.833056 167.652439 \nL 169.05295 168.266321 \nL 169.662897 167.395816 \nL 172.102686 168.594547 \nL 172.712633 169.455259 \nL 173.32258 169.74 \nL 173.932528 170.584701 \nL 174.542475 170.860009 \nL 175.152422 168.904642 \nL 175.762369 169.74 \nL 176.372316 170.015414 \nL 176.982263 169.74 \nL 177.592211 167.832984 \nL 178.202158 168.65613 \nL 178.812105 167.853381 \nL 179.422052 168.667666 \nL 180.031999 168.406672 \nL 180.641946 168.678946 \nL 181.251894 168.420641 \nL 181.861841 167.640002 \nL 183.691682 168.447692 \nL 184.301629 168.197153 \nL 184.911577 167.437467 \nL 186.741418 168.228005 \nL 187.351365 168.987765 \nL 187.961312 169.240993 \nL 188.57126 168.995185 \nL 189.181207 168.257655 \nL 190.401101 168.761363 \nL 191.011048 168.035654 \nL 191.620995 168.286163 \nL 192.230943 167.569677 \nL 193.450837 167.112517 \nL 194.060784 167.362651 \nL 194.670731 166.663948 \nL 195.890626 167.161408 \nL 197.11052 166.72065 \nL 197.720467 166.965693 \nL 198.330414 166.287959 \nL 199.550309 165.863077 \nL 200.160256 166.107579 \nL 200.770203 166.801895 \nL 201.38015 166.590003 \nL 201.990097 166.828001 \nL 202.600044 166.171866 \nL 203.819939 167.529482 \nL 204.429886 167.319047 \nL 205.039833 167.986958 \nL 205.64978 167.776372 \nL 206.869675 168.225842 \nL 208.089569 167.809792 \nL 208.699516 168.031538 \nL 209.309463 167.400773 \nL 210.529358 167.842101 \nL 211.139305 166.380003 \nL 211.749252 166.184814 \nL 212.969146 167.458526 \nL 213.579093 167.261324 \nL 214.189041 167.888574 \nL 214.798988 167.281466 \nL 216.018882 168.520646 \nL 216.628829 168.323144 \nL 218.458671 168.94 \nL 219.068618 168.743954 \nL 220.288512 169.147071 \nL 220.898459 168.165009 \nL 221.508407 167.975031 \nL 222.118354 168.177221 \nL 222.728301 167.599462 \nL 223.338248 168.189237 \nL 224.558142 168.585805 \nL 225.16809 168.398561 \nL 225.778037 167.830911 \nL 226.387984 168.408685 \nL 226.997931 167.84527 \nL 227.607878 168.418658 \nL 228.827773 168.80321 \nL 229.43772 168.620006 \nL 230.047667 167.694256 \nL 230.657614 167.145892 \nL 231.267561 167.709231 \nL 231.877508 167.532712 \nL 232.487456 166.990912 \nL 233.70735 166.64687 \nL 234.317297 167.201873 \nL 234.927244 167.030325 \nL 235.537191 167.580011 \nL 236.147139 167.767059 \nL 236.757086 168.310226 \nL 237.367033 167.424805 \nL 237.97698 167.255496 \nL 238.586927 166.733688 \nL 239.196874 166.567983 \nL 239.806822 166.754641 \nL 240.416769 167.290012 \nL 241.026716 166.775294 \nL 241.636663 166.959309 \nL 242.24661 167.488462 \nL 242.856557 167.323568 \nL 243.466505 167.503827 \nL 244.076452 168.025725 \nL 244.686399 168.20238 \nL 246.51624 167.710478 \nL 247.126188 166.537327 \nL 247.736135 166.716009 \nL 248.346082 166.558611 \nL 249.565976 166.91228 \nL 250.785871 166.600337 \nL 252.005765 166.949125 \nL 252.615712 166.794551 \nL 253.225659 166.967195 \nL 253.835606 166.813551 \nL 254.445554 166.33679 \nL 255.055501 166.186165 \nL 255.665448 166.680576 \nL 256.275395 166.850832 \nL 256.885342 166.700012 \nL 257.495289 166.869127 \nL 258.105237 166.401212 \nL 258.715184 166.570191 \nL 259.325131 166.106152 \nL 259.935078 165.330004 \nL 260.545025 165.814772 \nL 261.154972 165.357402 \nL 261.76492 165.214933 \nL 262.374867 165.695555 \nL 262.984814 165.552937 \nL 263.594761 166.029578 \nL 264.814655 165.130248 \nL 265.424603 165.603841 \nL 266.03455 164.852732 \nL 266.644497 165.019759 \nL 267.254444 164.882172 \nL 268.474338 165.816649 \nL 269.084286 165.677921 \nL 269.694233 166.140008 \nL 270.30418 166.00113 \nL 270.914127 166.459536 \nL 271.524074 166.320538 \nL 272.134021 165.589421 \nL 272.743969 165.453787 \nL 273.353916 165.02422 \nL 273.963863 165.184908 \nL 274.57381 164.758614 \nL 275.183757 164.626961 \nL 275.793704 164.204753 \nL 276.403652 164.075458 \nL 277.013599 163.657246 \nL 277.623546 163.81909 \nL 278.233493 163.116002 \nL 278.84344 162.704113 \nL 280.673282 163.190848 \nL 281.283229 163.067336 \nL 281.893176 163.227648 \nL 283.723018 162.860896 \nL 285.552859 163.335585 \nL 286.162806 162.381326 \nL 287.382701 162.69782 \nL 288.602595 162.461534 \nL 289.212542 161.796522 \nL 289.822489 161.954642 \nL 291.652331 161.610976 \nL 292.262278 161.227401 \nL 293.482172 161.004003 \nL 294.702067 161.317718 \nL 295.312014 161.206673 \nL 295.921961 161.362163 \nL 296.531908 161.251583 \nL 297.141855 161.406143 \nL 298.36175 162.239218 \nL 298.971697 161.602504 \nL 299.581644 161.230915 \nL 300.191591 161.383522 \nL 301.411485 162.205993 \nL 302.021433 161.836672 \nL 302.63138 161.986155 \nL 303.241327 161.877092 \nL 304.461221 162.173594 \nL 306.90101 160.726411 \nL 308.730851 161.172006 \nL 309.340799 161.067689 \nL 309.950746 160.713133 \nL 311.780587 161.153335 \nL 312.390534 161.546899 \nL 313.000482 161.690869 \nL 314.830323 160.643424 \nL 315.44027 161.033443 \nL 316.050217 160.687584 \nL 316.660165 160.587458 \nL 317.270112 160.974788 \nL 317.880059 160.874467 \nL 319.099953 161.158712 \nL 320.319848 160.477959 \nL 320.929795 160.860002 \nL 322.149689 160.663236 \nL 322.759636 160.803841 \nL 323.369583 160.706044 \nL 323.979531 160.845883 \nL 325.809372 160.554954 \nL 326.419319 160.22392 \nL 327.029266 160.128841 \nL 328.249161 160.87334 \nL 328.859108 160.77742 \nL 329.469055 160.914196 \nL 330.079002 160.586902 \nL 330.688949 160.492304 \nL 331.298897 160.859462 \nL 332.518791 161.129528 \nL 334.348632 160.845883 \nL 335.568527 161.112975 \nL 336.788421 160.473633 \nL 337.398368 160.607119 \nL 338.008315 160.96501 \nL 339.22821 160.332006 \nL 340.448104 160.59665 \nL 341.058051 160.505567 \nL 341.667998 160.858951 \nL 342.277946 160.546167 \nL 342.887893 160.676844 \nL 343.49784 161.02754 \nL 344.717734 160.845883 \nL 345.937629 161.103137 \nL 346.547576 161.01273 \nL 347.76747 161.267595 \nL 348.377417 161.610976 \nL 348.987365 161.736576 \nL 350.207259 161.555385 \nL 350.817206 161.895236 \nL 351.427153 161.804694 \nL 352.0371 162.142547 \nL 353.256995 162.387784 \nL 353.866942 162.296971 \nL 354.476889 162.418741 \nL 355.086836 162.328244 \nL 355.696783 162.0268 \nL 356.306731 161.937489 \nL 356.916678 162.059003 \nL 358.136572 161.881373 \nL 359.356466 162.122615 \nL 359.966414 161.409432 \nL 360.576361 161.738364 \nL 361.186308 161.858527 \nL 361.796255 161.771213 \nL 363.016149 161.185404 \nL 363.626097 161.511436 \nL 364.845991 161.749764 \nL 366.675832 160.881827 \nL 367.895727 161.525915 \nL 369.115621 161.760849 \nL 369.725568 161.675999 \nL 369.725568 161.675999 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 65.361932 220.140003 \nL 65.971879 169.74 \nL 66.581826 85.740008 \nL 67.191773 94.140015 \nL 67.80172 79.020015 \nL 68.411668 68.94001 \nL 69.021615 76.140009 \nL 69.631562 106.740003 \nL 70.241509 119.340005 \nL 70.851456 119.340005 \nL 71.461403 101.012735 \nL 72.071351 110.940013 \nL 73.291245 112.140006 \nL 73.901192 112.620012 \nL 74.511139 125.640014 \nL 75.121086 131.198827 \nL 75.731034 130.540004 \nL 76.340981 135.255799 \nL 76.950928 129.42001 \nL 77.560875 133.740004 \nL 78.170822 142.249103 \nL 78.780769 136.870444 \nL 80.000664 135.468007 \nL 80.610611 127.09385 \nL 81.830505 133.740004 \nL 82.440452 140.195173 \nL 83.0504 139.5 \nL 83.660347 145.352912 \nL 85.490188 151.951766 \nL 87.32003 149.307581 \nL 87.929977 148.518951 \nL 89.149871 152.100003 \nL 89.759819 151.300979 \nL 90.369766 152.940002 \nL 90.979713 152.158612 \nL 91.58966 146.830917 \nL 92.199607 148.460011 \nL 92.809554 152.209576 \nL 93.419502 151.510213 \nL 94.029449 155.04 \nL 94.639396 156.368581 \nL 95.85929 162.822354 \nL 96.469237 163.924623 \nL 97.079185 159.279634 \nL 97.689132 158.540001 \nL 98.299079 154.161819 \nL 99.518973 160.013695 \nL 100.12892 161.050356 \nL 101.348815 156.300013 \nL 101.958762 155.694107 \nL 102.568709 158.359366 \nL 103.178656 159.340001 \nL 104.398551 158.109232 \nL 105.008498 159.049102 \nL 105.618445 155.447472 \nL 106.228392 153.434127 \nL 106.838339 152.940002 \nL 107.448286 153.900014 \nL 108.058234 151.993523 \nL 109.278128 156.6222 \nL 110.498022 155.628002 \nL 111.107969 153.824221 \nL 111.717917 155.994559 \nL 112.327864 154.232309 \nL 112.937811 155.066586 \nL 113.547758 150.840004 \nL 114.157705 150.451111 \nL 115.3776 154.559288 \nL 115.987547 151.740009 \nL 116.597494 152.544711 \nL 117.207441 154.502796 \nL 118.427335 153.703637 \nL 119.037283 154.45012 \nL 119.64723 154.060011 \nL 120.257177 155.893848 \nL 120.867124 156.592175 \nL 123.306913 155.04 \nL 124.526807 156.368581 \nL 125.746701 155.628002 \nL 126.966596 156.892942 \nL 127.576543 155.549716 \nL 128.18649 155.201544 \nL 128.796437 152.940002 \nL 129.406384 153.573966 \nL 130.626279 152.940002 \nL 131.236226 154.481287 \nL 132.45612 153.848118 \nL 133.066067 152.64 \nL 133.676015 153.237345 \nL 134.285962 152.055798 \nL 134.895909 151.771312 \nL 136.115803 152.940002 \nL 136.72575 152.655261 \nL 137.335698 153.222355 \nL 137.945645 151.260003 \nL 138.555592 150.163141 \nL 139.165539 149.910498 \nL 139.775486 148.842445 \nL 140.385433 148.604522 \nL 140.995381 149.176813 \nL 141.605328 148.940002 \nL 142.215275 150.294344 \nL 143.435169 149.814428 \nL 144.045116 150.355387 \nL 144.655064 151.657562 \nL 145.265011 149.885459 \nL 147.094852 149.206674 \nL 148.314747 151.713724 \nL 149.534641 152.698279 \nL 150.144588 151.740009 \nL 152.584377 150.840004 \nL 153.194324 151.317937 \nL 154.414218 153.625727 \nL 155.024165 152.712984 \nL 155.634113 153.165518 \nL 156.24406 152.268005 \nL 156.854007 153.38504 \nL 157.463954 153.824221 \nL 158.073901 154.916472 \nL 158.683848 155.340001 \nL 159.293796 156.4084 \nL 159.903743 156.170778 \nL 160.51369 157.220266 \nL 161.123637 156.342536 \nL 161.733584 157.37774 \nL 162.343531 157.770012 \nL 163.563426 157.295563 \nL 164.78332 158.061963 \nL 166.003214 156.380973 \nL 169.05295 158.245271 \nL 169.662897 158.01908 \nL 171.492739 159.084009 \nL 172.102686 160.003646 \nL 174.542475 159.100006 \nL 175.152422 159.437243 \nL 175.762369 160.324616 \nL 176.982263 159.879142 \nL 179.422052 161.161281 \nL 180.031999 160.406673 \nL 180.641946 160.190529 \nL 181.251894 159.448914 \nL 181.861841 159.24001 \nL 182.471788 159.555543 \nL 183.081735 158.828663 \nL 183.691682 158.626158 \nL 184.301629 157.911429 \nL 184.911577 158.227322 \nL 186.131471 157.836492 \nL 186.741418 156.636004 \nL 187.351365 155.948957 \nL 187.961312 155.767737 \nL 189.181207 156.398832 \nL 189.791154 157.201476 \nL 190.401101 157.50699 \nL 191.011048 158.296521 \nL 191.620995 158.593849 \nL 192.230943 159.370628 \nL 192.84089 157.740001 \nL 193.450837 157.558014 \nL 194.060784 157.85321 \nL 194.670731 156.725916 \nL 195.280678 157.022253 \nL 195.890626 156.378149 \nL 196.500573 157.140013 \nL 197.11052 155.572261 \nL 197.720467 155.868448 \nL 198.330414 155.241378 \nL 198.940361 155.536363 \nL 199.550309 154.916472 \nL 200.770203 154.597409 \nL 201.38015 155.340001 \nL 202.600044 155.913464 \nL 203.209992 155.752341 \nL 203.819939 155.150534 \nL 205.039833 154.839132 \nL 206.259727 155.402081 \nL 206.869675 154.814679 \nL 207.479622 154.663078 \nL 208.089569 154.083833 \nL 208.699516 153.936618 \nL 209.309463 154.641269 \nL 210.529358 155.189377 \nL 211.139305 154.62 \nL 211.749252 155.310126 \nL 212.359199 155.578029 \nL 212.969146 154.599272 \nL 213.579093 154.041641 \nL 214.798988 154.579024 \nL 215.408935 155.252553 \nL 216.018882 155.107741 \nL 216.628829 155.368916 \nL 217.238776 156.031209 \nL 217.848724 155.483431 \nL 219.068618 156.79139 \nL 220.288512 157.288249 \nL 220.898459 156.35251 \nL 221.508407 156.600706 \nL 222.118354 156.06559 \nL 222.728301 156.312976 \nL 223.338248 155.007706 \nL 223.948195 154.871035 \nL 225.16809 155.367384 \nL 225.778037 155.230908 \nL 226.387984 154.334718 \nL 227.607878 154.827642 \nL 228.217825 153.942987 \nL 228.827773 153.439639 \nL 229.43772 152.56667 \nL 231.877508 155.024679 \nL 232.487456 154.894918 \nL 233.097403 155.131308 \nL 233.70735 155.7299 \nL 234.317297 155.236406 \nL 234.927244 155.107741 \nL 235.537191 154.62 \nL 236.147139 154.49446 \nL 236.757086 154.012351 \nL 237.367033 153.88983 \nL 238.586927 155.06211 \nL 239.196874 154.584762 \nL 239.806822 154.461956 \nL 240.416769 153.640011 \nL 241.026716 153.870108 \nL 241.636663 153.403455 \nL 243.466505 153.054682 \nL 244.076452 152.597147 \nL 244.686399 153.167801 \nL 245.906293 153.618787 \nL 246.51624 153.503761 \nL 247.126188 153.726633 \nL 247.736135 153.276007 \nL 248.346082 153.163264 \nL 248.956029 152.71749 \nL 249.565976 152.940002 \nL 250.175923 152.829482 \nL 250.785871 153.050176 \nL 251.395818 153.598825 \nL 253.225659 154.244866 \nL 253.835606 154.132259 \nL 255.055501 155.201544 \nL 255.665448 154.764932 \nL 256.275395 154.973129 \nL 256.885342 154.86001 \nL 257.495289 155.066586 \nL 258.105237 154.953888 \nL 259.325131 155.362577 \nL 259.935078 155.880014 \nL 260.545025 155.452159 \nL 261.154972 155.966096 \nL 262.374867 155.11779 \nL 262.984814 155.317846 \nL 263.594761 154.589088 \nL 264.204708 153.556528 \nL 264.814655 154.06683 \nL 265.424603 153.654896 \nL 266.03455 153.55091 \nL 269.084286 154.544778 \nL 270.30418 154.33586 \nL 271.524074 154.724076 \nL 272.743969 154.51654 \nL 273.353916 154.708425 \nL 273.963863 154.60531 \nL 275.183757 154.98522 \nL 275.793704 154.590876 \nL 277.013599 154.967586 \nL 278.233493 154.764 \nL 278.84344 155.237442 \nL 279.453387 155.135454 \nL 280.063335 154.748499 \nL 280.673282 154.93322 \nL 281.283229 154.549014 \nL 281.893176 155.016418 \nL 282.503123 154.351766 \nL 283.11307 154.53554 \nL 284.332965 154.340005 \nL 284.942912 153.684606 \nL 285.552859 154.146633 \nL 286.162806 154.32844 \nL 286.772753 154.232309 \nL 287.382701 154.689049 \nL 287.992648 154.867881 \nL 288.602595 154.771075 \nL 289.212542 154.400883 \nL 289.822489 154.579024 \nL 290.432436 154.483795 \nL 291.042384 154.660765 \nL 292.262278 155.552344 \nL 292.872225 155.725034 \nL 293.482172 155.628002 \nL 294.092119 155.79958 \nL 294.702067 155.702879 \nL 295.312014 155.873345 \nL 295.921961 155.776944 \nL 296.531908 156.211588 \nL 297.141855 156.114812 \nL 297.751802 156.282409 \nL 298.36175 156.185964 \nL 298.971697 156.615006 \nL 299.581644 156.518184 \nL 300.191591 156.683018 \nL 300.801538 156.586512 \nL 301.411485 155.970932 \nL 302.021433 156.135901 \nL 302.63138 156.041543 \nL 303.241327 156.205475 \nL 303.851274 155.854299 \nL 304.461221 155.761383 \nL 305.071168 156.180616 \nL 306.291063 156.503645 \nL 306.90101 156.917846 \nL 307.510957 155.810364 \nL 308.120904 155.971593 \nL 308.730851 155.376005 \nL 309.340799 155.537519 \nL 309.950746 155.196722 \nL 310.560693 155.357876 \nL 313.000482 155.003891 \nL 313.610429 154.669417 \nL 314.220376 155.075943 \nL 315.44027 154.902053 \nL 316.050217 154.571079 \nL 317.270112 154.887828 \nL 317.880059 154.559288 \nL 320.319848 154.223057 \nL 320.929795 153.900014 \nL 321.539742 154.057352 \nL 322.149689 153.736216 \nL 322.759636 153.1783 \nL 323.979531 153.019069 \nL 324.589478 153.413248 \nL 325.199425 153.56952 \nL 325.809372 152.782994 \nL 326.419319 152.940002 \nL 327.029266 152.627443 \nL 327.639214 153.017972 \nL 328.249161 152.940002 \nL 328.859108 153.095207 \nL 329.469055 152.785172 \nL 330.079002 153.171736 \nL 330.688949 153.325334 \nL 331.298897 153.247559 \nL 331.908844 153.400286 \nL 332.518791 152.863473 \nL 334.348632 153.320092 \nL 334.95858 153.243398 \nL 335.568527 153.621085 \nL 336.178474 153.544046 \nL 336.788421 153.015344 \nL 338.008315 152.41501 \nL 339.22821 152.716003 \nL 339.838157 153.089004 \nL 340.448104 153.014337 \nL 341.058051 153.162528 \nL 341.667998 152.865996 \nL 342.277946 153.013857 \nL 342.887893 153.382111 \nL 343.49784 153.307625 \nL 344.717734 153.598825 \nL 345.937629 153.012896 \nL 347.157523 153.302864 \nL 347.76747 152.795175 \nL 348.377417 152.723228 \nL 348.987365 152.435287 \nL 349.597312 152.364421 \nL 351.427153 152.797023 \nL 352.0371 152.725991 \nL 352.647048 152.86882 \nL 355.086836 152.587068 \nL 355.696783 152.72868 \nL 356.306731 152.237078 \nL 356.916678 152.168405 \nL 358.136572 151.612772 \nL 358.746519 151.127554 \nL 359.356466 151.270442 \nL 360.576361 151.970106 \nL 361.796255 152.250071 \nL 362.406202 152.59575 \nL 363.016149 152.733877 \nL 364.236044 152.597853 \nL 365.455938 152.05401 \nL 366.675832 152.329093 \nL 367.28578 151.856132 \nL 367.895727 151.790718 \nL 368.505674 151.927959 \nL 369.115621 151.660642 \nL 369.725568 151.797612 \nL 369.725568 151.797612 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 65.361932 220.140003 \nL 65.971879 119.340005 \nL 66.581826 152.940002 \nL 67.191773 194.940005 \nL 67.80172 179.820005 \nL 68.411668 136.140003 \nL 69.021615 148.140002 \nL 69.631562 144.54001 \nL 70.241509 152.940002 \nL 70.851456 129.42001 \nL 72.071351 144.54001 \nL 72.681298 150.355387 \nL 73.291245 148.140002 \nL 74.511139 169.74 \nL 75.121086 166.775294 \nL 76.340981 172.392642 \nL 76.950928 174.78001 \nL 77.560875 172.14 \nL 78.780769 176.31392 \nL 79.390717 182.340003 \nL 80.000664 183.851998 \nL 80.610611 177.493845 \nL 81.220558 164.140001 \nL 81.830505 162.540001 \nL 82.440452 164.526219 \nL 83.0504 163.020007 \nL 83.660347 158.359366 \nL 84.880241 162.103644 \nL 86.100135 159.66001 \nL 87.32003 152.0319 \nL 87.929977 153.824221 \nL 88.539924 152.940002 \nL 89.149871 154.62 \nL 90.369766 152.940002 \nL 90.979713 147.470244 \nL 92.199607 150.700014 \nL 92.809554 150.018269 \nL 93.419502 151.510213 \nL 94.029449 148.740006 \nL 94.639396 150.197147 \nL 95.85929 148.987061 \nL 96.469237 152.293855 \nL 97.079185 147.868314 \nL 98.299079 150.496368 \nL 98.909026 149.940013 \nL 99.518973 152.940002 \nL 100.12892 150.622764 \nL 102.568709 148.604522 \nL 103.178656 151.340002 \nL 103.788603 152.41501 \nL 104.398551 151.906165 \nL 105.008498 149.885459 \nL 105.618445 152.438517 \nL 106.228392 151.951766 \nL 106.838339 152.940002 \nL 107.448286 155.340001 \nL 108.668181 151.540013 \nL 109.278128 151.09891 \nL 109.888075 147.945413 \nL 110.498022 143.532008 \nL 111.717917 140.28546 \nL 112.327864 136.140003 \nL 112.937811 135.927345 \nL 114.157705 140.495565 \nL 114.767652 140.237575 \nL 115.3776 138.771332 \nL 115.987547 139.74001 \nL 117.207441 132.233025 \nL 118.427335 131.940008 \nL 119.64723 133.900001 \nL 120.257177 133.740004 \nL 120.867124 132.48783 \nL 121.477071 134.514199 \nL 122.087018 135.425109 \nL 122.696966 133.133691 \nL 123.306913 134.040005 \nL 123.91686 133.888465 \nL 124.526807 134.768583 \nL 126.356649 134.310297 \nL 126.966596 133.175298 \nL 127.576543 133.040986 \nL 128.18649 131.940008 \nL 128.796437 131.82001 \nL 129.406384 133.604159 \nL 130.016332 134.412901 \nL 131.236226 134.136331 \nL 132.45612 135.685953 \nL 133.066067 137.340011 \nL 133.676015 136.288675 \nL 134.895909 135.993915 \nL 135.505856 136.719324 \nL 136.115803 135.709234 \nL 136.72575 136.42476 \nL 137.335698 134.587062 \nL 137.945645 135.300004 \nL 139.775486 139.82782 \nL 140.385433 140.475498 \nL 140.995381 140.306414 \nL 141.605328 140.940003 \nL 142.215275 139.976221 \nL 142.825222 139.815007 \nL 143.435169 141.219081 \nL 144.045116 141.050778 \nL 144.655064 141.654507 \nL 145.265011 143.012739 \nL 145.874958 143.592645 \nL 146.484905 143.41165 \nL 147.094852 143.980006 \nL 148.314747 146.563359 \nL 148.924694 147.096522 \nL 149.534641 146.896841 \nL 150.144588 147.420001 \nL 150.754535 145.791072 \nL 151.97443 146.830917 \nL 152.584377 146.640008 \nL 153.194324 145.756555 \nL 153.804271 145.575619 \nL 154.414218 146.768582 \nL 155.024165 146.583246 \nL 155.634113 145.047383 \nL 156.854007 143.371801 \nL 157.463954 144.54001 \nL 158.073901 145.03412 \nL 159.293796 143.401947 \nL 159.903743 144.54001 \nL 160.51369 144.379502 \nL 161.123637 145.496973 \nL 161.733584 145.332455 \nL 162.343531 144.54001 \nL 162.953479 144.383483 \nL 163.563426 144.851112 \nL 164.173373 144.694615 \nL 164.78332 145.154644 \nL 165.393267 143.776374 \nL 166.003214 143.62916 \nL 167.223109 142.14001 \nL 167.833056 142.005097 \nL 170.272845 143.811687 \nL 170.882792 143.671037 \nL 171.492739 142.956007 \nL 172.712633 143.82815 \nL 173.932528 141.302024 \nL 174.542475 140.620009 \nL 175.762369 141.493849 \nL 176.372316 139.720334 \nL 177.592211 140.589743 \nL 180.641946 140.030534 \nL 181.251894 140.449948 \nL 181.861841 141.390013 \nL 183.081735 142.201864 \nL 183.691682 143.118467 \nL 184.911577 142.87706 \nL 186.131471 143.653568 \nL 186.741418 143.532008 \nL 187.351365 142.91015 \nL 188.57126 143.671037 \nL 189.181207 143.05765 \nL 189.791154 142.941963 \nL 190.401101 143.316706 \nL 191.011048 142.713924 \nL 191.620995 143.086158 \nL 193.450837 145.614883 \nL 194.060784 145.966419 \nL 194.670731 145.84142 \nL 197.720467 147.545511 \nL 199.550309 149.899278 \nL 200.770203 150.529247 \nL 201.38015 150.390008 \nL 201.990097 149.356006 \nL 202.600044 149.223196 \nL 203.209992 149.535605 \nL 203.819939 149.40317 \nL 204.429886 148.831705 \nL 205.039833 147.388698 \nL 205.64978 148.140002 \nL 206.259727 147.58138 \nL 206.869675 148.325414 \nL 207.479622 148.201541 \nL 208.089569 148.936607 \nL 208.699516 149.238307 \nL 209.309463 148.686849 \nL 209.91941 149.41059 \nL 210.529358 149.28478 \nL 211.139305 150.000005 \nL 211.749252 149.872782 \nL 212.359199 148.497036 \nL 213.579093 148.258032 \nL 214.189041 148.55144 \nL 215.408935 148.314899 \nL 216.018882 148.604522 \nL 216.628829 148.486988 \nL 217.238776 147.967208 \nL 218.458671 147.74001 \nL 219.678565 145.928989 \nL 220.288512 145.824717 \nL 221.508407 146.403047 \nL 222.118354 147.079549 \nL 222.728301 147.361632 \nL 223.338248 148.029242 \nL 223.948195 148.305527 \nL 224.558142 148.194962 \nL 226.387984 149.009441 \nL 227.607878 148.7872 \nL 228.217825 149.429561 \nL 229.43772 149.953336 \nL 230.047667 149.840383 \nL 230.657614 150.098824 \nL 231.267561 148.878464 \nL 231.877508 149.138541 \nL 232.487456 149.763284 \nL 233.097403 149.287829 \nL 233.70735 149.543611 \nL 234.317297 150.160152 \nL 234.927244 149.688392 \nL 235.537191 150.300006 \nL 236.757086 150.080439 \nL 237.367033 150.684182 \nL 237.97698 150.218881 \nL 238.586927 150.817909 \nL 239.196874 151.060292 \nL 240.416769 152.240007 \nL 241.026716 152.126168 \nL 241.636663 152.360696 \nL 242.24661 151.554433 \nL 242.856557 151.789321 \nL 244.076452 151.568582 \nL 244.686399 151.11764 \nL 245.906293 151.582431 \nL 246.51624 152.150741 \nL 247.126188 152.378135 \nL 247.736135 152.940002 \nL 249.565976 153.605359 \nL 250.175923 154.155795 \nL 250.785871 154.372134 \nL 251.395818 154.257649 \nL 252.005765 154.47226 \nL 253.225659 153.592426 \nL 253.835606 153.807098 \nL 254.445554 153.372168 \nL 255.665448 153.154703 \nL 256.275395 152.725991 \nL 256.885342 152.940002 \nL 258.105237 152.728019 \nL 258.715184 152.306052 \nL 259.325131 152.834679 \nL 260.545025 153.254033 \nL 261.154972 152.835655 \nL 261.76492 153.356111 \nL 262.374867 152.6289 \nL 264.204708 153.248265 \nL 264.814655 152.837563 \nL 266.03455 152.634547 \nL 266.644497 152.838494 \nL 267.254444 152.43398 \nL 267.864391 152.334606 \nL 268.474338 151.330419 \nL 269.694233 152.340013 \nL 270.30418 152.541196 \nL 270.914127 152.144733 \nL 271.524074 152.047972 \nL 272.134021 152.248239 \nL 272.743969 152.151733 \nL 273.963863 152.548165 \nL 274.57381 152.158612 \nL 275.183757 152.355664 \nL 276.403652 152.165371 \nL 277.013599 151.78139 \nL 277.623546 152.266083 \nL 278.84344 152.078463 \nL 279.453387 152.271821 \nL 280.063335 152.749634 \nL 283.723018 152.191266 \nL 284.332965 151.820008 \nL 284.942912 151.730036 \nL 285.552859 151.362112 \nL 286.162806 151.82926 \nL 286.772753 151.740009 \nL 287.992648 151.012137 \nL 288.602595 150.925845 \nL 289.822489 151.300979 \nL 290.432436 151.759461 \nL 291.042384 151.943776 \nL 291.652331 151.585164 \nL 293.482172 152.133603 \nL 294.702067 151.959637 \nL 295.312014 151.606673 \nL 295.921961 151.521538 \nL 296.531908 151.702113 \nL 297.141855 151.352604 \nL 297.751802 151.268805 \nL 298.36175 150.922255 \nL 298.971697 151.365011 \nL 299.581644 150.758188 \nL 300.191591 150.937937 \nL 300.801538 150.856286 \nL 301.411485 150.515263 \nL 302.021433 150.694246 \nL 302.63138 151.130783 \nL 304.461221 151.657562 \nL 305.071168 151.575536 \nL 306.291063 151.921831 \nL 306.90101 151.83976 \nL 307.510957 152.264626 \nL 308.120904 151.929476 \nL 308.730851 152.351999 \nL 309.340799 152.521054 \nL 309.950746 152.940002 \nL 310.560693 153.106758 \nL 311.780587 153.935567 \nL 312.390534 153.60207 \nL 313.000482 153.517895 \nL 313.610429 153.928237 \nL 317.270112 154.887828 \nL 317.880059 154.559288 \nL 318.490006 154.474618 \nL 319.099953 154.632091 \nL 320.319848 154.463638 \nL 320.929795 154.62 \nL 321.539742 154.296777 \nL 322.149689 154.691662 \nL 322.759636 154.846387 \nL 323.369583 154.762648 \nL 323.979531 154.916472 \nL 324.589478 155.306206 \nL 325.199425 154.985911 \nL 326.419319 155.289653 \nL 327.029266 155.20605 \nL 327.639214 154.88897 \nL 328.249161 154.806673 \nL 329.469055 155.107741 \nL 330.079002 155.02552 \nL 330.688949 154.481287 \nL 331.298897 153.708879 \nL 331.908844 153.170144 \nL 332.518791 153.093089 \nL 333.128738 153.245456 \nL 333.738685 152.940002 \nL 334.348632 153.092038 \nL 334.95858 152.560767 \nL 335.568527 152.940002 \nL 336.788421 153.241355 \nL 337.398368 153.165518 \nL 338.008315 153.315 \nL 338.618263 153.688332 \nL 339.22821 153.612013 \nL 339.838157 153.312507 \nL 341.058051 153.162528 \nL 341.667998 153.310044 \nL 342.277946 153.013857 \nL 342.887893 152.497907 \nL 343.49784 152.20477 \nL 344.107787 152.133017 \nL 344.717734 152.500791 \nL 345.327681 152.428708 \nL 345.937629 152.794244 \nL 346.547576 152.940002 \nL 347.157523 152.867438 \nL 349.597312 153.44365 \nL 350.207259 153.155394 \nL 350.817206 153.298222 \nL 351.427153 153.225959 \nL 352.0371 152.940002 \nL 352.647048 153.08238 \nL 353.256995 153.011048 \nL 354.476889 153.293686 \nL 355.086836 153.222355 \nL 358.136572 153.917963 \nL 358.746519 153.846226 \nL 359.356466 153.983482 \nL 359.966414 154.32844 \nL 360.576361 154.04846 \nL 361.186308 154.184454 \nL 361.796255 154.112897 \nL 362.406202 154.248201 \nL 363.016149 153.970684 \nL 363.626097 154.311437 \nL 364.236044 154.24021 \nL 364.845991 153.964391 \nL 365.455938 154.098628 \nL 366.675832 153.958187 \nL 367.28578 154.091613 \nL 367.895727 154.427334 \nL 369.115621 153.882695 \nL 369.725568 153.813601 \nL 369.725568 153.813601 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 65.361932 220.140003 \nL 66.581826 220.140003 \nL 67.191773 245.34 \nL 67.80172 220.140003 \nL 68.411668 236.940001 \nL 69.021615 234.540001 \nL 69.631562 207.540008 \nL 70.241509 186.539998 \nL 71.461403 192.649098 \nL 72.071351 186.539998 \nL 72.681298 165.863077 \nL 73.901192 173.100012 \nL 74.511139 163.440006 \nL 75.121086 148.987061 \nL 75.731034 147.340002 \nL 76.340981 140.561054 \nL 76.950928 139.5 \nL 77.560875 133.740004 \nL 78.170822 123.921819 \nL 79.390717 115.140009 \nL 80.610611 123.216928 \nL 81.830505 122.940012 \nL 82.440452 115.864141 \nL 83.0504 122.700002 \nL 83.660347 122.591615 \nL 84.270294 119.340005 \nL 84.880241 113.23092 \nL 85.490188 119.340005 \nL 86.100135 116.460014 \nL 87.32003 122.06434 \nL 87.929977 121.992632 \nL 88.539924 124.509235 \nL 89.149871 121.860003 \nL 90.369766 121.740005 \nL 90.979713 126.372572 \nL 92.199607 130.540004 \nL 92.809554 130.296524 \nL 94.029449 134.040005 \nL 94.639396 127.568584 \nL 95.85929 131.198827 \nL 96.469237 130.970773 \nL 97.689132 134.273347 \nL 98.299079 134.001824 \nL 98.909026 131.940008 \nL 99.518973 131.718953 \nL 100.12892 133.24346 \nL 100.738868 131.299329 \nL 101.348815 132.780007 \nL 101.958762 132.559673 \nL 102.568709 133.972264 \nL 103.178656 133.740004 \nL 104.398551 136.398474 \nL 105.008498 139.194546 \nL 105.618445 135.889268 \nL 107.448286 135.180006 \nL 108.668181 137.540007 \nL 109.278128 137.290699 \nL 110.498022 142.188 \nL 111.107969 143.213696 \nL 111.717917 142.903646 \nL 112.327864 145.186156 \nL 114.767652 148.842445 \nL 115.987547 150.540002 \nL 117.207441 154.502796 \nL 117.817388 155.257254 \nL 118.427335 157.140013 \nL 119.037283 152.184958 \nL 120.257177 153.678463 \nL 121.477071 157.275496 \nL 122.087018 155.79958 \nL 123.91686 157.789493 \nL 124.526807 157.397146 \nL 125.136754 158.030916 \nL 125.746701 157.644006 \nL 126.356649 156.266743 \nL 126.966596 155.904707 \nL 128.796437 157.740001 \nL 130.016332 155.138143 \nL 130.626279 156.673345 \nL 131.236226 155.406061 \nL 131.846173 155.078181 \nL 132.45612 155.664336 \nL 133.066067 157.140013 \nL 133.676015 156.805494 \nL 134.285962 155.592644 \nL 134.895909 156.153925 \nL 135.505856 157.574492 \nL 136.115803 157.247694 \nL 136.72575 157.780691 \nL 137.945645 160.500009 \nL 139.165539 159.825249 \nL 140.385433 160.798073 \nL 141.605328 158.540001 \nL 142.215275 156.64395 \nL 142.825222 157.140013 \nL 143.435169 156.84698 \nL 144.655064 153.19649 \nL 145.265011 152.176366 \nL 145.874958 150.413696 \nL 146.484905 150.181796 \nL 147.094852 150.700014 \nL 147.704799 150.469421 \nL 148.314747 149.506435 \nL 148.924694 150.748695 \nL 149.534641 150.522744 \nL 150.754535 151.510213 \nL 152.584377 150.840004 \nL 153.804271 151.789321 \nL 155.024165 149.988649 \nL 156.24406 150.924013 \nL 156.854007 150.714839 \nL 158.073901 148.987061 \nL 158.683848 149.449102 \nL 159.293796 149.254844 \nL 159.903743 150.355387 \nL 160.51369 150.799884 \nL 161.123637 150.600759 \nL 161.733584 149.770193 \nL 162.343531 150.840004 \nL 162.953479 151.270442 \nL 164.173373 153.352281 \nL 164.78332 153.759513 \nL 165.393267 153.55091 \nL 166.003214 153.952059 \nL 167.223109 155.940005 \nL 167.833056 156.319885 \nL 168.443003 157.288249 \nL 169.05295 157.066323 \nL 170.272845 157.795501 \nL 170.882792 156.415865 \nL 171.492739 156.780004 \nL 172.712633 155.217976 \nL 173.32258 156.148999 \nL 173.932528 156.506483 \nL 174.542475 157.420007 \nL 175.152422 157.766527 \nL 175.762369 155.340001 \nL 176.982263 157.140013 \nL 177.592211 156.935675 \nL 178.202158 157.275496 \nL 179.422052 159.016597 \nL 180.031999 159.340001 \nL 180.641946 160.190529 \nL 181.251894 159.448914 \nL 181.861841 159.765002 \nL 182.471788 159.03327 \nL 183.081735 159.348247 \nL 183.691682 158.109232 \nL 184.301629 158.425726 \nL 184.911577 158.227322 \nL 186.131471 156.823428 \nL 186.741418 157.140013 \nL 187.351365 157.954928 \nL 188.57126 156.58139 \nL 189.181207 156.892942 \nL 189.791154 156.709769 \nL 190.401101 155.549716 \nL 191.011048 155.861749 \nL 192.230943 154.547662 \nL 192.84089 155.340001 \nL 193.450837 155.647123 \nL 194.060784 155.000391 \nL 194.670731 155.306206 \nL 195.280678 153.725056 \nL 195.890626 153.096289 \nL 197.11052 152.785172 \nL 197.720467 152.169367 \nL 198.330414 152.479732 \nL 199.550309 150.355387 \nL 200.770203 150.077224 \nL 201.38015 149.490003 \nL 201.990097 149.356006 \nL 202.600044 149.669211 \nL 203.209992 149.535605 \nL 204.429886 150.152236 \nL 205.039833 150.894783 \nL 205.64978 150.321831 \nL 206.259727 150.188285 \nL 206.869675 149.623264 \nL 208.089569 151.081276 \nL 208.699516 150.946784 \nL 209.91941 151.528237 \nL 210.529358 151.393564 \nL 211.139305 151.680003 \nL 211.749252 151.127554 \nL 212.969146 152.525199 \nL 214.189041 153.077153 \nL 214.798988 152.530246 \nL 216.018882 153.075485 \nL 216.628829 152.130374 \nL 217.848724 152.672279 \nL 218.458671 151.740009 \nL 219.068618 152.408776 \nL 219.678565 152.675433 \nL 220.288512 152.14942 \nL 221.508407 152.678527 \nL 222.118354 152.158612 \nL 222.728301 152.0319 \nL 223.338248 152.293855 \nL 223.948195 151.78139 \nL 224.558142 152.042294 \nL 225.16809 151.534681 \nL 225.778037 151.794548 \nL 226.387984 152.432839 \nL 227.607878 152.940002 \nL 228.827773 151.940757 \nL 229.43772 152.193339 \nL 230.047667 152.07211 \nL 230.657614 152.322364 \nL 231.877508 152.081618 \nL 233.097403 153.305222 \nL 233.70735 152.454814 \nL 234.317297 153.060871 \nL 237.367033 154.246008 \nL 237.97698 154.832959 \nL 238.586927 154.35474 \nL 239.196874 154.232309 \nL 239.806822 154.461956 \nL 240.416769 153.990001 \nL 241.026716 154.218896 \nL 241.636663 154.098628 \nL 242.24661 154.67197 \nL 242.856557 154.550966 \nL 244.076452 154.997146 \nL 245.296346 154.075136 \nL 245.906293 154.297588 \nL 246.51624 153.84202 \nL 247.126188 154.063751 \nL 247.736135 153.612013 \nL 248.346082 153.833023 \nL 248.956029 153.718808 \nL 250.175923 154.155795 \nL 251.395818 155.245884 \nL 252.005765 155.128935 \nL 253.225659 154.244866 \nL 253.835606 154.78258 \nL 254.445554 154.668621 \nL 255.055501 154.232309 \nL 255.665448 154.44288 \nL 256.275395 154.331083 \nL 256.885342 154.540002 \nL 257.495289 153.790635 \nL 258.105237 154.317925 \nL 258.715184 153.890956 \nL 259.325131 153.782644 \nL 259.935078 153.045009 \nL 261.154972 152.835655 \nL 261.76492 152.419876 \nL 262.374867 152.317783 \nL 262.984814 151.285853 \nL 264.204708 151.70698 \nL 264.814655 152.222929 \nL 265.424603 152.122984 \nL 266.03455 152.634547 \nL 266.644497 152.229433 \nL 267.254444 152.43398 \nL 268.474338 152.235817 \nL 269.084286 151.836725 \nL 269.694233 150.840004 \nL 271.524074 150.561241 \nL 272.134021 151.062365 \nL 272.743969 150.673728 \nL 273.353916 150.876849 \nL 273.963863 150.197147 \nL 275.183757 150.602622 \nL 275.793704 150.512259 \nL 276.403652 150.131944 \nL 277.013599 150.622764 \nL 277.623546 150.533123 \nL 278.233493 151.020008 \nL 278.84344 150.92975 \nL 279.453387 151.126367 \nL 280.063335 151.03632 \nL 281.283229 150.289868 \nL 281.893176 150.20293 \nL 282.503123 150.681178 \nL 283.11307 150.59364 \nL 283.723018 150.787357 \nL 284.332965 150.700014 \nL 286.162806 151.273897 \nL 287.382701 150.546581 \nL 288.602595 150.37652 \nL 289.212542 150.566092 \nL 290.432436 150.397308 \nL 291.042384 149.770193 \nL 291.652331 149.959359 \nL 292.872225 149.795622 \nL 293.482172 150.252001 \nL 294.092119 150.169795 \nL 295.312014 150.540002 \nL 295.921961 150.45769 \nL 296.531908 150.641059 \nL 297.141855 151.088035 \nL 297.751802 150.74105 \nL 298.36175 151.185442 \nL 298.971697 151.1025 \nL 299.581644 151.543648 \nL 300.191591 151.46021 \nL 300.801538 151.898144 \nL 302.021433 152.249005 \nL 302.63138 152.16462 \nL 303.241327 151.822877 \nL 303.851274 151.997143 \nL 304.461221 151.657562 \nL 305.071168 151.575536 \nL 305.681116 152.004308 \nL 306.90101 152.347568 \nL 307.510957 152.771158 \nL 308.120904 152.434746 \nL 308.730851 152.351999 \nL 310.560693 152.856639 \nL 311.17064 152.524163 \nL 311.780587 152.442227 \nL 312.390534 152.112424 \nL 313.000482 152.279571 \nL 313.610429 152.198837 \nL 314.220376 152.611401 \nL 316.050217 153.103108 \nL 316.660165 153.021367 \nL 317.270112 153.183482 \nL 317.880059 153.58771 \nL 318.490006 153.26309 \nL 320.319848 153.741909 \nL 320.929795 153.660003 \nL 321.539742 153.817912 \nL 322.149689 153.497348 \nL 322.759636 153.893194 \nL 323.369583 154.049436 \nL 323.979531 153.493412 \nL 324.589478 153.413248 \nL 325.199425 153.56952 \nL 325.809372 153.960561 \nL 326.419319 154.114835 \nL 327.029266 153.799542 \nL 327.639214 153.25184 \nL 328.249161 152.940002 \nL 328.859108 152.862406 \nL 330.079002 153.171736 \nL 331.298897 153.939547 \nL 332.518791 154.241141 \nL 333.738685 154.082857 \nL 334.348632 153.776201 \nL 335.568527 154.075136 \nL 336.178474 153.99709 \nL 336.788421 154.145386 \nL 337.398368 154.067521 \nL 338.008315 153.765011 \nL 338.618263 153.239343 \nL 339.22821 153.612013 \nL 339.838157 153.759513 \nL 341.667998 153.532075 \nL 342.277946 153.678463 \nL 342.887893 153.382111 \nL 343.49784 153.307625 \nL 344.107787 153.453548 \nL 345.327681 153.305222 \nL 345.937629 153.012896 \nL 346.547576 152.940002 \nL 347.157523 152.649733 \nL 347.76747 152.795175 \nL 348.377417 152.723228 \nL 350.207259 153.80154 \nL 350.817206 153.942987 \nL 352.0371 153.796058 \nL 352.647048 153.509499 \nL 353.256995 153.437251 \nL 353.866942 153.790635 \nL 354.476889 153.930325 \nL 355.086836 154.281186 \nL 355.696783 154.207931 \nL 356.306731 154.345863 \nL 356.916678 154.062339 \nL 358.136572 154.337091 \nL 359.356466 154.192175 \nL 359.966414 154.32844 \nL 360.576361 153.840623 \nL 361.186308 153.769637 \nL 361.796255 153.491955 \nL 363.016149 153.352281 \nL 363.626097 153.488576 \nL 364.236044 153.419031 \nL 365.455938 154.098628 \nL 366.675832 154.365465 \nL 367.895727 153.818874 \nL 369.725568 153.612013 \nL 369.725568 153.612013 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 50.14375 152.604007 \nL 384.94375 152.604007 \n\" clip-path=\"url(#pd99faac0cd)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 256.68 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 256.68 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 256.68 \nL 384.94375 256.68 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 302.089063 103.26875 \nL 377.94375 103.26875 \nQ 379.94375 103.26875 379.94375 101.26875 \nL 379.94375 14.2 \nQ 379.94375 12.2 377.94375 12.2 \nL 302.089063 12.2 \nQ 300.089063 12.2 300.089063 14.2 \nL 300.089063 101.26875 \nQ 300.089063 103.26875 302.089063 103.26875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 304.089063 20.298437 \nL 314.089063 20.298437 \nL 324.089063 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- P(die=1) -->\n     <g transform=\"translate(332.089063 23.798437) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 304.089063 34.976562 \nL 314.089063 34.976562 \nL 324.089063 34.976562 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- P(die=2) -->\n     <g transform=\"translate(332.089063 38.476562) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 304.089063 49.654687 \nL 314.089063 49.654687 \nL 324.089063 49.654687 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- P(die=3) -->\n     <g transform=\"translate(332.089063 53.154687) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 304.089063 64.332812 \nL 314.089063 64.332812 \nL 324.089063 64.332812 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- P(die=4) -->\n     <g transform=\"translate(332.089063 67.832812) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 304.089063 79.010937 \nL 314.089063 79.010937 \nL 324.089063 79.010937 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- P(die=5) -->\n     <g transform=\"translate(332.089063 82.510937) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-35\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 304.089063 93.689062 \nL 314.089063 93.689062 \nL 324.089063 93.689062 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- P(die=6) -->\n     <g transform=\"translate(332.089063 97.189062) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-36\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd99faac0cd\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"334.8\" height=\"249.48\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 概率论公理\n",
        "\n",
        "*概率*（probability）可以被认为是将集合映射到真实值的函数。\n",
        "在给定的样本空间$\\mathcal{S}$中，事件$\\mathcal{A}$的概率，\n",
        "表示为$P(\\mathcal{A})$，满足以下属性：\n",
        "\n",
        "* 对于任意事件$\\mathcal{A}$，其概率从不会是负数，即$P(\\mathcal{A}) \\geq 0$；\n",
        "* 整个样本空间的概率为$1$，即$P(\\mathcal{S}) = 1$；\n",
        "* 对于*互斥*（mutually exclusive）事件（对于所有$i \\neq j$都有$\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$）的任意一个可数序列$\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$。"
      ],
      "metadata": {
        "id": "pJ_y7tRQCXru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 随机变量\n",
        "通过$P(X=a)$，我们区分了随机变量$X$和$X$可以采取的值（例如$a$）。我们可以简单用$P(a)$表示随机变量取值$a$的概率。"
      ],
      "metadata": {
        "id": "wBVQGGJNDmME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 处理多个随机变量"
      ],
      "metadata": {
        "id": "Dnm4cgjVD4X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 联合概率\n",
        "\n",
        "给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？\n",
        "请注意，对于任何$a$和$b$的取值，$P(A = a, B=b) \\leq P(A=a)$。"
      ],
      "metadata": {
        "id": "5RM-77nxD8FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 条件概率\n",
        "\n",
        "联合概率的不等式带给我们一个有趣的比率：\n",
        "$0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$。\n",
        "我们称这个比率为*条件概率*（conditional probability），\n",
        "并用$P(B=b \\mid A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。"
      ],
      "metadata": {
        "id": "SzWuA0bpEVCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 贝叶斯定理\n",
        "\n",
        "贝叶斯定理可以用以下公式表示:\n",
        "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$\n",
        "\n",
        "其中：\n",
        "- P(A \\mid B) 是在事件 B 已经发生的情况下事件 A 发生的后验概率（或条件概率）\n",
        "- $P(B \\mid A) $ 是在事件 A 已经发生的情况下事件 B 发生的似然概率（或条件概率）。\n",
        "- $P(A)$是事件 A 发生的先验概率（或边缘概率）\n",
        "- $P(B)$  是事件 B 发生的边缘概率。\n",
        "\n",
        "在实际应用中，这个过程通常包括以下步骤：\n",
        "\n",
        "- 1.先验概率：开始时对假设A的概率有一个初步估计（先验）。\n",
        "- 2.似然度：观察到一些证据B后，评估在A为真的情况下看到B的可能程度。\n",
        "- 3.标准化：通过考虑B发生的总体概率来调整我们的估计，确保概率总和为1。\n",
        "- 4.后验概率：综合先验知识和新证据，得到事件A在新证据B出现后的更新概率（后验）。"
      ],
      "metadata": {
        "id": "1Masi3-oFyAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 查阅文档"
      ],
      "metadata": {
        "id": "-ZbxvDKsEmWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 查找模块中的所有函数和类\n",
        "\n",
        "为了知道模块中可以调用哪些函数和类，可以调用dir函数。\n",
        "\n",
        "通常可以忽略以“__”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“_”（单下划线）开始的函数，它们通常是内部函数。"
      ],
      "metadata": {
        "id": "ibHy3SaWEtYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(dir(torch.distributions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VbFN-PLExj0",
        "outputId": "41a2b98b-fe5f-4584-c32c-7c98bd3bea32"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'InverseGamma', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'inverse_gamma', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 查找特定函数和类的用法\n",
        "\n",
        "有关如何使用给定函数或类的更具体说明，可以调用help函数。"
      ],
      "metadata": {
        "id": "gf30Q5kqE4Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(torch.ones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4QSXq-uE9px",
        "outputId": "06948a57-d8ce-4343-ba25-72ef3fb5a9b4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function ones in module torch:\n",
            "\n",
            "ones(...)\n",
            "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
            "    \n",
            "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
            "    by the variable argument :attr:`size`.\n",
            "    \n",
            "    Args:\n",
            "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
            "            Can be a variable number of arguments or a collection like a list or tuple.\n",
            "    \n",
            "    Keyword arguments:\n",
            "        out (Tensor, optional): the output tensor.\n",
            "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n",
            "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
            "            Default: ``torch.strided``.\n",
            "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            "            Default: if ``None``, uses the current device for the default tensor type\n",
            "            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n",
            "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
            "        requires_grad (bool, optional): If autograd should record operations on the\n",
            "            returned tensor. Default: ``False``.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> torch.ones(2, 3)\n",
            "        tensor([[ 1.,  1.,  1.],\n",
            "                [ 1.,  1.,  1.]])\n",
            "    \n",
            "        >>> torch.ones(5)\n",
            "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在Jupyter记事本中，我们可以使用?指令在另一个浏览器窗口中显示文档。 例如，list?指令将创建与help(list)指令几乎相同的内容，并在新的浏览器窗口中显示它。"
      ],
      "metadata": {
        "id": "5xFdzuU2FR45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list?"
      ],
      "metadata": {
        "id": "peWTXiqLFTWO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VElpEITFZtc",
        "outputId": "a8b1e841-3907-4dc8-f4dc-4167810260db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class list in module builtins:\n",
            "\n",
            "class list(object)\n",
            " |  list(iterable=(), /)\n",
            " |  \n",
            " |  Built-in mutable sequence.\n",
            " |  \n",
            " |  If no argument is given, the constructor creates a new empty list.\n",
            " |  The argument must be an iterable if specified.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __iadd__(self, value, /)\n",
            " |      Implement self+=value.\n",
            " |  \n",
            " |  __imul__(self, value, /)\n",
            " |      Implement self*=value.\n",
            " |  \n",
            " |  __init__(self, /, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the list.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(self, /)\n",
            " |      Return the size of the list in memory, in bytes.\n",
            " |  \n",
            " |  append(self, object, /)\n",
            " |      Append object to the end of the list.\n",
            " |  \n",
            " |  clear(self, /)\n",
            " |      Remove all items from list.\n",
            " |  \n",
            " |  copy(self, /)\n",
            " |      Return a shallow copy of the list.\n",
            " |  \n",
            " |  count(self, value, /)\n",
            " |      Return number of occurrences of value.\n",
            " |  \n",
            " |  extend(self, iterable, /)\n",
            " |      Extend list by appending elements from the iterable.\n",
            " |  \n",
            " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
            " |      Return first index of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  insert(self, index, object, /)\n",
            " |      Insert object before index.\n",
            " |  \n",
            " |  pop(self, index=-1, /)\n",
            " |      Remove and return item at index (default last).\n",
            " |      \n",
            " |      Raises IndexError if list is empty or index is out of range.\n",
            " |  \n",
            " |  remove(self, value, /)\n",
            " |      Remove first occurrence of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  reverse(self, /)\n",
            " |      Reverse *IN PLACE*.\n",
            " |  \n",
            " |  sort(self, /, *, key=None, reverse=False)\n",
            " |      Sort the list in ascending order and return None.\n",
            " |      \n",
            " |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n",
            " |      order of two equal elements is maintained).\n",
            " |      \n",
            " |      If a key function is given, apply it once to each list item and sort them,\n",
            " |      ascending or descending, according to their function values.\n",
            " |      \n",
            " |      The reverse flag can be set to sort in descending order.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  __class_getitem__(...) from builtins.type\n",
            " |      See PEP 585\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n"
          ]
        }
      ]
    }
  ]
}