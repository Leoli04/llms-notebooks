{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d787ddc-e5e8-41dd-b54d-5f91a507ebb1",
   "metadata": {},
   "source": [
    "# 深度学习计算\n",
    "在本章中，我们将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。 这些知识将使读者从深度学习“基础用户”变为“高级用户”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd25832-a699-4388-ac0c-9c619303d50a",
   "metadata": {},
   "source": [
    "## 模型构造\n",
    "\n",
    "在深度学习中，块是指由一层或多层神经网络组成的组件。它允许我们将复杂的神经网络结构分解为更小的、更易于管理的部分。每个块可以执行特定的功能，如特征提取、降维、分类等。通过将不同的块组合在一起，我们可以创建出具有强大学习和表示能力的深度学习模型。\n",
    "\n",
    "从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。 注意，有些块不需要任何参数。 最后，为了计算梯度，块必须具有反向传播函数。 在定义我们自己的块时，由于自动微分 提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ab0af-3afb-474c-91a5-03b885d927ee",
   "metadata": {},
   "source": [
    "我们先回顾一下多层感知机）的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f18f4c5-4432-4d06-aa02-2d2133abbe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2957, -0.1580, -0.1441,  0.1106,  0.1475,  0.0851,  0.0494,  0.0813,\n",
       "          0.0594, -0.0870],\n",
       "        [ 0.1876, -0.0722, -0.1385,  0.0900,  0.0947,  0.0330,  0.0284,  0.0260,\n",
       "         -0.0755, -0.1958]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d534cf-9716-4d9c-9838-2fc268cf5bfa",
   "metadata": {},
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，\n",
    "即在PyTorch中表示一个块的类，\n",
    "它维护了一个由`Module`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Module`的子类。\n",
    "另外，我们通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写，内部会自动执行前向传播。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd78fb-a910-421c-9f42-2a689a07b73f",
   "metadata": {},
   "source": [
    "###  自定义块\n",
    "在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。\n",
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72231411-0a72-4876-9024-0e3eeae29188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# MLP类继承了表示块的类\n",
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "    # # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self,X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f01bf4-4381-460a-b1c0-f605671cad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0252,  0.0586, -0.1102,  0.0751, -0.0104,  0.0304, -0.2154, -0.0178,\n",
       "          0.0019,  0.1334],\n",
       "        [-0.0383,  0.1141, -0.2240, -0.0026,  0.1134, -0.0567, -0.0893,  0.0066,\n",
       "          0.0319,  0.1294]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a9cff-48c6-4cfc-8271-25f7d12111c4",
   "metadata": {},
   "source": [
    "### 顺序块\n",
    "\n",
    "实现一个与`Sequential`功能相似的块。`Sequential`的设计是为了把其他模块串起来。\n",
    "为了构建我们自己的简化的`MySequential`，\n",
    "我们只需要定义两个关键函数：\n",
    "\n",
    "1. 一种将块逐个追加到列表中的函数；\n",
    "1. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "每个Module都有一个_modules属性，_modules的主要优点是： 在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1263167-90a2-493b-b8b4-0d9e741b3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e614df-8331-4b45-bda7-e45f5c4b9964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0774, -0.3473, -0.1783, -0.0619, -0.0510, -0.1176, -0.2447,  0.0689,\n",
       "          0.0095,  0.2464],\n",
       "        [ 0.0991, -0.2409, -0.2293,  0.0501, -0.0160, -0.0635, -0.1281,  0.0065,\n",
       "          0.0985,  0.1977]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用我们自己定义的顺序块 \n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f6c2e-521b-4cbf-9695-1b8637730a3d",
   "metadata": {},
   "source": [
    "### 在前向传播函数中执行代码\n",
    "\n",
    "有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为常数参数（constant parameter）。\n",
    "\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b7d4a8-af1c-480c-9540-b53bb44b8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd40e1-f089-43a5-8aa6-145d53e0ae63",
   "metadata": {},
   "source": [
    "在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情：\n",
    "它运行了一个while循环，在$L_1$范数大于$1$的条件下，\n",
    "将输出向量除以$2$，直到它满足条件为止。\n",
    "最后，模型返回了`X`中所有项的和。\n",
    "\n",
    "这里只是演示，不一定非要这么做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d18d3fb-43a5-4bf9-8a6a-5d73a5e550be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3223, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364dbf8-5e56-441e-8788-5fadc6d59ef8",
   "metadata": {},
   "source": [
    "### 嵌套块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ddecd6-3f2d-4a4b-8afa-c639488cb682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4193, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede82ee6-30de-4955-ae46-e14fbd682a6d",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4677f1-e4c2-48db-8407-a4cf98a5fb7a",
   "metadata": {},
   "source": [
    "## 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63fc05-377c-4d93-b399-744f42e03a9f",
   "metadata": {},
   "source": [
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。 此时，我们的目标是找到使损失函数最小化的模型参数值。 经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b0d6b-4d2f-4888-bab8-97da8d097013",
   "metadata": {},
   "source": [
    "我们首先看一下具有单隐藏层的多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07582575-c53d-4a54-a0de-57a89c3cd215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0827],\n",
       "        [-0.0209]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b499-da6b-495e-82ae-3968c50abab3",
   "metadata": {},
   "source": [
    "###  参数访问\n",
    "\n",
    "我们从已有模型中访问参数。 当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9570f39c-e4ba-4ebc-a94f-8e17e63cb3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.3533,  0.0904,  0.0796, -0.1262,  0.0314,  0.0165, -0.0221, -0.1721]])), ('bias', tensor([-0.0868]))])\n"
     ]
    }
   ],
   "source": [
    "# 检查第二个全连接层的参数\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6b4cc-e40a-4c80-b38b-db966c8e4847",
   "metadata": {},
   "source": [
    "#### 目标参数\n",
    "访问模型具体某一层的参数，通过字典访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3849c50-a844-4bdf-b20b-fc6af8a46f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0868], requires_grad=True)\n",
      "tensor([-0.0868])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].weight.grad == None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680fd50-a9b8-4655-9e14-6063f214fa18",
   "metadata": {},
   "source": [
    "#### 一次性访问所有参数\n",
    "\n",
    "通过 .named_parameters() 方法，可以获取每个参数的名称及其对应的张量形状。它返回一个迭代器，其中每个元素是一个元组，包含参数的名称和参数本身（一个 Tensor 对象）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bed7b4d1-9926-4892-9311-297f285f9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e22e4-ac54-4f66-b77f-632d6674391d",
   "metadata": {},
   "source": [
    "也可以通过.state_dict()查看，这个方法返回一个包含模型所有权重和偏置的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66de8247-e012-421d-aed7-6b291917833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.3671, -0.4369, -0.0284, -0.4646],\n",
       "                      [-0.4833,  0.3435,  0.3080,  0.1832],\n",
       "                      [-0.3523, -0.2193, -0.4342,  0.4728],\n",
       "                      [-0.1467, -0.3278, -0.3771,  0.1030],\n",
       "                      [-0.4927, -0.0544, -0.1226, -0.4228],\n",
       "                      [-0.2052,  0.0317, -0.2381,  0.2881],\n",
       "                      [-0.2318, -0.2508, -0.4626,  0.3863],\n",
       "                      [ 0.1460, -0.0357, -0.1638, -0.0190]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0872,  0.0024, -0.1970,  0.0468, -0.0976,  0.1706,  0.1079, -0.1310])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.3533,  0.0904,  0.0796, -0.1262,  0.0314,  0.0165, -0.0221, -0.1721]])),\n",
       "             ('2.bias', tensor([-0.0868]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e7eb8-8f1c-4c3f-b96a-5e725c725f86",
   "metadata": {},
   "source": [
    "#### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86313fa5-cbd0-43e0-afb9-81e6881743bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0828],\n",
       "        [0.0828]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个生成块的函数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aca3a14-7507-4985-8745-cf19ffde2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d369534-8800-43e8-a719-ce28927d2efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4369, -0.1162,  0.3699,  0.0328,  0.2302,  0.0987,  0.2278,  0.3333])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过索引访问： 第一个主要的块中、第二个子块的第一层的偏置项\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc05d9-0e59-4ad6-95ba-8a9f4144aeb9",
   "metadata": {},
   "source": [
    "### 参数初始化\n",
    "\n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的nn.init模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4484c-4b9a-464a-9c0d-4a6929a515dc",
   "metadata": {},
   "source": [
    "#### 内置初始化\n",
    "nn.init 它包含了多种用于初始化神经网络权重和偏置的方法。以下是nn.init中常用的一些初始化方法：\n",
    "- normal：此方法从给定均值和标准差的正态分布中生成值来填充张量。用户可以指定正态分布的均值（mean）和标准差（std），默认值分别为0和1。\n",
    "- constant：使用此方法，可以用一个常数值（val）来填充整个张量。这对于需要特定值初始化的场景非常有用。\n",
    "- uniform：该方法从均匀分布U(a, b)中生成值来填充输入的张量。其中，a是分布的下界，b是分布的上界。默认情况下，a为0，b为1。\n",
    "- xavier_uniform 和 xavier_normal：这两种方法基于Xavier初始化策略，也称为Glorot初始化。它们分别根据均匀分布和正态分布来初始化权重，旨在解决权重初始化时可能出现的梯度消失或爆炸问题。\n",
    "- kaiming_uniform 和 kaiming_normal：这两种方法基于He初始化策略，也称为Kaiming初始化。与Xavier初始化类似，但更适合于ReLU激活函数及其变体。它们也分别使用均匀分布和正态分布。\n",
    "- sparse：此方法可以将张量中的一部分元素初始化为零，从而实现稀疏初始化。这有助于减少模型复杂度并可能提高泛化能力。\n",
    "- zeros：函数用于将张量的所有元素初始化为0。这个函数通常用于偏置（bias）的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ac6a0-efc0-4157-86d5-baa6a193f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        \n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "806e8b1c-86a4-466f-a967-b3895c984c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将所有参数初始化为给定的常数，比如初始化为1。\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf615604-48fb-405a-acd1-964d7ed2be1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5759, -0.3191, -0.5171,  0.6523])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 对某些块应用不同的初始化方法。\n",
    "# 使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb931fe-7417-45f2-b024-cbc04f9f48de",
   "metadata": {},
   "source": [
    "#### 自定义初始化\n",
    "通过实现了一个my_init函数来应用到net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2578e602-55e6-48f6-a869-8f291f3ec41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000,  0.0000,  5.5615],\n",
       "        [ 0.0000, -6.8724,  0.0000,  7.5669]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0a798-9fb8-427b-b7c0-311c79685ea2",
   "metadata": {},
   "source": [
    "通过直接设置参数的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "273f67a9-a080-413a-be3f-17129ec1ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  1.0000,  6.5615])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3cbd0-b01d-4fba-813d-071b25f88ac8",
   "metadata": {},
   "source": [
    "### 参数绑定\n",
    "\n",
    "有时我们希望在多个层间共享参数： 我们可以定义一个层，然后使用它的参数来设置另一个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c4ab108-cf61-4db9-9e20-7a6efabde7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036889c0-c408-403d-bcf9-19b04ba6ea92",
   "metadata": {},
   "source": [
    "第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "\n",
    "当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d322b3-f414-4379-8b6d-6b3612810df8",
   "metadata": {},
   "source": [
    "## 延后初始化\n",
    "你可能会奇怪，在前面的章节中，\n",
    "- 我们定义了网络架构，但没有指定输入维度。\n",
    "- 我们添加层时没有指定前一层的输出维度。\n",
    "- 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。\n",
    "\n",
    "代码能够运行诀窍是深度学习框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f917224f-8cd0-4c39-8b2e-08f1fbe4e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\workspace\\llms-notebooks\\pytorch\\env\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cda9f1c6-f279-4ee9-8004-1c02c13999c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<UninitializedParameter>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb73d789-596e-4dc8-bf3c-50f5f45d95a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d101b45-6ac0-48cf-a598-71db68eb7ff0",
   "metadata": {},
   "source": [
    "一旦我们知道输入维度 20，框架就可以通过插入值 20 来识别第一层权重矩阵的形状。识别出第一层的形状后，框架将进入第二层，依此类推计算图直到所有形状都已知。请注意，在这种情况下，只有第一层需要延迟初始化，但框架会顺序初始化。一旦知道所有参数形状，框架就可以最终初始化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad179cd2-d50e-4c54-9d8c-35eb7ff2ecd2",
   "metadata": {},
   "source": [
    "## 自定义层\n",
    "\n",
    "层是神经网络中最基本的构建单元。每个层负责接收输入数据，对其进行特定类型的变换（如线性变换、非线性变换、卷积、池化等），然后输出变换后的结果。典型的层包括全连接层（Dense Layer）、卷积层（Convolutional Layer）、池化层（Pooling Layer）、激活层（Activation Layer）等。\n",
    "\n",
    "功能：单个层通常执行单一类型的计算操作，比如卷积层用于提取特征，激活层引入非线性，全连接层用于整合特征等。\n",
    "\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。 在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40afb5-fef0-47bd-bda4-7a1609a20188",
   "metadata": {},
   "source": [
    "### 不带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "991f563c-fa58-4c73-8797-a9eba3673753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # 从其输入中减去均值\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c55308b-5f3a-4080-b14d-177aad74a445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2be0c0-9b8e-4106-ad37-e4da8b038234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将层作为组件合并到更复杂的模型中\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78db290c-2309-46e9-8120-9792f6c883f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3132e-10, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88723c2a-54e0-4ba4-80e4-dfa99c171fc8",
   "metadata": {},
   "source": [
    "### 带参数的层\n",
    "\n",
    "下面我们定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用**内置函数**来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f908643a-5548-4a95-a9db-e215e903a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60237bd5-3822-429d-b39c-d79ba392fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8507, -0.5058,  0.3367],\n",
       "        [-1.6526,  0.2127, -1.5380],\n",
       "        [-1.6145, -0.1324, -0.3577],\n",
       "        [-0.7560,  0.0280,  0.6306],\n",
       "        [-1.6107, -1.5456,  0.7213]], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化MyLinear类并访问其模型参数\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5b729c7-b333-4b8b-8a10-dad114f2b4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3856],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义层直接执行前向传播计算\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a072db-53a9-4b4d-b4a1-55a27f3d3e04",
   "metadata": {},
   "source": [
    "- 层是神经网络的基本组成单位，执行单一的转换操作。\n",
    "- 块是由一个或多个层构成的模块，旨在实现更复杂的变换或促进模型结构的复用性。块提供了更高层次的抽象，使得模型设计更加模块化和高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b80f96-f291-4581-bab5-e4eb13eade33",
   "metadata": {},
   "source": [
    "## 读写文件\n",
    "\n",
    "有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 还有，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70321c95-d526-4b57-a0fb-cd2797eb067b",
   "metadata": {},
   "source": [
    "### 加载和保存张量\n",
    "\n",
    "我们可以调用load和save函数分别读写它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bc3986a-3b1f-4f58-a5d9-a67c5d8204b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 存储、加载单个张量\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'data/x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3953a299-dc57-4a98-9f5e-13d57c06124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('data/x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e3a8d3b-ecb7-44f2-a605-d94aa4a3427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储、加载 张量列表 \n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c83c1f01-85f0-4120-9fcb-9b07c90deb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'data/mydict')\n",
    "mydict2 = torch.load('data/mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0cea44-1f58-4ed7-a7b1-94bffa87005f",
   "metadata": {},
   "source": [
    "### 加载和保存模型参数\n",
    "\n",
    "度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6aa3e181-096e-4efd-b455-5d251dfd368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层感知机模型 \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c58b3192-b4ee-4504-893f-9c3041c6c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型的参数存储在一个叫做“mlp.params”的文件中\n",
    "torch.save(net.state_dict(), 'data/mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51772b30-5063-4d9a-a474-9b27c8841b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 恢复模型\n",
    "\n",
    "# 实例化了原始多层感知机模型\n",
    "clone = MLP()\n",
    "#  这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。\n",
    "clone.load_state_dict(torch.load('data/mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeb8347c-f0c7-4d08-a9ac-97b943cdda27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 结果验证\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184d34d-ac91-40d3-be25-ef5cb17dc8dc",
   "metadata": {},
   "source": [
    "## GPU\n",
    "\n",
    "使用nvidia-smi命令来查看显卡信息."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "735d3f02-e10c-4c7c-ba3b-1f813a8f20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 17 14:27:42 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce 940MX         WDDM  |   00000000:02:00.0 Off |                  N/A |\n",
      "| N/A    0C    P0             N/A /  200W |       0MiB /   2048MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264aa0b5-3c51-46d9-9c7b-6b58cf16432d",
   "metadata": {},
   "source": [
    "在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为环境（context）。 \n",
    "默认情况下，所有变量和相关的计算都分配给CPU。 有时环境可能是GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111b577-f8cc-4410-bacc-0e68046a2b31",
   "metadata": {},
   "source": [
    "### 计算设备\n",
    "在PyTorch中，CPU和GPU可以用torch.device('cpu') 和torch.device('cuda')表示。\n",
    "如果有多个GPU，我们使用`torch.device(f'cuda:{i}')`\n",
    "来表示第$i$块GPU（$i$从0开始）。\n",
    "另外，`cuda:0`和`cuda`是等价的。\n",
    "可以通过`torch.cuda.device_count()`查看GPU数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ac5434d-01bb-443f-9c99-9a12f629c896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8876c174-9c7e-4600-ba89-1e647b7c6c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3659c98-c475-483b-b525-9fbd6ab3dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31325f84-e9e4-400d-8ec7-af040639fcb0",
   "metadata": {},
   "source": [
    "### 张量与GPU\n",
    "我们可以查询张量所在的设备。 默认情况下，张量是在CPU上创建的。\n",
    "\n",
    "**需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。**例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4dc003c-bbc9-490c-b6f1-c2a50ac6a212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225df3ac-76e5-47a6-9e1a-1e60f45ea915",
   "metadata": {},
   "source": [
    "#### 存储在GPU上\n",
    "\n",
    "有几种方法可以在GPU上存储张量:\n",
    "- 使用PyTorch的.to()方法：例如，如果要将一个张量移动到GPU上，可以使用tensor.to('cuda')。\n",
    "- 在创建张量时直接指定设备：例如，使用torch.tensor(data, device='cuda')可以直接在GPU上创建一个张量。\n",
    "- 使用torch.cuda模块：\n",
    "\n",
    "我们可以使用nvidia-smi命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "832b0789-3ef9-4edc-bf68-8f8e6e784ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在创建张量时指定存储设备\n",
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1a7d93d-e175-40c3-b700-8f2a221c9cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9941, 0.8429, 0.5843],\n",
       "        [0.8334, 0.5671, 0.1547]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(0))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356010f-c6d9-436d-b98a-878da05ffe06",
   "metadata": {},
   "source": [
    "#### 复制\n",
    "如果我们[**要计算`X + Y`，我们需要决定在哪里执行这个操作**]，因为`X 、Y`不在同一设备上，直接相加，运行时引擎会因为在同一设备上找不到数据会导致失败。我们可以通过`.cuda()`复制数据到指定设备上。\n",
    "如果复制的数据已经在指定的设备上了，再次执行`.cuda()`，并不会重新复制、分配新内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6327adbd-d3ef-426e-a740-25efa0cf2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "Z = X.cuda(0)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc21012c-3c5a-40e1-bca1-b6cf866fa7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9941, 1.8429, 1.5843],\n",
       "        [1.8334, 1.5671, 1.1547]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49035526-cb18-425f-a6a6-884f3ab7ede2",
   "metadata": {},
   "source": [
    "### 神经网络与GPU\n",
    "\n",
    "神经网络模型也可以指定设备。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19df6cc4-afb2-487a-8ec8-32ef05d6bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型参数放在GPU上\n",
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3277d621-4733-41a0-90e4-496b796907cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3958],\n",
       "        [-0.3958]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6277539f-be80-4af5-9bcb-64c92c16c215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型参数存储位置\n",
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103c945-ed30-4efa-a8d3-8f1918c08a5e",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\n",
    "* 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。\n",
    "* 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
