{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fec8b2-becf-41ea-aa3a-2ecad618b7a7",
   "metadata": {},
   "source": [
    "# 自然语言处理: 预训练\n",
    "\n",
    "预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务\n",
    "\n",
    "![预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务](./image/nlp-map-pretrain.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb2ed-8d5e-4185-9b33-37a44b77c752",
   "metadata": {},
   "source": [
    "## 词嵌入（word2vec）\n",
    "\n",
    "在前面的章节中我们使用独热向量来表示词（字符就是单词），但是独热向量不能准确表达不同词之间的相似度。word2vec工具就是为了解决这个问题提出来的。\n",
    "\n",
    " - 词向量是用于表示单词意义的向量，也可以看作词的特征向量。将词映射到实向量的技术称为词嵌入。\n",
    "\n",
    "- word2vec工具包含跳元模型和连续词袋模型。\n",
    "\n",
    "- 跳元模型假设一个单词可用于在文本序列中，生成其周围的单词；而连续词袋模型假设基于上下文词来生成中心单词。\n",
    "\n",
    "- 跳元模型的主要思想是使用softmax运算来计算基于给定的中心词$w_c$生成上下文字$w_o$的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb4f5e-6215-4071-a697-9c2594f0227b",
   "metadata": {},
   "source": [
    "## 近似训练\n",
    "\n",
    "由于softmax操作的性质，跳元模型的梯度计算和连续词袋模型的梯度计算都包含与整个词表大小一样多的项的求和，在一个词典上求和的梯度的计算成本是巨大的！\n",
    "为了降低上述计算复杂度，将介绍两种近似训练方法：**负采样和分层softmax。**\n",
    "\n",
    "- 负采样（Negative Sampling）：负采样主要用于处理不平衡数据集，在这些场景中，某些类别的样本（如正样本）可能远远少于其他类别的样本（如负样本）。负采样通过减少负样本的数量来平衡数据集，从而提高模型的训练效率和准确性。\n",
    "- 分层softmax使用二叉树中从根节点到叶节点的路径构造损失函数。训练的计算成本取决于词表大小的对数。\n",
    "  \n",
    "> 在传统的Softmax输出层中，所有类别的概率都通过一个全连接层进行计算，这样的计算复杂度随着类别数量的增加而显著增加。而分层Softmax通过将类别分成多个层次，每个层次只计算一部分类别的概率，从而有效降低了计算复杂度。\n",
    "> \n",
    "> 具体来说，分层Softmax会构建一个二叉树（或其他类型的树结构），树的每个叶子节点代表一个类别。从根节点到叶子节点的路径上，每个内部节点都对应一个二分类问题。这样，原本的一个多分类问题就被转化为了多个二分类问题。在推理时，模型从根节点开始，通过解决一系列二分类问题，最终到达代表目标类别的叶子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7671fd-a7bc-45e9-8d28-32bcbb406375",
   "metadata": {},
   "source": [
    "## 用于预训练词嵌入的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02847aad-8f42-4a11-8a8c-8452851d1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e5e5df-e6ff-40c8-a498-cce7e0622527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences数: 42069'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
    "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "\n",
    "#@save\n",
    "def read_ptb():\n",
    "    \"\"\"将PTB数据集加载到文本行的列表中\"\"\"\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Readthetrainingset.\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()\n",
    "f'# sentences数: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cae3c1-d42c-4f8c-bc66-f66e63ca894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 6719'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出现次数少于10次的任何单词都将由“<unk>”词元替换\n",
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "f'vocab size: {len(vocab)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9d500-67b4-423b-aba2-0ca441ec49b8",
   "metadata": {},
   "source": [
    "训练词嵌入模型时，可以对高频单词进行下采样。\n",
    "\n",
    "下采样（Downsampling）：下采样主要用于减少数据集的大小，以便更快地训练和评估模型，同时减少计算资源的使用。它通过减少样本的数量来实现，可以是随机选择样本，也可以是基于某种策略（如保留最具有代表性的样本）来选择样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426550e-6e69-4b97-8efc-a502240f97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def subsample(sentences, vocab):\n",
    "    \"\"\"下采样高频词\"\"\"\n",
    "    # 排除未知词元'<unk>'\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                 for line in sentences]\n",
    "    counter = d2l.count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # 如果在下采样期间保留词元，则返回True\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "            counter)\n",
    "\n",
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbccac7e-deea-45cd-b6e1-8bf24359207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_list_len_pair_hist(\n",
    "    ['origin', 'subsampled'], '# tokens per sentence',\n",
    "    'count', sentences, subsampled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993feb3-7b49-4462-80ca-86f71f879cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'\"{token}\"的数量：'\n",
    "            f'之前={sum([l.count(token) for l in sentences])}, '\n",
    "            f'之后={sum([l.count(token) for l in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163eb288-7a0c-4a18-ad61-cb51e96f3b5d",
   "metadata": {},
   "source": [
    "## 全局向量的词嵌入（GloVe）\n",
    "\n",
    "与Word2Vec只关注局部上下文窗口不同，GloVe利用全局的统计信息，即整个语料库中词的共现情况，来学习词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723fc95-dce1-42ac-a61f-d4127c78fd8e",
   "metadata": {},
   "source": [
    "- GloVe使用平方损失来拟合预先计算的全局语料库统计数据。\n",
    "- 对于GloVe中的任意词，中心词向量和上下文词向量在数学上是等价的。\n",
    "\n",
    "- GloVe可以从词-词共现概率的比率来解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2869a-52f4-499e-b1b3-8e329093e688",
   "metadata": {},
   "source": [
    "## 子词嵌入\n",
    "\n",
    "\n",
    "子词嵌入（Subword Embedding）是一种在自然语言处理（NLP）中用于表示单词的技术，它特别适用于处理稀有词和未登录词（Out-of-Vocabulary，OOV）。子词嵌入的典型代表是fastText模型。\n",
    "\n",
    "子词嵌入是指将单词表示为其构成子词（如字符n-grams）的向量组合。这种方法允许模型利用单词内部的形态学信息，从而更准确地表示单词的语义。\n",
    "\n",
    "\n",
    "**n-gram表示单词：**\n",
    "- 传统的词向量模型（如word2vec）将每个单词视为独立的原子单位，忽略了单词内部的形态特征。\n",
    "- fastText使用字符级别的n-grams来表示单词。例如，对于单词“book”，假设n的取值为3，则它的trigram有：“<bo”, “boo”, “ook”, “ok>”，其中“<”和“>”分别表示前缀和后缀。\n",
    "- 这些n-grams的向量被叠加起来表示原始单词的向量，这样低频词和OOV词可以通过共享n-grams获得更好的向量表示。\n",
    "\n",
    "字节对编码执行训练数据集的统计分析，以发现词内的公共符号。作为一种贪心方法，字节对编码迭代地合并最频繁的连续符号对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e9ccdc-7e6b-4c32-a65f-32de5756bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c08540-b9ea-4d74-a555-f20541e58bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1a6eb0-363e-476e-99cf-2954d8bec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # “pairs”的键是两个连续符号的元组\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get)  # 具有最大值的“pairs”键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e421f4-e0ba-4d39-9fae-6656e0aa42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28d8652-b44b-44b8-841f-cff541da00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并# 1: ('t', 'a')\n",
      "合并# 2: ('ta', 'l')\n",
      "合并# 3: ('tal', 'l')\n",
      "合并# 4: ('f', 'a')\n",
      "合并# 5: ('fa', 's')\n",
      "合并# 6: ('fas', 't')\n",
      "合并# 7: ('e', 'r')\n",
      "合并# 8: ('er', '_')\n",
      "合并# 9: ('tall', '_')\n",
      "合并# 10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'合并# {i+1}:',max_freq_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8be42d-3fd7-4908-80f0-0a29052d44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fab0ce5-d788-4162-be61-1f21879148bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48de3ae7-2c5e-44b7-941a-c06d4fb2abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # 具有符号中可能最长子字的词元段\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049cbb81-ea4f-47f1-b50f-e134ff80681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09c090-a965-412d-8ded-e4418dfdf6b0",
   "metadata": {},
   "source": [
    "##  词的相似性和类比任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b6fd61-682a-4f0c-b5ae-bad383898e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64230c-c6a7-4826-9771-109d3b788c8e",
   "metadata": {},
   "source": [
    "### 加载预训练词向量\n",
    "数据来源：\n",
    "- [GloVe网站](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText网站](https://fasttext.cc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b544c783-0b03-402f-b3a0-74a3b253ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
    "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
    "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
    "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
    "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2ec2b2-a849-4f71-b836-f90238c45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class TokenEmbedding:\n",
    "    \"\"\"GloVe嵌入\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = d2l.download_extract(embedding_name)\n",
    "        # GloVe网站：https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText网站：https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r',errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # 跳过标题信息，例如fastText中的首行\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19acae40-2a0b-4b20-9ebb-adb3eebb5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b50d = TokenEmbedding('glove.6b.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8faa88d7-6f77-4da7-9bdf-95c7f7cf8baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe80b2b-0b8c-4f00-a54c-54d88b2fb1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 'beautiful')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据 \n",
    "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dee5c-c071-4e0a-b788-bb0a087291e5",
   "metadata": {},
   "source": [
    "### 应用预训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce4b6c-dd8f-4331-9598-b32b69f41560",
   "metadata": {},
   "source": [
    "#### 词相似度\n",
    "余弦相似度是一种通过计算两个向量在向量空间中夹角的余弦值来评估它们相似度的方法。\n",
    "在文本处理领域，词可以被表示为向量，这些向量通常基于词频、TF-IDF值或其他词嵌入技术（如Word2Vec、GloVe等）构建。因此，通过计算这些词向量之间的余弦相似度，我们可以评估词之间的相似程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fadcff4-9207-4a2b-8943-9d5f32fc1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # 增加1e-9以获得数值稳定性\n",
    "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
    "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
    "        torch.sqrt((x * x).sum()))\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841cf964-ff57-4dc3-b672-1b4bf210edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # 排除输入词\n",
    "        print(f'{embed.idx_to_token[int(i)]}：cosine相似度={float(c):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d17cb4-53c5-408b-897a-19c9d9af4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chips：cosine相似度=0.856\n",
      "intel：cosine相似度=0.749\n",
      "electronics：cosine相似度=0.749\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad57be-e9d5-444c-8ad8-91d2cd1471e7",
   "metadata": {},
   "source": [
    "#### 词类比\n",
    "\n",
    "词类比任务可以定义为：\n",
    "对于单词类比$a : b :: c : d$，给出前三个词$a$、$b$和$c$，找到$d$。\n",
    "用$\\text{vec}(w)$表示词$w$的向量，\n",
    "为了完成这个类比，我们将找到一个词，\n",
    "其向量与$\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$的结果最相似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92427ff9-2465-411a-b4b0-a7e8364e0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
    "    return embed.idx_to_token[int(topk[0])]  # 删除未知词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0041fe-b40d-458f-8c1a-cbb4997e3758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb69393-bbe2-4b3d-b1f4-c165d8bfb1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cef55f7-4d00-49e6-b47a-59870bd2fd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5bea1c-0b7d-462b-86ca-f4076bd2e65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('do', 'did', 'go', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6df24d-1ebc-463a-8067-6960bc461991",
   "metadata": {},
   "source": [
    "## 来自Transformers的双向编码器表示（BERT）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abf6e5-393b-4f33-8c41-374af53509aa",
   "metadata": {},
   "source": [
    "### 从上下文无关到上下文敏感\n",
    "\n",
    "word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文。\n",
    "\n",
    "词元$x$的上下文敏感表示是函数$f(x, c(x))$，其取决于$x$及其上下文$c(x)$。流行的上下文敏感表示包括TagLM（language-model-augmented sequence tagger，语言模型增强的序列标记器） 、CoVe（Context Vectors，上下文向量）和ELMo（Embeddings from Language Models，来自语言模型的嵌入） 。\n",
    "\n",
    "以ELMo为例：通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示（例如GloVe）连结起来。一方面，在加入ELMo表示后，冻结了预训练的双向LSTM模型中的所有权重。另一方面，现有的监督模型是专门为给定的任务定制的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c84f35-c3b6-4d77-9b9b-c02cc07aa04d",
   "metadata": {},
   "source": [
    "### 从特定于任务到不可知任务\n",
    "\n",
    "尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定于任务的架构。\n",
    "GPT（Generative Pre Training，生成式预训练）模型为上下文的敏感表示设计了通用的任务无关模型。\n",
    "\n",
    "GPT建立在Transformer解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推断、问答、句子相似性和分类等12项任务上进行了评估，并在对模型架构进行最小更改的情况下改善了其中9项任务的最新水平。\n",
    "\n",
    "GPT不足：由于语言模型的自回归特性，GPT只能向前看（从左到右）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c93dcb-7f25-47a6-a85d-4cb805f9197a",
   "metadata": {},
   "source": [
    "### BERT：把两个最好的结合起来\n",
    "\n",
    "ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双向编码，并且对于大多数的自然语言处理任务只需要最少的架构改变。\n",
    "\n",
    "ELMo、GPT和BERT之间的差异如下：\n",
    "\n",
    "\n",
    "![ELMo、GPT和BERT的比较](../image/elmo-gpt-bert.svg)\n",
    "\n",
    "在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型架构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b46265-ad31-440b-a7cc-a75b2286d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde96499-9a4f-48c9-82d8-bc7c3ec3a6ea",
   "metadata": {},
   "source": [
    "## 输入表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f68fac-435e-45e3-a851-515d090a2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd39566-aad9-4245-a7d9-c10d44037f42",
   "metadata": {},
   "source": [
    "BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。\n",
    "\n",
    "![BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和](../image/bert-input.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa163da-79a1-47a5-bd0a-7454c78f0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12e0ce81-06f5-4294-80c7-3038c5dcafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1de27-ead1-49a1-ba09-5af6e679d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f7d5a-04fd-486c-b167-fda5fdc53fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
