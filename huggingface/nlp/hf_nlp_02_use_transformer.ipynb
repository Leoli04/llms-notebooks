{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "4QX7L7ITwJOF"
      ],
      "authorship_tag": "ABX9TyO0syn7i7HmWI+p5Tbfmas4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9e735fb070c433b8d5885e46eef20dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b90ab1d974d4486dbe2552fdf77168c8",
              "IPY_MODEL_5fc151f25d544152be31221c26347f86",
              "IPY_MODEL_054836a2d6f2489594d93599841c40e0"
            ],
            "layout": "IPY_MODEL_fb508e2310a3447db3be5b2d392154c5"
          }
        },
        "b90ab1d974d4486dbe2552fdf77168c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99a1fc54b334fa78e4040462506a6b0",
            "placeholder": "​",
            "style": "IPY_MODEL_42ce7a4cd5af43d3a27deb15b7cf904e",
            "value": "config.json: 100%"
          }
        },
        "5fc151f25d544152be31221c26347f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19b613a533de4fbf908a595009e167fa",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ac5ad61186d4fc0a1d18929cd60d424",
            "value": 570
          }
        },
        "054836a2d6f2489594d93599841c40e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a7e2580b90466a8dbca3219db5c5d9",
            "placeholder": "​",
            "style": "IPY_MODEL_a56ee7c7e6fa406c87f8f0b669d8f920",
            "value": " 570/570 [00:00&lt;00:00, 31.6kB/s]"
          }
        },
        "fb508e2310a3447db3be5b2d392154c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99a1fc54b334fa78e4040462506a6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ce7a4cd5af43d3a27deb15b7cf904e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19b613a533de4fbf908a595009e167fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac5ad61186d4fc0a1d18929cd60d424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5a7e2580b90466a8dbca3219db5c5d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ee7c7e6fa406c87f8f0b669d8f920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22f4ce50f2244467af9feb1852c4b033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bd0bfbf660f4d739706c7577be2ddb0",
              "IPY_MODEL_6e3fc2071e3a4980a0d219e1eccdede1",
              "IPY_MODEL_182fa9fe1b474babacbdf121f5c883e8"
            ],
            "layout": "IPY_MODEL_84d93fa686b54dae9b923a38377d1bc7"
          }
        },
        "9bd0bfbf660f4d739706c7577be2ddb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f170bce8f8dd4d51abde2cc539a44c74",
            "placeholder": "​",
            "style": "IPY_MODEL_0d57bc6feda049d0a8f5209275246c96",
            "value": "model.safetensors: 100%"
          }
        },
        "6e3fc2071e3a4980a0d219e1eccdede1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9d69c73b6847bf9d4b01ab1887f473",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc980aeb251740139027c146d915b9a6",
            "value": 435755784
          }
        },
        "182fa9fe1b474babacbdf121f5c883e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdebfa5e4817400185d654d9d2ebf2f7",
            "placeholder": "​",
            "style": "IPY_MODEL_89dea32bc40242f78ac88592d6fe2e6e",
            "value": " 436M/436M [00:03&lt;00:00, 137MB/s]"
          }
        },
        "84d93fa686b54dae9b923a38377d1bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f170bce8f8dd4d51abde2cc539a44c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d57bc6feda049d0a8f5209275246c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9d69c73b6847bf9d4b01ab1887f473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc980aeb251740139027c146d915b9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdebfa5e4817400185d654d9d2ebf2f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89dea32bc40242f78ac88592d6fe2e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leoli04/llms-notebooks/blob/main/huggingface/hf_nlp_02_use_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用transformer"
      ],
      "metadata": {
        "id": "uP7BPTYCZhg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "B3FkGILQec70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QbbUpO8Ierbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行结果：\n",
        "\n",
        "```\n",
        "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
        " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n",
        "```\n",
        "pipeline将三个步骤组合在一起：**预处理**、**向模型传递输入**以及**后处理**：\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg\" alt=\"pipeline处理步骤\" style=\"width: 500px; height: 300px;\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VMMecrbHfDeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### pipeline内部原理"
      ],
      "metadata": {
        "id": "iwqa8v2Jv3wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 使用tokenizer进行预处理\n",
        "tokenizer作用将文本输入转换为模型可以理解的数字。主要处理内容：\n",
        "- 将输入拆分为单词、子单词或符号（如标点符号），这些单词、子单词或符号（如标点符号）被称为tokens\n",
        "- 将每个tokens映射到一个整数\n",
        "- 添加可能对模型有用的额外输入\n",
        "\n",
        "预处理都需要以与预训练模型时完全相同的方式完成。我们可以使用`AutoTokenizer.from_pretrained(checkpoint)`获取与模型的标记生成器关联的数据并缓存它。\n",
        "\n",
        "查看`sentiment-analysis`模型 `checkpoint`（默认为`distilbert-base-uncased-finetuned-sst-2-english `）标记数据，如下：\n"
      ],
      "metadata": {
        "id": "NFG6kDhHgqOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# tensorflow\n",
        "# inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxPFqDjOhjJu",
        "outputId": "c6864997-ebc1-48b2-ede5-d0503d883aea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 向模型传递输入\n",
        "- 使用`AutoModel.from_pretrained(checkpoint)`实例化一个模型\n",
        "- 将预处理后的数据传递给实例化后的模型\n",
        "\n",
        "通过上面两步处理，将得到模型的高维向量：\n",
        "\n",
        "**Batch size**：一次处理的序列数(（即预处理结果中input_ids大小）)\n",
        "**Sequence length**：序列的数字表示的长度（即预处理结果中input_ids元素长度）\n",
        "**Hidden size**：每个模型输入的向量维度"
      ],
      "metadata": {
        "id": "zz8y4QH-lOyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "#tensorflow 使用 TFAutoModel\n",
        "# from transformers import TFAutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "# shape 函数返回一个表示张量维度的元组。对于三维数组，返回形如 (n, m, p) 的元组\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp2mvWtdkZvF",
        "outputId": "804e7e99-9d2b-415e-e94e-7e2ba8203a93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
            "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
            "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
            "         ...,\n",
            "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
            "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
            "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
            "\n",
            "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
            "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
            "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
            "         ...,\n",
            "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
            "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
            "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
            "torch.Size([2, 16, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model传递输入内部机制\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg\"/>\n",
        "\n",
        "在这个图中。模型有`embeddings layer`和 `subsequent layers`。\n",
        "- `embeddings layer`将标记化输入中的每个输入 ID 转换为表示关联标记的向量。\n",
        "- `subsequent layers`使用注意力机制操纵这些向量来产生句子的最终表示。\n",
        "\n",
        "`model heads`将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。（Transformer模型的输出直接发送到模型头进行处理）\n",
        "\n",
        "\n",
        "Transformers 中可用的架构（每种架构都是围绕解决特定任务而设计的）：\n",
        "- *Model (retrieve the hidden states)\n",
        "- *ForCausalLM\n",
        "- *ForMaskedLM\n",
        "- *ForMultipleChoice\n",
        "- *ForQuestionAnswering\n",
        "- *ForSequenceClassification\n",
        "- *ForSequenceClassification\n",
        "- and others"
      ],
      "metadata": {
        "id": "Ai3tZxCyoOIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#tensorflow 使用 TFAutoModelForSequenceClassification\n",
        "# from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXuwtFPzpiTB",
        "outputId": "29fa9641-8160-401f-a469-248d2d56b6a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 后置处理输出\n",
        "上面模型预测第一个句子的 [-1.5607, 1.6123] 和第二个句子的 [ 4.1692, -3.3464]。这些不是概率，要转换为概率，需要经过 SoftMax 层"
      ],
      "metadata": {
        "id": "KJ2Zx0VQk-Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)\n",
        "\n",
        "# tensorflow框架\n",
        "# import tensorflow as tf\n",
        "\n",
        "# predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
        "# print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwR_WSyKsPre",
        "outputId": "3541bd28-dc83-493f-ece2-f32b7ff06b0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以看到模型为第一个句子预测了 [0.0402, 0.9598] ，为第二个句子预测了 [0.9995, 0.0005] 。这些是可识别的概率分数。"
      ],
      "metadata": {
        "id": "ShZZPVtttSDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取与每个位置相对应的标签\n",
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKhJCLu0s81a",
        "outputId": "beb10fbe-47ad-4f7e-853b-f0c4ef718ded"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型的创建和使用\n"
      ],
      "metadata": {
        "id": "4QX7L7ITwJOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 创建Transformer"
      ],
      "metadata": {
        "id": "Slxp-P1uwi37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 使用默认方式创建：\n",
        "\n",
        "从默认配置创建模型会使用随机值对其进行初始化。这种方式模型可以使用，但会输出乱码；它需要先经过训练。为了避免不必要的重复工作，可以共享和重用已经训练过的模型。"
      ],
      "metadata": {
        "id": "m0F6GUbHxX4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, TFBertModel\n",
        "\n",
        "# Building the config\n",
        "config = BertConfig()\n",
        "\n",
        "# Building the model from the config\n",
        "model = TFBertModel(config)\n",
        "\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ5_rotAwo4r",
        "outputId": "e80a3b67-020a-48a7-fdda-dcb138ad984a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 使用训练好的的模型\n",
        "可以使用 `from_pretrained()` 方法加载已经训练好的 Transformer 模型。\n",
        "这种方式不需要使用 `BertConfig` ，而是通过 `bert-base-cased `标识符加载预训练模型，使用模型检查点的所有权重进行初始化。\n",
        "\n",
        "用于加载模型的标识符可以是模型中心上任何模型的标识符，只要它与 BERT 架构兼容即可。\n",
        "我们可以使用 https://huggingface.co/models?other=bert 来查看哪些模型与BERT 架构兼容。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hWvk19cFyAW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "model = TFBertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "f9e735fb070c433b8d5885e46eef20dc",
            "b90ab1d974d4486dbe2552fdf77168c8",
            "5fc151f25d544152be31221c26347f86",
            "054836a2d6f2489594d93599841c40e0",
            "fb508e2310a3447db3be5b2d392154c5",
            "c99a1fc54b334fa78e4040462506a6b0",
            "42ce7a4cd5af43d3a27deb15b7cf904e",
            "19b613a533de4fbf908a595009e167fa",
            "0ac5ad61186d4fc0a1d18929cd60d424",
            "a5a7e2580b90466a8dbca3219db5c5d9",
            "a56ee7c7e6fa406c87f8f0b669d8f920",
            "22f4ce50f2244467af9feb1852c4b033",
            "9bd0bfbf660f4d739706c7577be2ddb0",
            "6e3fc2071e3a4980a0d219e1eccdede1",
            "182fa9fe1b474babacbdf121f5c883e8",
            "84d93fa686b54dae9b923a38377d1bc7",
            "f170bce8f8dd4d51abde2cc539a44c74",
            "0d57bc6feda049d0a8f5209275246c96",
            "5b9d69c73b6847bf9d4b01ab1887f473",
            "dc980aeb251740139027c146d915b9a6",
            "fdebfa5e4817400185d654d9d2ebf2f7",
            "89dea32bc40242f78ac88592d6fe2e6e"
          ]
        },
        "collapsed": true,
        "id": "rlBzW_9DyH8S",
        "outputId": "20fdaea0-41ae-48a9-e9bc-7a6195a55a58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9e735fb070c433b8d5885e46eef20dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22f4ce50f2244467af9feb1852c4b033"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 保存模型\n",
        "\n",
        "使用 save_pretrained() 方法保存模型。这会将`config.json` `tf_model.h5`两个文件保存到您的磁盘上。\n",
        "> 如果使用的是pytorch生成的是config.json、 pytorch_model.bin两个文件\n",
        "\n",
        "- config.json ：该文件还包含一些元数据，例如检查点的起源位置以及上次保存检查点时使用的  Transformers 版本。\n",
        "- tf_model.h5 ： 被称为状态字典；它包含模型的所有权重。\n"
      ],
      "metadata": {
        "id": "kCUNIDf10B80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "Y92Otq7O0Q1O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 使用Transformer 模型进行推理\n",
        "\n"
      ],
      "metadata": {
        "id": "JVuaSBlG2k7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
        "# tokenizer 转换为input ID\n",
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ],
      "metadata": {
        "id": "EvI_hg5428jr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_inputs = tf.constant(encoded_sequences)\n",
        "\n",
        "# import torch\n",
        "\n",
        "# model_inputs = torch.tensor(encoded_sequences)\n",
        "\n",
        "output = model(model_inputs)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzrDuIFU3AzH",
        "outputId": "643e30f2-fa60-4b58-ec3b-73d033745cea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n",
            "array([[[ 4.4495744e-01,  4.8276284e-01,  2.7797198e-01, ...,\n",
            "         -5.4032274e-02,  3.9393425e-01, -9.4769992e-02],\n",
            "        [ 2.4942899e-01, -4.4092962e-01,  8.1772339e-01, ...,\n",
            "         -3.1916568e-01,  2.2992219e-01, -4.1171715e-02],\n",
            "        [ 1.3667546e-01,  2.2517854e-01,  1.4501992e-01, ...,\n",
            "         -4.6914592e-02,  2.8224233e-01,  7.5565845e-02],\n",
            "        [ 1.1788858e+00,  1.6738570e-01, -1.8187097e-01, ...,\n",
            "          2.4671350e-01,  1.0440764e+00, -6.1971312e-03]],\n",
            "\n",
            "       [[ 3.6435828e-01,  3.2463297e-02,  2.0257683e-01, ...,\n",
            "          6.0110353e-02,  3.2451293e-01, -2.0995583e-02],\n",
            "        [ 7.1865958e-01, -4.8725206e-01,  5.1740402e-01, ...,\n",
            "         -4.4012007e-01,  1.4553043e-01, -3.7544727e-02],\n",
            "        [ 3.3223230e-01, -2.3270901e-01,  9.4876081e-02, ...,\n",
            "         -2.5268167e-01,  3.2172003e-01,  8.1142318e-04],\n",
            "        [ 1.2523208e+00,  3.5754293e-01, -5.1321149e-02, ...,\n",
            "         -3.7839708e-01,  1.0526475e+00, -5.6254762e-01]],\n",
            "\n",
            "       [[ 2.4042264e-01,  1.4717777e-01,  1.2110285e-01, ...,\n",
            "          7.6061681e-02,  3.3564472e-01,  2.8261665e-01],\n",
            "        [ 6.5700614e-01, -3.2786581e-01,  2.4967620e-01, ...,\n",
            "         -2.5919506e-01,  2.0174681e-01,  3.3275127e-01],\n",
            "        [ 2.0159575e-01,  1.5782663e-01,  9.8975692e-03, ...,\n",
            "         -3.8850459e-01,  4.1307470e-01,  3.9731947e-01],\n",
            "        [ 1.0174985e+00,  6.4386743e-01, -7.8146642e-01, ...,\n",
            "         -4.2109150e-01,  1.0925050e+00, -4.8456412e-02]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
            "array([[-0.6856277 ,  0.5262493 ,  0.99995273, ...,  0.99998754,\n",
            "        -0.61123455,  0.9970657 ],\n",
            "       [-0.60545814,  0.49971724,  0.99981916, ...,  0.9999408 ,\n",
            "        -0.67532754,  0.97692657],\n",
            "       [-0.77015084,  0.54474187,  0.9999417 , ...,  0.9999845 ,\n",
            "        -0.4654935 ,  0.98939013]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers(分词器)\n",
        "\n",
        "分词器是 NLP 管道的核心组件之一。它的目的只有一个：**将文本转换为模型可以处理的数据**——模型只能处理数字，因此分词器需要将我们的文本输入转换为数字数据。\n"
      ],
      "metadata": {
        "id": "DWWZE17FT-bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 分词器的分类\n",
        "\n",
        "在了解分词器原理之前，我们先来理解一下有分词器实现有哪些方式\n",
        "\n",
        " #### 基于单词的\n",
        "基于单词的分词通常很容易设置和使用，只需一些规则，并且通常会产生不错的结果。\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg\"/>\n",
        "\n",
        "我们可以通过应用 Python 的 `split()`函数，使用空格将文本标记为单词.\n",
        "```\n",
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)\n",
        "```\n",
        "```\n",
        "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n",
        "\n",
        "```\n",
        "\n",
        "使用这种标记器，我们最终可以获得一些相当大的“词汇表”，其中词汇表是由我们语料库中拥有的独立标记的总数定义的。\n",
        "\n",
        "每个单词都会分配一个 ID，从 0 开始，一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。\n",
        "\n",
        "如果我们想用基于单词的分词器完全覆盖一种语言，我们需要为该语言中的每个单词都有一个标识符，这将生成大量的标记。例如，英语中有超过 500,000 个单词，因此要构建从每个单词到输入 ID 的映射，我们需要跟踪这么多 ID。但是，像“dog”这样的词与“dogs”这样的词的表示方式不同，模型最初无法知道“dog”和“dogs”是相似的：它会将这两个词识别为不相关。\n",
        "\n",
        "我们需要一个自定义标记来表示不在我们词汇表中的单词。这被称为“未知”令牌，通常表示为“ [unk]”或“”。如果您看到标记生成器正在生成大量此类标记，这通常是因为它无法检索单词的合理表示，并且您会在此过程中丢失信息。\n",
        "\n",
        "**减少未知标记数量的一种方法是使用基于字符的分词器。**\n",
        "\n",
        " #### 基于字符\n",
        "\n",
        " 基于字符的分词器将文本拆分为字符有两个主要好处：\n",
        " - 词汇量要小得多。\n",
        " - 词汇表外（未知）标记要少得多，因为每个单词都可以由字符构建。\n",
        "\n",
        " <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg\"/>\n",
        "\n",
        " 这种方法也并不完美。直观上来说，它的意义不大：每个字符本身并没有多大意义，而单词就是这样。然而，这又根据语言的不同而有所不同；例如，在中文中，每个字符比拉丁语言中的字符携带更多信息。\n",
        " 还有，我们的模型最终会处理大量的标记：虽然一个单词在基于单词的标记生成器中只是一个标记，但它可以很容易地变成 10 个或更多标记当转换成字符时。\n",
        "\n",
        " 为了充分利用这两种方法，我们可以使用结合了两种方法的第三种技术：**子词分词。**\n",
        "\n",
        " #### 基于子词分词\n",
        "\n",
        " 子词分词算法依赖于这样的原则：频繁使用的词不应该被分割成更小的子词，但罕见的词应该被分解成有意义的子词。\n",
        "\n",
        " 下面示例，显示子词分词算法如何标记序列\"Let’s do tokenization!\"\n",
        "\n",
        " <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg\"/>\n",
        "\n",
        " ##### 其他\n",
        " - Byte-level BPE, as used in GPT-2\n",
        " - WordPiece, as used in BERT\n",
        " - SentencePiece or Unigram, as used in several multilingual models\n"
      ],
      "metadata": {
        "id": "miCx2bGwA821"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 分词器加载和保存\n",
        "\n",
        "加载和保存分词器与模型一样简单。它基于相同的两个方法：from_pretrained() 和 save_pretrained()。\n",
        "\n",
        "加载使用与 BERT 相同的检查点训练的 BERT 分词器的方式与加载模型相同，\n",
        "方式一：我们使用 BertTokenizer 类："
      ],
      "metadata": {
        "id": "fb6OJ3nU9Eah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3O78dlOv9cGn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "方式二：使用AutoTokenizer\n",
        "与 AutoModel 类似， AutoTokenizer 类将根据检查点名称在库中获取正确的分词器类，并且可以直接与任何检查点一起使用："
      ],
      "metadata": {
        "id": "9QRaZpyQ-BOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "YjZSIBKC-EGL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysysyMjP-LVu",
        "outputId": "3d63c548-1dbf-412a-c7e0-4fd1f8719c52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### encoding(编码)\n",
        "\n",
        "将文本转换为数字称为编码。编码分两步过程完成：标记化，然后转换为输入 ID。"
      ],
      "metadata": {
        "id": "spQc9w-5-k6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenization （分词过程）\n",
        "\n",
        "分词过程由分词器的 tokenize() 方法完成。结果就是得到token数量"
      ],
      "metadata": {
        "id": "e_PIkVPW-_9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlfR1sSe_Sql",
        "outputId": "11f73016-f0d0-43cd-a337-168f09c5c949"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 从token到imput ID\n",
        "\n",
        "到输入 ID 的转换由 tokenizer.convert_tokens_to_ids()  方法处理："
      ],
      "metadata": {
        "id": "8uy37-cr_mif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZe4ZcN0_4Le",
        "outputId": "5fb3aaab-e747-4ef4-ae70-9cac48f76bc2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoding(解码)\n",
        "\n",
        "从词汇索引中，得到我们想要的字符串。可以使用 decode() 方法。\n",
        "\n",
        "decode 方法不仅将索引转换回token，而且还将属于相同单词的token组合在一起以生成可读的句子。"
      ],
      "metadata": {
        "id": "bNbpGmpdAKn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsqXcsfLAZiN",
        "outputId": "f4a32162-7960-4351-9b9f-166e9120f227"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a transformer network is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 处理多个序列\n",
        "\n",
        "自己分部实现一次处理多条文本推理的情况，需要注意以下几点：\n",
        "\n",
        "- 向模型传递文本分词后的数字列表时，需要在其上添加了一个维度"
      ],
      "metadata": {
        "id": "W45FXmqhCkBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7WZB2dgC_nH",
        "outputId": "cb8ebf50-fc91-4832-8939-1e981e10e9d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用封装好的api如下，可以看到分词器不仅将输入 ID 列表转换为张量，还在其上添加了一个维度"
      ],
      "metadata": {
        "id": "YR6RVmtoksGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "outputs = model(**tokenized_inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RTMG7sDkIWT",
        "outputId": "74ed3eb7-74b6-42b7-8029-c95c018a5cf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "所以我们在自己处理的时候，需要给ids增加一个维度，`torch.tensor([ids])`"
      ],
      "metadata": {
        "id": "YDPS8AQwmeFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# 注意，这里给的是数组\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAkNzPFBkcFI",
        "outputId": "f48620d2-2b72-4ab2-bf12-7eb42e0f1088"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 当使用多个文本推理的时候，每个文本长度可能不同，这个时候通常会填充输入。\n",
        "填充通过向具有较少值的句子添加一个称为`padding token`的特殊单词来确保所有句子具有相同的长度。"
      ],
      "metadata": {
        "id": "0nsdsL6enodi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emGXucHPoYnY",
        "outputId": "ca57586a-0fc6-4dc7-a0b0-384a20602ebb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention masks（注意力忽略）\n",
        "\n",
        "Attention masks 是与输入 ID 张量维度完全相同的张量，填充有 0 和 1。1 表示应注意相应的标记，0 表示不应注意相应的标记（即，应忽略它们）。\n",
        "\n",
        "上面例子中，批处理的第二行应该与第二句话的逻辑相同，但我们得到了完全不同的值！就是因为Transformer 模型的关键特征是将每个标记置于上下文中的注意力层，这些将考虑填充标记。\n",
        "\n",
        "为了在通过模型传递不同长度的单个句子或传递具有相同句子并应用填充的批次时获得相同的结果，我们需要告诉这些注意层忽略填充标记。这是通过使用注意力掩模来完成的。"
      ],
      "metadata": {
        "id": "a76Neu93pbms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW45bkjmqm6s",
        "outputId": "ccaef4d6-b772-4c0e-d7e4-2b19f8f91201"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 长文本\n",
        "\n",
        "对于 Transformer 模型，我们可以传递模型的序列长度是有限的。大多数模型可处理最多 512 或 1024 个标记的序列，并且当要求处理更长的序列时会崩溃。这个问题有两种解决方案：\n",
        "\n",
        "- 使用支持的序列长度更长的模型。\n",
        "\n",
        "- 文本截断\n",
        "\n",
        "如何要处理很长的文本，可以查看这个两个模型：\n",
        "[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) 和 [LED](https://huggingface.co/docs/transformers/model_doc/led)\n",
        "\n",
        "如果选择截断的方式，可以使用 max_sequence_length：\n",
        "```\n",
        "sequence = sequence[:max_sequence_length]\n",
        "```"
      ],
      "metadata": {
        "id": "X8II_CwErJBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenizer 整合\n",
        "\n",
        "上面我们探索了分词器的工作原理，并研究了分词、转换为输入 ID、填充、截断和注意力掩码。\n",
        "\n",
        "这些都可以通过tokenizer 直接处理"
      ],
      "metadata": {
        "id": "xyHhvHM3t4ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ],
      "metadata": {
        "id": "rLeU3XzbuMaL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 处理多个序列"
      ],
      "metadata": {
        "id": "drwNFd0Zvwnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)"
      ],
      "metadata": {
        "id": "fvxuaj7LuTdb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 填充"
      ],
      "metadata": {
        "id": "Hpb-XlFOvkTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
      ],
      "metadata": {
        "id": "nn9R_5CTu58e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 截断序列"
      ],
      "metadata": {
        "id": "2Znktb0cvFtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ],
      "metadata": {
        "id": "54l07_KOvDQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 处理到特定框架张量的转换\n",
        "\n",
        " \"pt\" 返回 PyTorch 张量，\n",
        "\"tf\" 返回 TensorFlow 张量，\n",
        "\"np\" 返回 NumPy 数组："
      ],
      "metadata": {
        "id": "dbqZtOUwvNaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Returns PyTorch tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Returns TensorFlow tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Returns NumPy arrays\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
      ],
      "metadata": {
        "id": "YEw6mKmFvXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 特殊 tokens"
      ],
      "metadata": {
        "id": "gNRMEgctwYks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgwB923wcbL",
        "outputId": "e18c1fc2-da1c-4620-ca4f-82fe789b0a83"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbK-PiW3wf-G",
        "outputId": "219f27b9-19a8-4cee-b9ec-b3c289a0dea3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
            "i've been waiting for a huggingface course my whole life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分词器在开头添加特殊单词 [CLS] ，在末尾添加特殊单词 [SEP] 。这是因为模型是用这些进行预训练的，因此为了获得相同的推理结果，我们还需要添加它们。"
      ],
      "metadata": {
        "id": "-NMAAntPwnpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformers 使用总结\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "```"
      ],
      "metadata": {
        "id": "C0vQLLhfwsbx"
      }
    }
  ]
}