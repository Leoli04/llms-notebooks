{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leoli04/llms-notebooks/blob/main/pytorch/basic/neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxAmF3malys1"
      },
      "source": [
        "神经网络\n",
        "===============\n",
        "\n",
        "神经网络可以使用 torch.nn 包构建。\n",
        "\n",
        "现在您已经了解了 autograd ， nn 依赖于 autograd 来定义模型并区分它们。 nn.Module 包含层和返回 output 的方法 forward(input) 。\n",
        "\n",
        "例如，看看这个对数字图像进行分类的网络:\n",
        "\n",
        "![convnet](https://pytorch.org/tutorials/_static/img/mnist.png)\n",
        "\n",
        "这是一个简单的前馈网络。它接受输入，将其逐层输入，最后给出输出。\n",
        "\n",
        "\n",
        "\n",
        "神经网络的典型训练过程如下:\n",
        "\n",
        "-   定义具有一些可学习参数（或权重）的神经网络\n",
        "-   迭代输入数据集\n",
        "-   通过网络处理输入\n",
        "-   计算损失（输出距离正确还有多远）\n",
        "-   将梯度传播回网络参数\n",
        "-   更新网络的权重，通常使用简单的更新规则:\n",
        " `weight = weight - learning_rate * gradient`\n",
        "\n",
        "定义网络\n",
        "------------------\n",
        "\n",
        "让我们定义这个网络：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ufgagEcdlys3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b81a11-9886-49d9-8f05-a87024e376a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a (N, 16, 10, 10) Tensor\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # Fully connected layer F5: (N, 400) Tensor input,\n",
        "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        # Fully connected layer F6: (N, 120) Tensor input,\n",
        "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
        "        # outputs a (N, 10) Tensor\n",
        "        output = self.fc3(f6)\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyjI9LEHlys4"
      },
      "source": [
        "您只需定义 forward 函数，然后使用 autograd 为您自动定义 backward 函数（计算梯度）。您可以在 forward 函数中使用任何张量运算。\n",
        "\n",
        "模型的可学习参数由 net.parameters() 返回\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4W3Et8xolys4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559864f6-84c7-4dba-e695-8eac83ae99ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoYbgg2llys4"
      },
      "source": [
        "让我们尝试随机 32x32 输入。注意：该网络 (LeNet) 的预期输入大小为 32x32。要在 MNIST 数据集上使用此网络，请将数据集中的图像大小调整为 32x32。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mYQaXkMmlys4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3265230f-a34b-427a-d838-c85f30e54421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0075,  0.0251,  0.0323,  0.0477, -0.0540, -0.0502, -0.0344,  0.0977,\n",
            "         -0.0236,  0.0073]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3guFZSK5lys5"
      },
      "source": [
        "先清空梯度缓存，然后反向传播随机梯度："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R6RQvJL8lys5"
      },
      "outputs": [],
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dJ1Bzqslys5"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p><code>torch.nn</code> 只支持小批量(mini-batches)数据，也就是输入不能是单个样本，比如对于 nn.Conv2d 接收的输入是一个 4 维张量--nSamples * nChannels * Height * Width 。\n",
        "所以，如果你输入的是单个样本，需要采用 input.unsqueeze(0) 来扩充一个假的 batch 维度，即从 3 维变为 4 维。</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "**回顾:**\n",
        "\n",
        ":   -   `torch.Tensor` - 支持自动分级操作的多维数组，例如 backward() 。还保存梯度 w.r.t.张量。\n",
        "    -   `nn.Module` - 神经网络模块。封装参数的便捷方式，带有将参数移动到 GPU、导出、加载等的帮助程序。\n",
        "    -   `nn.Parameter` - 一种张量，当作为属性分配给 Module 时会自动注册为参数。\n",
        "    -   `autograd.Function` - 实现自动分级操作的前向和后向定义。每个 Tensor 操作至少创建一个 Function 节点，该节点连接到创建 Tensor 的函数并对其历史记录进行编码。\n",
        "\n",
        "\n",
        "\n",
        "损失函数\n",
        "=============\n",
        "\n",
        "损失函数采用（输出，目标）输入对，并计算一个值来估计输出与目标的距离。\n",
        "\n",
        " nn 包下有几种不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。一个简单的损失是： nn.MSELoss ，它计算输出和目标之间的均方误差。\n",
        "\n",
        "For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_wMb51WClys6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe32f781-3b3d-476e-ce79-57ff855ee809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4960, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84N_GL9Slys6"
      },
      "source": [
        "现在，如果您使用 .grad_fn 属性沿向后方向跟踪 loss ，您将看到如下所示的计算图：:\n",
        "\n",
        "``` {.sourceCode .sh}\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "```\n",
        "\n",
        "当我们调用 loss.backward() 时，整个图是微分的。神经网络参数，并且图中所有具有 requires_grad=True 的张量将其 .grad 张量随梯度累积。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-hqGL9fKlys6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24157205-c8cc-4f4b-ad6f-fee6c4bbfdde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MseLossBackward0 object at 0x7e19f4c1f340>\n",
            "<AddmmBackward0 object at 0x7e19f5a4ad10>\n",
            "<AccumulateGrad object at 0x7e19f4c1f340>\n"
          ]
        }
      ],
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ0RhJidlys6"
      },
      "source": [
        "反向传播\n",
        "========\n",
        "反向传播的实现只需要调用 loss.backward() 即可，当然首先需要清空当前梯度缓存，即.zero_grad() 方法，否则之前的梯度会累加到当前的梯度，这样会影响权值参数的更新。\n",
        "\n",
        "下面是一个简单的例子，以 conv1 层的偏置参数 bias 在反向传播前后的结果为例：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "grHPow17lys7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd09eb0a-ff80-4cad-d570-ab6c990fae16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([-0.0014, -0.0056, -0.0041, -0.0026,  0.0006, -0.0085])\n"
          ]
        }
      ],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwDStvalys7"
      },
      "source": [
        "了解更多有关 torch.nn 库,可以看[这里](https://pytorch.org/docs/nn)\n",
        "\n",
        "更新权重\n",
        "==================\n",
        "\n",
        "采用随机梯度下降(Stochastic Gradient Descent, SGD)方法的最简单的更新权重规则如下：\n",
        "\n",
        "``` {.sourceCode .python}\n",
        "weight = weight - learning_rate * gradient\n",
        "```\n",
        "\n",
        "按照这个规则，代码实现如下所示：e:\n",
        "\n",
        "``` {.sourceCode .python}\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)\n",
        "```\n",
        "\n",
        "但是这只是最简单的规则，深度学习有很多的优化算法，不仅仅是 SGD，还有 Nesterov-SGD, Adam, RMSProp 等等，为了采用这些不同的方法，这里采用 torch.optim 库，使用例子如下所示：:\n",
        "\n",
        "``` {.sourceCode .python}\n",
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGoBxJcElys7"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>需要调用 optimizer.zero_grad() 方法清空梯度缓存。</p>\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}