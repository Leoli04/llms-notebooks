{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leoli04/llms-notebooks/blob/main/pytorch/60_minute_blitz/torch_autograd_Introduction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaYiOYnW6sEh"
      },
      "source": [
        "`torch.autograd`的简要介绍\n",
        "=========================================\n",
        "\n",
        "`torch.autograd` 是 PyTorch 的自动微分引擎，为神经网络训练提供动力。在本节中，您将对 autograd 如何帮助神经网络训练有一个概念性的了解。\n",
        "\n",
        "背景\n",
        "----------\n",
        "\n",
        "神经网络 (NN) 是对某些输入数据执行的嵌套函数的集合。这些函数由参数（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。\n",
        "\n",
        "训练神经网络分两步进行：:\n",
        "\n",
        "**Forward Propagation（前向传播）**: 在前向传播中，神经网络对正确的输出做出最佳猜测。它通过每个函数运行输入数据来进行猜测。\n",
        "\n",
        "**Backward Propagation（反向传播）**: 在反向传播中，神经网络根据其猜测的误差按比例调整其参数。它通过从输出向后遍历，收集误差相对于函数参数（梯度）的导数，并使用梯度下降优化参数来实现这一点。有关反向传播的更详细演练，请观看 [\n",
        "3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).\n",
        "\n",
        "在 PyTorch 中的用法\n",
        "----------------\n",
        "\n",
        "让我们看一下单个训练步骤。对于此示例，我们从 torchvision 加载预训练的 resnet18 模型。我们创建一个随机数据张量来表示具有 3 个通道、高度和宽度为 64 的单个图像，并将其相应的 label 初始化为一些随机值。预训练模型中的标签具有形状 (1,1000)。.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>本教程仅适用于 CPU，不适用于 GPU 设备（即使张量移至 CUDA）。.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VLRuroEr6sEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56594f7d-9199-464e-c576-ab12b81e7fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 45.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "labels = torch.rand(1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUkeudQ6sEl"
      },
      "source": [
        "接下来，我们通过模型的每一层运行输入数据以进行预测。这就是**向前传递**。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4CVzmWSR6sEl"
      },
      "outputs": [],
      "source": [
        "prediction = model(data) # forward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2mG4n7v6sEm"
      },
      "source": [
        "我们使用模型的预测和相应的标签来计算误差（ loss ）。下一步是通过网络反向传播此错误。当我们对误差张量调用 .backward() 时，反向传播就开始了。然后，Autograd 计算每个模型参数的梯度并将其存储在参数的 .grad 属性中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U77sXK8d6sEm"
      },
      "outputs": [],
      "source": [
        "loss = (prediction - labels).sum()\n",
        "loss.backward() # backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCb9wJ_e6sEn"
      },
      "source": [
        "接下来，我们加载优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。我们在优化器中注册模型的所有参数\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zKBBL1qj6sEn"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjk6mJm6sEo"
      },
      "source": [
        "最后，我们调用 .step() 来启动梯度下降。优化器通过存储在 .grad 中的梯度来调整每个参数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4sLlil3B6sEo"
      },
      "outputs": [],
      "source": [
        "optim.step() #gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ml5_eN66sEo"
      },
      "source": [
        "此时，您已拥有训练神经网络所需的一切。以下部分详细介绍了 autograd 的工作原理 - 可以跳过它们。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DVAfQRD6sEp"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp2_70s46sEp"
      },
      "source": [
        "Autograd 中的差异化\n",
        "===========================\n",
        "\n",
        "我们来看看 autograd 是如何收集梯度的。我们使用 requires_grad=True 创建两个张量 a 和 b 。这向 autograd 发出信号，应跟踪对它们的每个操作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_NLixrfB6sEp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9PJz_aS6sEp"
      },
      "source": [
        "我们从 a 和 b 创建另一个张量 Q 。\n",
        "\n",
        "$$Q = 3a^3 - b^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ekm2ptIt6sEp"
      },
      "outputs": [],
      "source": [
        "Q = 3*a**3 - b**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH4t3oWf6sEp"
      },
      "source": [
        "我们假设 a 和 b 是神经网络的参数， Q 是错误。在神经网络训练中，我们需要误差的梯度。参数，即\n",
        "\n",
        "\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
        "\n",
        "当我们在 Q 上调用 .backward() 时，autograd 会计算这些梯度并将它们存储在相应张量的 .grad 属性中。\n",
        "\n",
        "我们需要在 Q.backward() 中显式传递 gradient 参数，因为它是一个向量。 gradient 是与 Q 形状相同的张量，它表示 Q w.r.t 的梯度。本身，即\n",
        "\n",
        "$$\\frac{dQ}{dQ} = 1$$\n",
        "\n",
        "同样，我们也可以将 Q 聚合为标量并隐式向后调用，如 Q.sum().backward() 。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YMr1IDFo6sEq"
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J05IExOg6sEq"
      },
      "source": [
        "梯度现在存放在 a.grad 和 b.grad 中\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "No_S6mKq6sEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3068c880-27f8-47da-b9c1-14b7102eac24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ],
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisO4Q7W6sEr"
      },
      "source": [
        "计算图\n",
        "===================\n",
        "\n",
        "从概念上讲，autograd 在由 Function 对象组成的有向无环图 (DAG) 中保存数据（张量）和所有执行的操作（生成的新张量）的记录。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶追踪该图，您可以使用链式法则自动计算梯度。\n",
        "\n",
        "在前向传递中，autograd 同时执行两件事：:\n",
        "\n",
        "-   运行请求的操作来计算结果张量\n",
        "-   在 DAG 中维护操作的梯度函数。\n",
        "\n",
        "当在 DAG 根上调用 .backward() 时，向后传递开始。 autograd 然后:\n",
        "\n",
        "-   计算每个 .grad_fn 的梯度，\n",
        "-   将它们累积到各自张量的 .grad 属性中\n",
        "-   使用链式法则，一直传播到叶张量。\n",
        "\n",
        "下面是我们示例中 DAG 的直观表示。图中，箭头指向前向传递的方向。节点代表前向传递中每个操作的后向函数。蓝色的叶节点代表我们的叶张量 a 和 b 。\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/dag_autograd.png)\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>需要注意的重要一点是图是从头开始重新创建的；每次 .backward() 调用后，autograd 开始填充新图表。这正是允许您在模型中使用控制流语句的原因；如果需要，您可以在每次迭代时更改形状、大小和操作。</p>\n",
        "</div>\n",
        "\n",
        "从 DAG 中排除\n",
        "----------------------\n",
        "\n",
        "torch.autograd 跟踪所有 requires_grad 标志设置为 True 的张量上的操作。对于不需要梯度的张量，将此属性设置为 False 会将其从梯度计算 DAG 中排除。\n",
        "\n",
        "即使只有一个输入张量具有 requires_grad=True ，操作的输出张量也将需要梯度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tJ9CVnKG6sEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e88ddcb-6336-40bf-d53e-73cb3f1edfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does `a` require gradients?: False\n",
            "Does `b` require gradients?: True\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, 5)\n",
        "y = torch.rand(5, 5)\n",
        "z = torch.rand((5, 5), requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
        "b = x + z\n",
        "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJLnSu9V6sEr"
      },
      "source": [
        "在神经网络中，不计算梯度的参数通常称为**冻结参数**。如果您事先知道不需要这些参数的梯度，那么“冻结”模型的一部分会很有用（这通过减少自动梯度计算来提供一些性能优势）。\n",
        "\n",
        "在微调中，我们冻结大部分模型，通常只修改分类器层以对新标签进行预测。让我们通过一个小例子来演示这一点。和之前一样，我们加载预训练的 resnet18 模型，并冻结所有参数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BWuTNXzg6sEr"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# Freeze all the parameters in the network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMMBC9Fo6sEr"
      },
      "source": [
        "假设我们想要在具有 10 个标签的新数据集上微调模型。在 resnet 中，分类器是最后一个线性层 model.fc 。我们可以简单地用一个新的线性层（默认情况下未冻结）替换它作为我们的分类器。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k0OyO1p76sEs"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KqhHV366sEs"
      },
      "source": [
        "现在模型中的所有参数（除了 model.fc 的参数）都被冻结。计算梯度的唯一参数是 model.fc 的权重和偏差。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "izeqLP2K6sEs"
      },
      "outputs": [],
      "source": [
        "# Optimize only the classifier\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh0G7Sac6sEs"
      },
      "source": [
        "请注意，虽然我们在优化器中注册了所有参数，但计算梯度（因此在梯度下降中更新）的唯一参数是分类器的权重和偏差。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbMNOxY26sEs"
      },
      "source": [
        "Further readings:\n",
        "=================\n",
        "\n",
        "-   [In-place operations & Multithreaded\n",
        "    Autograd](https://pytorch.org/docs/stable/notes/autograd.html)\n",
        "-   [Example implementation of reverse-mode\n",
        "    autodiff](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC)\n",
        "-   [Video: PyTorch Autograd Explained - In-depth\n",
        "    Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}